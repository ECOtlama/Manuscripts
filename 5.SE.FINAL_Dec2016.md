#SE: Final dataset

####Beelzebufo backup

External drives not ejecting on mac may be due to spotlight indexing the drive. 

Problem seen to be Spotlight, i must stop Spotlight with this command before eject:
```
sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.metadata.mds.plist
```
And restart Spotlight after because i use it often:
```
sudo launchctl load -w /System/Library/LaunchDaemons/com.apple.metadata.mds.plist
```



After speaking with Victoria, I will include all the populations (even with <5 individuals per population) in the final dataset. Also, based on population structure analyses, I will exclude the DE populations, as they are from a different gene pool. 

I'm also removing 12 Skane individuals that look different from the rest. I think something went wrong when I concatenated data from different runs. These individuals all tend to have higher genetic diversity than expected. I will have to map these data to determine what the correct SNP calls are. 

Diversity and F-statistics will be based on the whole dataset for now. I will reassess after speaking with Victoria. 

##Step 1: Remove DE

And Sk individuals. 

/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE133

```
vcftools --vcf SEFinalc94d6m4p3.vcf --recode --recode-INFO-all --out SEFinalc94d6m4p3.vcf
bcftools query -l SEFinalc94d6m4p3.vcf

bcftools reheader SEFinalc94d6m4p3.vcf.recode.vcf -s raw.SE193.names --out SE193.raw.vcf

vcftools --vcf SE193.raw.vcf --remove DE.Sk.names --recode --recode-INFO-all --out SE151
```

##Step 2: Remove loci genotyped in <80% individuals

```
vcftools --vcf SE151.recode.vcf --max-missing 0.8 --recode --recode-INFO-all --out SE151.s1

After filtering, kept 18800 out of a possible 1300946 Sites
```

Look at missingness
```
vcftools --vcf SE151.s1.recode.vcf --missing-indv

##R
library(ggplot2)
SE.151.s1 <- read.table("out.imiss", header=T)
pop <- read.table("SE151.popnames", header=T)
SE.151.s1$pop <- pop$pop
SE.151.s1$pop.order <- pop$pop.order

SE.151.s1.sort <- SE.151.s1[order(SE.151.s1$pop.order),]

SE.151.s1.sort$pop <- factor(SE.151.s1.sort$pop, levels=SE.151.s1.sort$pop)   ##sort pop.nr. Numbers from south to North

qplot(pop, F_MISS, data=SE.151.s1.sort, geom=c("boxplot", "jitter"))
```

![alt_txt][s1.missing]
[s1.missing]:https://cloud.githubusercontent.com/assets/12142475/20881932/eff4ffee-bade-11e6-80a6-5d3ff2f83310.png


Calculate SFS & LD for s1
```
vcftools --vcf SE151.s1.recode.vcf --plink --out SE151.s1.plink 
plink --file SE151.s1.plink --recode --recodeA --out SE151.plink

plink --file SE151.s1.plink --freq --out SE151.s1.plink
plink --file SE151.s1.plink --r2 --out SE151.s1.plink
```


##Step 3-5 MAF, HWE, LD, thin

After each step, calculate LD & SFS
```
vcftools --vcf SE151.s1.recode.vcf --maf 0.05 --recode --recode-INFO-all --out SE151.s2

After filtering, kept 3897 out of a possible 18800 Sites

vcftools --vcf SE151.s2.recode.vcf --plink --out SE151.s2.plink 
plink --file SE151.s2.plink --recode --recodeA --out SE151.s2.plink

plink --file SE151.s2.plink --freq --out SE151.s2.plink
plink --file SE151.s2.plink --r2 --out SE151.s2.plink
```

Calculate HWE and LD
```
mkdir popnames.plink.folder ##directory with 15 files each containing the population names
mkdir subset.data
for i in $(ls popnames.plink.folder/); do plink --file SE151.s2.plink --keep popnames.plink.folder/$i --recode --recodeA --out subset.data/$i.plink; done

for i in $(ls popnames.plink.folder/); do plink --file subset.data/$i.plink --freq --out subset.data/$i; done
for i in $(ls popnames.plink.folder/); do plink --file subset.data/$i.plink --r2 --out subset.data/$i; done

##R
Sk.Ho.frq <- read.table("subset.data/Sk.Ho.frq", header=T)
Sk.SF.frq <- read.table("subset.data/Sk.SF.frq", header=T)
Sk.SL.frq <- read.table("subset.data/Sk.SL.frq", header=T)

Upp.Gra.frq <- read.table("subset.data/Upp.Gra.frq", header=T)
Upp.K.frq <- read.table("subset.data/Upp.K.frq", header=T)
Upp.O.frq <- read.table("subset.data/Upp.O.frq", header=T)

Um.Taf.frq <- read.table("subset.data/Um.Taf.frq", header=T)
Um.Gr.frq <- read.table("subset.data/Um.Gr.frq", header=T)
Um.UM3.frq <- read.table("subset.data/Um.UM3.frq", header=T)

LT1.frq <- read.table("subset.data/LT1.frq", header=T)
LT2.frq <- read.table("subset.data/LT2.frq", header=T)
LT3.frq <- read.table("subset.data/LT3.frq", header=T)

Kir.L.frq <- read.table("subset.data/Kir.L.frq", header=T)
Kir.G.frq <- read.table("subset.data/Kir.G.frq", header=T)

FIN.frq <- read.table("subset.data/FIN.frq", header=T)

par(mfrow=c(3.5))
hist(Sk.Ho.frq$MAF, main="Sk.Ho (10, 3874, 85.3% genotyping) SFS")
hist(Sk.SF.frq$MAF, main="Sk.SF (10, 3874, 95.5%) SFS")
hist(Sk.SL.frq$MAF, main="Sk.SL (17, 3874, 92.6%) SFS")

hist(Upp.Gra.frq$MAF, main="Upp.Gra (10, 3874, 85.9%) SFS")
hist(Upp.K.frq$MAF, main="Upp.K (10, 3874, 95.8%) SFS")
hist(Upp.O.frq$MAF, main="Upp.O (10, 3874, 84.3%) SFS")

hist(Um.UM3.frq$MAF, main="Umea.UT3 (4, 3874, 53.3%) SFS")
hist(Um.Taf.frq$MAF, main="Umea.Taf (10, 3874, 91.8%) SFS")
hist(Um.Gr.frq$MAF, main="Umea.Gr (10, 3874, 90.6%) SFS")

hist(LT1.frq$MAF, main="LT1 (10, 3874, 91.8%) SFS")
hist(LT2.frq$MAF, main="LT2 (10, 3874, 57.2%) SFS")
hist(LT3.frq$MAF, main="LT3 (10, 3874, 79.3%) SFS")

hist(Kir.G.frq$MAF, main="Kir.G (10, 3874, 95.9%) SFS")
hist(Kir.L.frq$MAF, main="Kir.L (10, 3874, 94.2%) SFS")

hist(FIN.frq$MAF, main="FIN (10, 3874, 74.8%) SFS")
```


![alt_txt][SFS.s2.new]
[SFS.s2.new]:https://cloud.githubusercontent.com/assets/12142475/20882278/721773b6-bae0-11e6-8fb4-0c21de3acc85.png



Only variable loci
```
#Create a list of all the .frq dataframes
SE.s2.freq.list <- setNames(lapply(ls(pattern=".frq"), function(x) get(x)), ls(pattern=".frq"))

#and remove all .frq from the global env
rm(list=ls(pattern=".frq"))

#Filter all for MAF>0.001
SE.s2.freq.var.list <- lapply(SE.s2.freq.list, subset, MAF>0.01)

#plot hist of all new MAF with title
par(mfrow=c(3,5))
for(i in names(SE.s2.freq.var.list))
{
    df1 = as.data.frame(SE.s2.freq.var.list[[i]])
    hist(df1$MAF, main=i)
}

```

![alt_txt][SFS.s2]
[SFS.s2]:https://cloud.githubusercontent.com/assets/12142475/20882391/04a5595a-bae1-11e6-87d5-a6665866e982.png

Variable loci across all populations
```
attach(SE.s2.freq.var.list)
SE.s2.var.loci.freq <- do.call(rbind, lapply(names(SE.s2.freq.var.list), get))
summary(SE.s2.var.loci.freq)

SE.s2.var.loci.freq.keep <- data.frame(table(SE.s2.var.loci.freq$SNP)) 
#SE.var.loci.freq.keep <- subset(SE.var.loci.freq.keep, Freq>0) 
hist(SE.s2.var.loci.freq.keep$Freq, xlab="Nr pops", ylab="Frequency", main="Frequency of variable loci across 15 pops", breaks=seq(0.5,15.5, by=1.0))
detach(SE.s2.freq.var.list)
```

![alt_txt][variable.s2]
[variable.s2]:https://cloud.githubusercontent.com/assets/12142475/20882428/30704716-bae1-11e6-8f18-b331bfbb02aa.png

Fixed loci across all populations
```
#Filter all for fixed loci
SE.s2.freq.fixed.list <- lapply(SE.s2.freq.list, subset, MAF<0.000001)

attach(SE.s2.freq.fixed.list)
SE.s2.fixed.loci.freq <- do.call(rbind, lapply(names(SE.s2.freq.fixed.list), get))
summary(SE.s2.fixed.loci.freq)

SE.s2.fixed.loci.freq.keep <- data.frame(table(SE.s2.fixed.loci.freq$SNP)) 
#SE.var.loci.freq.keep <- subset(SE.var.loci.freq.keep, Freq>0) 
hist(SE.s2.fixed.loci.freq.keep$Freq, xlab="Nr pops", ylab="Frequency", main="Frequency of fixed loci across 15 pops", breaks=seq(0,14, by=1.0))
detach(SE.s2.freq.fixed.list)
```
![alt_txt][fixed.s2]
[fixed.s2]:https://cloud.githubusercontent.com/assets/12142475/20882453/4acd1878-bae1-11e6-97d3-b227c39b4c6d.png

##Thin to 1 SNP per locus
```
vcftools --vcf SE151.s2.recode.vcf --thin 130 --recode --recode-INFO-all --out SE151.s3

After filtering, kept 2192 out of a possible 3897 Sites

vcftools --vcf SE151.s3.recode.vcf --plink --out SE151.s3.plink
```



##step 6: Final checks: HWE, LD, missingness

LD
```
#R write tables of fixed loci
##Actually I can just filter for MAF < 0.005, so I've changed this code
for(i in names(SE.s3.freq.fixed.list)){
write.table(SE.s3.freq.fixed.list[[i]][2], paste(i,".fixedloci", sep=""), row.names=F, col.names=F, quote=F)
}
##linux
#for i in $(ls popnames.plink.folder/); do plink --file subset.data/$i.plink --exclude $i.frq.fixedloci --recode --recodeA --out ld.subset.data/$i.var.plink; done



##new code
##LINUX
mkdir ld.subset.data

for i in $(ls popnames.plink.folder/); do plink --file subset.data/$i.plink --maf 0.005 --recode --recodeA --out ld.subset.data/$i.var.plink; done

for i in $(ls popnames.plink.folder/); do plink --file ld.subset.data/$i.var.plink --r2 --out ld.subset.data/$i.var; done

##R
setwd('/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE133/')
ld.subset.path <- './ld.subset.data/'
ld.var.files <- list.files(ld.subset.path, pattern='ld$')
setwd('/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE133/ld.subset.data/')
ld.var.files.list <- lapply(ld.var.files, read.table, header=T)
names(ld.var.files.list) <- ld.var.files
names(ld.var.files.list)

##plot ld for each population
par(mfrow=c(3,5))
for(i in names(ld.var.files.list))
{
    df1 = as.data.frame(ld.var.files.list[[i]])
    hist(df1$R2, main=i)
}

#Calculate frequency lf LD
SE.s3.ld.R2.08.list <- lapply(ld.var.files.list, subset, R2>0.8)
attach(SE.s3.ld.R2.08.list)
SE.s3.ld.R2.08.freq <- do.call(rbind, lapply(names(SE.s3.ld.R2.08.list), get))
summary(SE.s3.ld.R2.08.freq)

SE.s3.ld.R2.08.freq.keep <- data.frame(table(SE.s3.ld.R2.08.freq$SNP_A, SE.s3.ld.R2.08.freq$SNP_B)) 
summary(SE.s3.ld.R2.08.freq.keep)
hist(SE.s3.ld.R2.08.freq.keep$Freq, xlab="Nr pops", ylab="Frequency", main="Frequency of linkage (R2>0.8) across 15 pops", breaks=seq(0,14, by=1.0))
#SE.s3.ld.R2.08.freq.keep <- subset(SE.s3.ld.R2.08.freq.keep, Freq>1) 
#hist(SE.s3.ld.R2.08.freq.keep$Freq, xlab="Nr pops", ylab="Frequency", main="Frequency of linkage (R2>0.8) across 15 pops", breaks=seq(0,14, by=1.0))

```

![alt_txt][LD.all]
[LD.all]:https://cloud.githubusercontent.com/assets/12142475/20882834/2bbd5d92-bae3-11e6-989c-74ab9d283f68.png

Most of the loci in LD are found only in 1 population. 

![alt_txt][LD.freq]
[LD.freq]:https://cloud.githubusercontent.com/assets/12142475/20882899/89ca3c0c-bae3-11e6-8c5d-bc0a23f47ca5.png

In >1pop: 

![alt_txt][LD.freq.HighFreq]
[LD.freq.HighFreq]:https://cloud.githubusercontent.com/assets/12142475/20882937/c19ea5dc-bae3-11e6-868d-fb6e65d46097.png



So LD between loci are mostly only in 1 population. The max is 5 (1 locus). So I will not filter any of these loci out. 


HWE
Make sure that all the missing data are also filtered out of the datasets. Using the old script above, only the fixed loci per population are removed, but the ungenotyped loci remain. So there will be gaps in the .hwe file, and this can't be imported into R. 
```
##Linux
for i in $(ls popnames.plink.folder/); do plink --file ld.subset.data/$i.var.plink --maf 0.005 --out ld.subset.data/$ivar.plink; done
for i in $(ls popnames.plink.folder/); do plink --file ld.subset.data/$i.var.plink --hardy --out ld.subset.data/$i; done

##R
setwd('/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE133/')
hwe.subset.path <- './ld.subset.data/'
hwe.files <- list.files(hwe.subset.path, pattern='hwe$')
hwe.files
setwd('./ld.subset.data')
hwe.files.list <- lapply(hwe.files, read.table, header=T)
names(hwe.files.list) <- hwe.files  ##name each dataframe

#subset the data to include only TEST=="ALL"
hwe.files.list <- lapply(hwe.files.list, subset, TEST=="ALL")

#create a list of df with all loci out of HWE and O.Het>0.5
hwe.files.list.remove <- lapply(hwe.files.list, subset, (P<0.050001|O.HET.>0.5))

attach(hwe.files.list.remove)
hwe.remove.freq <- do.call(rbind, lapply(names(hwe.files.list.remove), get))
summary(hwe.remove.freq)
detach(hwe.files.list.remove)

hwe.remove.freq.freq <- data.frame(table(hwe.remove.freq$SNP)) 
summary(hwe.remove.freq.freq)
hist(hwe.remove.freq.freq$Freq, xlab="Nr pops", ylab="Frequency", main="Frequency of loci deviating from HWE and O.Het>0.5 across 15 pops")

```

![alt_txt][HWE.freq]
[HWE.freq]:https://cloud.githubusercontent.com/assets/12142475/20883013/26f22882-bae4-11e6-8232-8189b4ed0e08.png


153 Loci to remove
```
#R
HWE.loc.remove <- subset(hwe.remove.freq.freq, Freq>5)
summary(HWE.loc.remove)

setwd("/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE133/")
write.table(HWE.loc.remove$Var1, "HWE.loc.remove", col.name=F, row.name=F, quote=F)


#Linux
plink --file SE151.s3.plink --exclude HWE.loc.remove --recode --recodeA --out SE151.s4.plink

#2081SNPs
```


Convert to vcf using pgdspider






Missingness
```
vcftools --vcf SE151.s4.vcf --recode --recode-INFO-all --out SE151.s4
vcftools --vcf SE151.s4.recode.vcf --missing-indv

##R
setwd('/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE133/')
library(ggplot2)
SE.151.s4 <- read.table("out.imiss", header=T)
pop <- read.table("SE151.popnames", header=T)
SE.151.s4$pop <- pop$pop
SE.151.s4$pop.order <- pop$pop.order

SE.151.s4.sort <- SE.151.s4[order(SE.151.s4$pop.order),]

SE.151.s4.sort$pop <- factor(SE.151.s4.sort$pop, levels=SE.151.s4.sort$pop)   ##sort pop.nr. Numbers from south to North

qplot(pop, F_MISS, data=SE.151.s4.sort, geom=c("boxplot", "jitter"))
```

![alt_txt][missing.s4]
[missing.s4]:https://cloud.githubusercontent.com/assets/12142475/20883961/0ad3a2f2-bae9-11e6-8a81-eabc03bbc9fe.png


remove individuals with >40% missingness
```
SE151.imiss.40 <- subset(SE.151.s4, F_MISS>0.4)
summary(SE151.imiss.40)
write.table(SE151.imiss.40$INDV, "SE151.imiss40", col.name=F, row.name=F, quote=F)

#LINUX
vcftools --vcf SE151.s4.recode.vcf --remove SE151.imiss40 --recode --recode-INFO-all --out SE132.FINAL

##rename individuals
bcftools reheader SE132.FINAL.recode.vcf -s names.SE132.FINAL --out SE132.FINAL.vcf
```




```
#convert to plink
vcftools --vcf SE132.FINAL.vcf --plink --out SE132.FINAL.plink
plink --file SE132.FINAL.plink --recode --recodeA --out SE132.FINAL.plink

##subset into populations
mkdir FINAL.subset.data

for i in $(ls popnames.plink.folder/); do plink --file SE132.FINAL.plink --keep popnames.plink.folder/$i --recode --recodeA --out FINAL.subset.data/$i.FINAL.plink; done

for i in $(ls popnames.plink.folder/); do plink --file FINAL.subset.data/$i.FINAL.plink --maf 0.05 --recode --recodeA --out FINAL.subset.data/$i.FINAL.var.plink; done

for i in $(ls popnames.plink.folder/); do plink --file FINAL.subset.data/$i.FINAL.var.plink --r2 --out FINAL.subset.data/$i.FINAL.var; done

for i in $(ls popnames.plink.folder/); do plink --file FINAL.subset.data/$i.FINAL.var.plink --freq --out FINAL.subset.data/$i.FINAL.var; done


##R: Final graphs of per population SFS and LD
##LD
setwd('/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE133')
FINAL.subset.path <- './FINAL.subset.data/'
FINAL.ld.files <- list.files(FINAL.subset.path, pattern='ld$')
setwd('/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE133/FINAL.subset.data/')
FINAL.ld.files.list <- lapply(FINAL.ld.files, read.table, header=T)
names(FINAL.ld.files.list) <- FINAL.ld.files
names(FINAL.ld.files.list)

##plot ld for each population
par(mfrow=c(3,5))
for(i in names(FINAL.ld.files.list))
{
    df1 = as.data.frame(FINAL.ld.files.list[[i]])
    hist(df1$R2, main=i)
}

##SFS
setwd('/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE133/')
FINAL.subset.path <- './FINAL.subset.data/'
FINAL.sfs.files <- list.files(FINAL.subset.path, pattern='var.frq$')
setwd('/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE133/FINAL.subset.data/')
FINAL.sfs.files.list <- lapply(FINAL.sfs.files, read.table, header=T)
names(FINAL.sfs.files.list) <- FINAL.sfs.files
names(FINAL.sfs.files.list)

##plot ld for each population
par(mfrow=c(3,5))
for(i in names(FINAL.sfs.files.list))
{
    df1 = as.data.frame(FINAL.sfs.files.list[[i]])
    hist(df1$MAF, main=i)
}


```

![alt_txt][FINAL.ld.perpop]
[FINAL.ld.perpop]:https://cloud.githubusercontent.com/assets/12142475/20884085/a7e47616-bae9-11e6-8fa2-101aedd4ab8d.png

![alt_txt][FINAL.sfs.perpop]
[FINAL.sfs.perpop]:https://cloud.githubusercontent.com/assets/12142475/20884119/c8e27f5c-bae9-11e6-92e9-cc4e460af2e6.png


####FINAL DATA

SE132.FINAL.plink /.vcf

132 individuals

2081 loci

15 populations

6 regions


##Average Heterozygosity

/Volumes/Beelzebufo/SEFinalSamples/outfiles/SEFinalc94d6m4p3.phy

To calculate average heterozygosity, I am using the phylip file from pyrad output. This is the unfiltered data.

For the SE193 data the total number of characters in each sequence (from the *nex outfile):

45170301

In linux, count the number of each character

```
#Missing data
tr -d -c 'N\n'< SEFinalc94d6m4p3.phy |awk '{print length; }'
##gaps
sed 's/[^-]//g' SEFinalc94d6m4p3.phy |awk '{print length}'

#Transitions
tr -d -c 'R\n'< SEFinalc94d6m4p3.phy |awk '{print length; }'
tr -d -c 'Y\n'< SEFinalc94d6m4p3.phy |awk '{print length; }'

#Transversions
tr -d -c 'S\n'< SEFinalc94d6m4p3.phy |awk '{print length; }'
tr -d -c 'W\n'< SEFinalc94d6m4p3.phy |awk '{print length; }'
tr -d -c 'K\n'< SEFinalc94d6m4p3.phy |awk '{print length; }'
tr -d -c 'M\n'< SEFinalc94d6m4p3.phy |awk '{print length; }'

##Print out all the names in the phylip file
grep -Eo '^[^ ]+' SEFinalc94d6m4p3.phy 
```

I copied all the names and read counts into an excel sheet and saved it in R:

```
##Counts file. This I concatenated in R from all the individual counts files. 
.../SE193.Het.counts
```

Read everything into R

```
counts <- read.table("SE193.Het.counts", header=T)

pop <- read.table("pop.SE193")  ##second column indicating which samples were used in the final data

counts.pop <- cbind(counts, pop$pop)
```

Calculate the average heterozygosity and Ti/Tv rate
```
counts$missing <- (counts$N + counts$dash)
counts$Ti <- (counts$R + counts$Y)
counts$Tv <- (counts$S + counts$W + counts$K + counts$M)
counts$nt.sequenced <- (45170301-counts$missing)
counts$avgHet <- (counts$Ti+counts$Tv)/counts$nt.sequenced
counts$Ti.Tv <- (counts$Ti/counts$Tv)

clean.counts <- subset(counts, FinalSamples=="Yes")  ##select samples in final dataset
```

and keep only the samples in the final dataset. And write the table (this includes only 132 individuals)

Based on Miller et al. 2013 (Estimating genome-wide heterozygosity:effects of demographic history and marker type), it seems like ~300 loci are needed for accurate estimates of heterozygosity. But this is for estimates in inbred populations (in this case a sheep).

Accurate estimates depend on the amount of inbreeding - i.e. the co-variation between loci due to IBD (Identity Disequilibrium) - i.e. the more ID the less loci needed to accurately estimate heterozygosity.

Schmeller et al. 2007 show correlation between heterozygosity and age, particularly in stressful environments. Heterozygosity has also been associated with larval development in Rana temporaria.

I can't find counts of genome-wide heterozygosity for any outbred populations...


Draw Final plots:

Avg het vs latitude

```
attach(clean.counts.2000.sort)
dat.sort <- clean.counts.2000.sort[order(Transect, elev),]
dat.sort$pop <- factor(dat.sort$pop, levels=dat.sort$pop)  ##make pop an ordered factor so that ggplot doesn't reorder

q <- qplot(pop, avgHet, fill=factor((elev.level), levels=c("Low","Mid","High")), data=dat.sort, geom="boxplot") 
pdf(file="SE.AvgHet.20170214.pdf")
q + theme_bw()+theme(legend.title=element_blank()) + geom_point() +theme(axis.text.x = element_text(angle = 90, hjust = 1))
dev.off()
```

![alt_txt][AvgHet.Final]
[AvgHet.Final]:https://cloud.githubusercontent.com/assets/12142475/22998727/56604d06-f3d8-11e6-8331-b75c3b3ccbed.png



##Population Structure

###Fst

Convert plink FINAL to structure using pgdspider, and import into R
```
/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/SumStats

library(adegenet)
library(hierfstat)
library(reshape)

SEall.132 <- read.structure("SE132.FINAL.str")
SEall.132

pop.SE132 <- read.table("popnames.SE132.structure", header=T)  ##make sure the populations are numbered "X1.Sk.Ho", etc.
pop.factor <- as.factor(pop.SE132$pop)
SEall.132@pop <- pop.factor

hier.SEall <- genind2hierfstat(SEall.132)

SEall.fst <- pairwise.fst(SEall.132, pop=NULL, res.type=c("dist", "matrix"))

m <- as.matrix(SEall.fst)
m2 <- melt(m)[melt(upper.tri(m))$value,]
names(m2)<- c("c1","c2", "distance")

library(gplots)

shadesOfGrey <- colorRampPalette(c("grey100", "grey0"))  ##define the colourpalette. 

Dend <- read.table("heatmap.popcolours", header=T)  ##list of colour names for each population based on R colour palatte. In alphabetical order (as in genind file)
Dend.Colours <- as.character(Dend$colours.pop)

par(oma=c(1,1,2,1))
heatmap.2(as.matrix(SEall.fst), trace="none", RowSideColors=Dend.Colours, ColSideColors=Dend.Colours, col=shadesOfGrey, labRow=F, labCol=F, key.ylab=NA, key.xlab=NA, key.title="Fst Colour Key", keysize=0.9, main="Pairwise Fst of SEall: 15pops, 6regions, 2081loci")  ##RowSideColors is for the dendrogram on the row, ColSideColors for the upper dendrogram. Colour order should be the same as the input. The pop order is alphabetical in the output. 
par(fig = c(0, 1, 0, 1), oma = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), new = TRUE)
plot(0, 0, type = "n", bty = "n", xaxt = "n", yaxt = "n")

popnames.all <- as.character(Dend$pop)
legend("bottom", popnames.all, xpd = TRUE, horiz = TRUE, inset = c(0, 0), bty="o", pch=15, col=Dend.Colours, title="Side Dendrogram:Region")
```

![alt_txt][Fst.SEall]
[Fst.SEall]:https://cloud.githubusercontent.com/assets/12142475/20884888/26b43e00-baee-11e6-9f84-3067feac8af8.png

####Frequency distributions of Fst per locus

This is to find loci that explain differentiation between populations more than neutral expectation


```
#Convert plink .ped files to structure using pgdspider
#create population files 

#R
library(adegenet)
library(hierfstat)
SE132.genind <- read.structure("SE132.plink.str")
hier.SE132 <- genind2hierfstat(SE132.genind)
SE132.genind@pop
SE132.pop <- read.table("SE132.pop", header=T)
SE132.pop.factor <- as.factor(SE132.pop$pop)
SE132.genind@pop <- SE132.pop.factor
SE132.genind@pop
hier.SE132 <- genind2hierfstat(SE132.genind)
stats.SE132 <- basic.stats(hier.SE132)
stats.SE132.perlocus <- stats.SE132$perloc

hist(stats.SE132.perlocus$Fst, xlim=c(-0.2, 1.0), breaks=120)
```

![alt_txt][Fst.perloc]
[Fst.perloc]:https://cloud.githubusercontent.com/assets/12142475/20885005/ce2dd128-baee-11e6-83be-a043cb171c9c.png


Write table with names of loci with Fst>0.5
```
SE132.Fstouliers <- subset(stats.SE132.perlocus, Fst>0.5)
summary(SE132.Fstouliers)

write.table(SE132.Fstouliers, "SE132.Fstoutliers", quote=F, sep=" ")
```

63 outliers in total from the per locus Fst

###IBD

Fst/(1-Fst) vs log.dist(km) - according to Rousset et al. 1997, this is correlated with the effective population density (De) and effective dispersal distance (variance)

```
##Fst 15 pops -> Fst/(1-Fst)

library(reshape)
library(fields)


m <- as.matrix(SEall.fst)
m
m2 <- melt(m)[melt(upper.tri(m))$value,]
names(m2) <- c("c1", "c2", "distance")
m2
m2$IBD <- m2$distance/(1-m2$distance)


SE.pop.coords <- read.table("SE.coords.15pop", header=T)
SEpop_lon.lat <- cbind(SE.pop.coords$Long, SE.pop.coords$Lat)
distance.matrix.SEpop <- rdist.earth(SEpop_lon.lat, miles=F)  ##great circle dist based on the coordinates
m.dist <- as.matrix(distance.matrix.SEpop)
summary(m.dist)

m2.dist <- melt(m.dist)[melt(upper.tri(m.dist))$value,]
names(m2.dist) <- c("c1", "c2", "distance")
summary(m2.dist)
m2.dist$log.km <- log(m2.dist$distance)


library(MASS)
#dens <- kde2d(m2$IBD,m2.dist$log.km, n=10)
#myPal <- colorRampPalette(c("white","blue","gold", "orange", "red"))
plot(m2$IBD~m2.dist$log.km, pch=20,cex=.5, xlab="log Geographic distance (km)", ylab="Fst/(1-Fst)")
#image(dens, col=transp(myPal(10),.7), add=TRUE)
abline(fit <- lm(m2$IBD~m2.dist$log.km))
legend("bottomright", bty="n", legend=paste("R2 =", format(summary(fit)$adj.r.squared, digits=4)))  ##and paste R2
title("Isolation by distance plot - SEall")
```

![alt_txt][IBD.SEall]
[IBD.SEall]:https://cloud.githubusercontent.com/assets/12142475/20885224/f1579fc0-baef-11e6-9f99-fcd105098e03.png



###AMOVA

http://grunwaldlab.github.io/Population_Genetics_in_R/AMOVA.html

I initially ran the AMOVA including within sample variation. But this is calculated from haplotypes without phasing info for my data.
So I'm excluding this level. 
Also note that loci are excluded if they have more than 5% missing data. This is 1288 loci for my dataset. 

Here is the new analysis (old one follows with figures). 
```
library(adegenet)
library(poppr)

SE132.strata.2 <- pop.SE132[,2:3]  ##from text file. each column has one hierarchy level specified for all individuals. (indiv, pop, region)
SEall.132.genind.2 <- SEall.132
SEall.132.genind.2@strata <- NULL ##renamed the SEall.132 genind to a better name
SEall.132.genind.2@strata <- SE132.strata.2

SEall.132.genclone.2 <- as.genclone(SEall.132.genind.2)

SE132.amova2 <- poppr.amova(SEall.132.genclone.2, ~region/pop, within=F, quiet=T)

SE132.amova2 
#$call
#ade4::amova(samples = xtab, distances = xdist, structures = xstruct)

#$results
#                               Df    Sum Sq   Mean Sq
#Between region                  5  6859.496 1371.8991
#Between samples Within region   9  2223.396  247.0440
#Within samples                117 16631.191  142.1469
#Total                         131 25714.083  196.2907
#
#$componentsofcovariance
#                                              Sigma          %
#Variations  Between region                 52.19375  25.232503
#Variations  Between samples Within region  12.51059   6.048109
#Variations  Within samples                142.14693  68.719389
#Total variations                          206.85128 100.000000
#
#$statphi
#                          Phi
#Phi-samples-total  0.31280611
#Phi-samples-region 0.08089222
#Phi-region-total   0.25232503


SE132.amova2.test <- randtest(SE132.amova2, nrepet=1000)

SE132.amova2.test

#class: krandtest 
#Monte-Carlo tests
#Call: randtest.amova(xtest = SE132.amova2, nrepet = 1000)
#
#Number of tests:   3 
#
#Adjustment method for multiple comparisons:   none 
#Permutation number:   1000 
#                        Test       Obs    Std.Obs   Alter      Pvalue
#1  Variations within samples 142.14693 -42.403620    less 0.000999001
#2 Variations between samples  12.51059  20.457302 greater 0.000999001
#3  Variations between region  52.19375   6.061697 greater 0.000999001

#other elements: adj.method call 

##plot results
plot(SE132.amova2.test)
```

![alt_txt][AMOVA.real]
[AMOVA.real]:https://cloud.githubusercontent.com/assets/12142475/23068061/57d1c840-f522-11e6-8edc-ab5908983a2e.png






OLD analysis
```
library(adegenet)
library(poppr)

SE132.strata <- pop.SE132[,1:3]  ##from text file. each column has one hierarchy level specified for all individuals. (indiv, pop, region)
SEall.132.genind <- SEall.132
SEall.132.genind@other <- SE132.strata ##renamed the SEall.132 genind to a better name

strata(SEall.132.genind) <- other(SEall.132.genind)

SEall.132.genclone <- as.genclone(SEall.132.genind)

SE132.amova <- poppr.amova(SEall.132.genclone, ~region/pop)

SE132.amova 
$call
ade4::amova(samples = xtab, distances = xdist, structures = xstruct)

$results
                            Df    Sum Sq    Mean Sq
Between region               5  5624.345 1124.86904
Between pop Within region    9  1563.566  173.72952
Between samples Within pop 117 10348.789   88.45119
Within samples             132 11068.430   83.85174
Total                      263 28605.130  108.76475

$componentsofcovariance
                                            Sigma          %
Variations  Between region              22.075461  19.481964
Variations  Between pop Within region    5.085375   4.487929
Variations  Between samples Within pop   2.299721   2.029542
Variations  Within samples              83.851744  74.000566
Total variations                       113.312301 100.000000

$statphi
                         Phi
Phi-samples-total 0.25999434
Phi-samples-pop   0.02669392
Phi-pop-region    0.05573818
Phi-region-total  0.19481964

SE132.amovatest <- randtest(SE145.amova, nrepet=999)

SE132.amovatest

class: krandtest 
Monte-Carlo tests
Call: randtest.amova(xtest = SE132.amova, nrepet = 999)

Number of tests:   4 

Adjustment method for multiple comparisons:   none 
Permutation number:   999 
                        Test       Obs    Std.Obs   Alter Pvalue
1  Variations within samples 83.851744 -15.211099    less  0.001
2 Variations between samples  2.299721   1.405146 greater  0.079
3     Variations between pop  5.085375  20.240561 greater  0.001
4  Variations between region 22.075461   6.671363    less  1.000


##Randomised samples
SE132.random <- SEall.132.genclone
set.seed(9001)
strata(SE132.random) <- strata(SEall.132.genclone)[sample(nInd(SEall.132.genclone)), -1]
head(strata(SE132.random))
head(strata(SEall.132.genclone))
SE132.random.amova <- poppr.amova(SE132.random, ~region/pop)

SE132.random.amova   ##now all the variation is within samples and within populations. So no population structure evident. 
$call
ade4::amova(samples = xtab, distances = xdist, structures = xstruct)

$results
                            Df     Sum Sq   Mean Sq
Between region               5   699.3817 139.87634
Between pop Within region    9  1263.0980 140.34422
Between samples Within pop 117 15574.2199 133.11299
Within samples             132 11068.4302  83.85174
Total                      263 28605.1298 108.76475

$componentsofcovariance
                                              Sigma            %
Variations  Between region              -0.02962594  -0.02720872
Variations  Between pop Within region    0.43121766   0.39603415
Variations  Between samples Within pop  24.63062339  22.62098440
Variations  Within samples              83.85174396  77.01019018
Total variations                       108.88395908 100.00000000

$statphi
                            Phi
Phi-samples-total  0.2298980982
Phi-samples-pop    0.2270472519
Phi-pop-region     0.0039592642
Phi-region-total  -0.0002720872


SE132.random.amovatest<- randtest(SE132.random.amova, nrepet=999) 

SE132.random.amovatest

class: krandtest 
Monte-Carlo tests
Call: randtest.amova(xtest = SE132.random.amova, nrepet = 999)

Number of tests:   4 

Adjustment method for multiple comparisons:   none 
Permutation number:   999 
                        Test         Obs     Std.Obs   Alter Pvalue
1  Variations within samples 83.85174396 -16.4837507    less  0.001
2 Variations between samples 24.63062339  14.8702325 greater  0.001
3     Variations between pop  0.43121766   0.6740932 greater  0.228
4  Variations between region -0.02962594  -0.2969393    less  0.409

other elements: adj.method call 


##plot both outputs
par(mfrow=c(2,1))
plot(SE132.amovatest)
plot(SE132.random.amovatest)
```

![alt_txt][AMOVA.SE145]
[AMOVA.SE145]:https://cloud.githubusercontent.com/assets/12142475/20885592/0b398014-baf2-11e6-83da-db379a4e48bb.png

![alt_txt][AMOVA.random]
[AMOVA.random]:https://cloud.githubusercontent.com/assets/12142475/20885605/1570c218-baf2-11e6-9a21-867c35fa379f.png


Structure between pops and between regions is higher than expected with random mating. 


###DAPC

tutorial-dapc: A tutorial for Discriminant Analysis of Principal Components (DAPC) using adegenet 2.0.0

total variance = (variance between groups) + (variance within groups)

or more simply, denoting X the data matrix:

VAR(X) = B(X) +W(X)

Usual approaches such as Principal Component Analysis (PCA) or Principal Coordinates Analysis (PCoA / MDS) focus on V AR(X). That is, they only describe the global diversity, possibly overlooking differences between groups. On the contrary, DAPC optimizes B(X) while minimizing W(X): it seeks synthetic variables, the discriminant functions, which show differences between groups as best as possible while minimizing variation within clusters.

```
##1. estimate the number of clusters

Using k-means. Which finds the number of clusters with minimises W(X) and maximises B(X). Compare using BIC

Run algorithm on PCA transformed data. I.e. reduce the dataset so that it can run faster. 

grp.SE132 <- find.clusters(SEall.132.genind, max.n.clust=40)

> choose nr of PCs: 200  ##I try to keep all the PCs

> choose k: 4 ##i used K4-6. See table below


names.15 <- c("Sk.Ho", "Sk.SF", "SK.SL", "Upp.Gra", "Upp.K", "Upp.O", "Um.Gr", "Um.Taf", "Um.UT3", "LT1", "LT2", "LT3", "Kir.G", "Kir.L", "FIN")
names.15 <- as.character(names.15)
table.value(table(pop(SEall.132.genind), grp.SE132$grp), col.lab=paste("inf", 1:4), row.lab=names.15)

dapc1.SE132 <- dapc(SEall.132.genind, grp.SE132$grp)
scatter(dapc1.SE132)
```

Nr of PCs: I kept 200

![alt_txt][PCs.initial]
[PCs.initial]:https://cloud.githubusercontent.com/assets/12142475/20885874/6f66c258-baf3-11e6-94da-a3fad9b0410a.png

Choose K: 4
![alt_txt][K.initial]
[K.initial]:https://cloud.githubusercontent.com/assets/12142475/20885882/7a3a1c0c-baf3-11e6-851c-221b9f0cc479.png


![alt_txt][K4]
[K4]:https://cloud.githubusercontent.com/assets/12142475/20885881/7a38ce56-baf3-11e6-95f1-1a33b6223d51.png

![alt_txt][K5]
[K5]:https://cloud.githubusercontent.com/assets/12142475/20885924/ba25f318-baf3-11e6-99c1-331cc8f2d088.png

![alt_txt][K6]
[K6]:https://cloud.githubusercontent.com/assets/12142475/20885923/ba255b92-baf3-11e6-96c5-14aa12ecd613.png



###PCA

PCAdapt in R:
```
##convert .vcf to plink 
##linux

vcftools --vcf SE.132.FINAL.recode.vcf --plink --out SE132.FINAL.plink

plink --file SE132.FINAL.plink --recode --recodeA --out SE132.FINAL.plink

##R
library(pcadapt)

SE.132 <- read.pcadapt("SE132.FINAL.plink.ped", type="ped")
Summary:

        - input file      SE145.FINAL.plink.ped
        - output file     SE145.FINAL.plink.pcadapt

	- number of individuals detected:	132
	- number of loci detected:		2081

File has been sucessfully converted.

##Check the nr of PCs

x <- pcadapt(SE.132, K=20)

Reading file SE132.FINAL.plink.pcadapt...
Number of SNPs: 2081
Number of individuals: 132
Number of SNPs with minor allele frequency lower than 0.05 ignored: 54
17046 out of 274692 missing data ignored.

plot(x, option="screeplot")  ##PC for pop structure = on the steep curve
```

Choose number of PCs:

![alt_txt][PCadapt.fig1]
[PCadapt.fig1]:https://cloud.githubusercontent.com/assets/12142475/20884529/1ede27c4-baec-11e6-8593-6cc980d39b90.png

K 4-5 

Plot the PCA using population information
```
pop.SE132  <- read.table("pop.SE132", header=T)
head(pop.SE132)
        indiv        pop region
1      FIN_21     X6.FIN    FIN
2      FIN_22     X6.FIN    FIN
3      FIN_23     X6.FIN    FIN
4      FIN_25     X6.FIN    FIN

poplist <- as.character(pop.SE132[,2])  ##select pops
plot(x,option="scores",pop=poplist)

poplist <- as.character(pop.SE132[,3]) ##select regions
plot(x,option="scores",pop=poplist)
```

Populations
![alt_txt][PCadapt.Fig2]
[PCadapt.Fig2]:https://cloud.githubusercontent.com/assets/12142475/20884791/ae0970d8-baed-11e6-82c3-9829e173bc27.png


Regions
![alt_txt][PCadapt.Fig3]
[PCadapt.Fig3]:https://cloud.githubusercontent.com/assets/12142475/20884801/b290de20-baed-11e6-9354-e428878eb8d7.png



##fastStructure



##TESS3


#Env Associations

##BIOclim variables

Find the uncorrelated variables from BIOclim: 

Extract BIOclim variables. See: https://github.com/alexjvr1/Manuscripts/blob/master/6.R.temp_SE.MS1.md


```
library(corrplot)
library(caret)
datMy <- read.table("SEonly.BIOclim", header=T)
datMy.scale <- scale(datMy[1:ncol(datMy)], center=T, scale=T)
corMatMy <- cor(datMy.scale, method="spearman")
corrplot(corMatMy, order="hclust", tl.cex=1)

highlyCor <- findCorrelation(corMatMy, 0.80) #After inspection, apply correlation filter at 0.70,

#then we remove all the variable correlated with more 0.8.
datMyFiltered.scale <- datMy.scale[,-highlyCor]
corMatMy <- cor(datMyFiltered.scale)
corrplot(corMatMy, order = "hclust", tl.cex=1)

```

![alt_txt][BIOclim.all]
[BIOclim.all]:https://cloud.githubusercontent.com/assets/12142475/20832670/d86ef6c2-b88b-11e6-9ad7-e06544448f51.png

![alt_txt][BIOclim.reduced]
[BIOclim.reduced]:https://cloud.githubusercontent.com/assets/12142475/20832675/dde218c8-b88b-11e6-8158-0eabc1f40338.png





##RDA
Input files:

MAF of all loci

Geographic coordinates

Climate variables
	
	1. BIO5: Mat temp warmest month
	
	2. BIO15: Precipitation seasonality
	
	3. BIO13: Precipitation Wettest month
	
	4. BIO18: Precipitation warmest quarter (correlated with growth season)
	
	5. BIO2: Mean diurnal range

2081 loci


```
###1. MAF

#Calculate MAF for the full dataset within region using PLINK

/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/RDA/

###Use the *nosex file to create a file for subsetting the data. Here I've made 1=within region, 2=within pop

plink --file SE132.FINAL.plink --freq --within SE132.region.cluster --out SE132.cluster

plink --file SE132.FINAL.plink --freq --within SE132.pop.cluster --out SE132.pop


###R
######Reformat PLINK output
###MAF for each locus -> melt and reformat rows as pops, and columns as loci.

SE.MAF <- read.table("SE132.pop.frq.strat", header=T)
SE.MAF2 <- SE.MAF[,c(3,2,6)]
summary(SE.MAF2)
         CLST               SNP             MAF        
 X1.Sk.Ho  : 2081   100865:23 :   15   Min.   :0.0000  
 X1.Sk.SF  : 2081   101108:100:   15   1st Qu.:0.0000  
 X1.Sk.SL  : 2081   101142:72 :   15   Median :0.1111  
 X2.Upp.Gra: 2081   101270:3  :   15   Mean   :0.2028  
 X2.Upp.K  : 2081   101367:84 :   15   3rd Qu.:0.3333  
 X2.Upp.O  : 2081   101609:30 :   15   Max.   :1.0000  
 (Other)   :18729   (Other)   :31125     

library("ggplot2")
library("reshape2")

SE.MAF3 <- melt(SE.MAF2, id.vars = c("CLST", "SNP"), variable_name = c("MAF"))
str(SE.MAF3)
head(SE.MAF3)


SE132.MAF4 <- dcast(SE.MAF3, formula= CLST ~ SNP)
head(SE145.MAF4)



##Add X infront of all locusnames. 
colnames(SE132.MAF4) <- paste("X", colnames(SE132.MAF4), sep=".")
write.csv(SE132.MAF4, file="SE.132.MAF.csv")
```


Run RDA

See this tutorial for the interpretation: REDUNDANCY ANALYSIS TUTORIAL: Landscape Genetics Paul Gugger redundancy-analysis-for-landscape-genetics.pdf on mac
```
library(vegan)

GenData <- read.csv("SE.132.MAF.csv", header=T)
GenData <- GenData[,15:2055]

Climate.Data <- read.csv("SE.145.MAF.csv", header=T)
names(Climate.Data)
Climate.variables <- Climate.Data[,1:14]
names(Climate.variables)
Climate.Data <- Climate.variables[,10:14]
Climate.Data$Lat <- Climate.variables$Lat
Climate.Data$Long <- Climate.variables$Long


##1. Run Full RDA model to determine how much of the variation is explainable by the expanatory variables we have
##H0: climate data does not affect genotype

RDA.SEfull <- rda(GenData ~ Lat + Long +bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled, Climate.Data) 
RDA.SEfull
Call: rda(formula = GenData ~ Lat + Long + bio5.scaled + bio15.scaled +
bio13.scaled + bio18.scaled + bio2.scaled, data = Climate.Data)

              Inertia Proportion Rank
Total         80.7640     1.0000     
Constrained   61.8140     0.7654    7
Unconstrained 18.9499     0.2346    7
Inertia is variance 

Eigenvalues for constrained axes:
  RDA1   RDA2   RDA3   RDA4   RDA5   RDA6   RDA7 
25.969 13.331  7.957  4.612  4.274  3.625  2.046 

Eigenvalues for unconstrained axes:
  PC1   PC2   PC3   PC4   PC5   PC6   PC7 
5.153 3.555 2.913 2.256 1.959 1.732 1.381 

anova(RDA.SEfull)

Permutation test for rda under reduced model
Permutation: free
Number of permutations: 999

Model: rda(formula = GenData ~ Lat + Long + bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled, data = Climate.Data)
         Df Variance      F Pr(>F)    
Model     7   68.333 3.2732  0.001 ***
Residual  7   20.876                  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

##to see which variables are most important, we can plot the results in a biplot

plot(RDA.SEfull)

```

![alt_txt][RDA.SEfull]
[RDA.SEfull]:https://cloud.githubusercontent.com/assets/12142475/20886240/61ca36c8-baf5-11e6-9b9d-420e099bac18.png



```
##Partial out geog
H0: Climate does not explain genetic data

pRDA.geog <- rda(GenData~bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled+ Condition(Lat + Long), Climate.Data)
head(summary(pRDA.geog))

anova(pRDA.geog)

Permutation test for rda under reduced model
Permutation: free
Number of permutations: 999

Model: rda(formula = GenData ~ bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled + Condition(Lat + Long), data = Climate.Data)
         Df Variance      F Pr(>F)   
Model     5   33.803 2.2668  0.003 **
Residual  7   20.876                 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


H0 rejected: Climate does explain GeneticData

pRDA.geog
Call: rda(formula = GenData ~ bio5.scaled + bio15.scaled + bio13.scaled
+ bio18.scaled + bio2.scaled + Condition(Lat + Long), data =
Climate.Data)

              Inertia Proportion Rank
Total         89.2091     1.0000     
Conditional   34.5300     0.3871    2
Constrained   33.8026     0.3789    5
Unconstrained 20.8765     0.2340    7
Inertia is variance 

Eigenvalues for constrained axes:
  RDA1   RDA2   RDA3   RDA4   RDA5 
15.549  9.094  4.488  2.712  1.960 

Eigenvalues for unconstrained axes:
  PC1   PC2   PC3   PC4   PC5   PC6   PC7 
6.190 3.872 3.460 2.250 2.179 1.662 1.264 


plot(pRDA.geog, main="pRDA (geog partialled out)")
```

![alt_txt][pRDA.geog]
[pRDA.geog]:https://cloud.githubusercontent.com/assets/12142475/20886394/22ed46a6-baf6-11e6-9d8b-41208c033d03.png


```
##Partial out climate

H0: Geog alone does not explain Genetic data

pRDA.climate <- rda(GenData~Lat+Long + Condition(bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled), Climate.Data)
pRDA.climate
Call: rda(formula = GenData ~ Lat + Long + Condition(bio5.scaled +
bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled), data =
Climate.Data)

               Inertia Proportion Rank
Total         89.20911    1.00000     
Conditional   60.53395    0.67856    5
Constrained    7.79869    0.08742    2
Unconstrained 20.87648    0.23402    7
Inertia is variance 

Eigenvalues for constrained axes:
 RDA1  RDA2 
5.364 2.435 

Eigenvalues for unconstrained axes:
  PC1   PC2   PC3   PC4   PC5   PC6   PC7 
6.190 3.872 3.460 2.250 2.179 1.662 1.264 


head(summary(pRDA.climate))

anova(pRDA.climate)

Permutation test for rda under reduced model
Permutation: free
Number of permutations: 999

Model: rda(formula = GenData ~ Lat + Long + Condition(bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled), data = Climate.Data)
         Df Variance      F Pr(>F)
Model     2   7.7987 1.3075  0.173
Residual  7  20.8765          

H0: not rejected -> Geography alone does not explain GenData

plot(pRDA.climate, main="pRDA (climate partialled out)")
```

![alt_txt][pRDA.climate]
[pRDA.climate]:https://cloud.githubusercontent.com/assets/12142475/20886517/a00af9b2-baf6-11e6-85f8-10ed240ea369.png


Find the most important loci associated with Climate
```
 summary(pRDA.geog)
 
Aattccumulated constrained eigenvalues
Importance of components:
                       RDA1  RDA2   RDA3    RDA4    RDA5
Eigenvalue            15.55 9.094 4.4877 2.71164 1.96009
Proportion Explained   0.46 0.269 0.1328 0.08022 0.05799
Cumulative Proportion  0.46 0.729 0.8618 0.94201 1.00000

                 RDA1     RDA2    RDA3     RDA4     RDA5 PC1
bio5.scaled   0.10875 -0.16439  0.2365 -0.07146  0.10935   0
bio15.scaled  0.51115  0.23431  0.3214 -0.04714 -0.05866   0
bio13.scaled  0.47356 -0.23902 -0.3016 -0.17095 -0.53324   0
bio18.scaled  0.47439 -0.06841  0.2092  0.02536 -0.26091   0
bio2.scaled  -0.05063 -0.13750  0.2565 -0.06603 -0.02417   0

RDA1: BIO13, BIO15, BIO18

RDA2: BIO13, BIO15
```

##LFMM

See tutorial for command line version: http://membres-timc.imag.fr/Olivier.Francois/lfmm/files/LEA_1.html

/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/LFMM/SE132.FINAL

Input files

1. .env environmental file

2. Genotype file
	
	2.1. 2081 loci (all loci)
	
	2.2. only loci variable in >5 populations


1. Env file
```
#
env <- read.table("SE132.norm.envvariables", header=F) ###read in the environmental data. One line perindividual. Same order as .vcf file
write.env(env, "SE133.env")   ##convert to correct .env format

```


2. Genotype file. Convert .vcf to .lfmm in LEA
```
library(LEA)
genotype = vcf2lfmm("SE132.FINAL.vcf")

	- number of detected individuals:	132
	- number of detected loci:		2081

For SNP info, please check ./SE145.FINAL.vcfsnp.

0 line(s) were removed because these are not SNPs.
Please, check ./SE145.FINAL.removed file, for more informations.

```


###Population structure with SNMF

The first step is to evaluate the population structure. SNMF works on the same principles as STRUCTURE, but is much faster. 

Test K1:10
```
obj.snmf = snmf(genotype, K=1:10, entropy=T, ploidy=2, project="new")

##
The project is saved into :
 SE132.FINAL.snmfProject 

To load the project, use:
 project = load.snmfProject("SE132.FINAL.snmfProject")

To remove the project, use:
 remove.snmfProject("SE132.FINAL.snmfProject")
```

Since the program was called with the entropy option, we can plot the values of the cross-entropy criterion for each K. The value for which the function plateaus or increases is our estimate of K

```
plot(obj.snmf)

```
![alt_txt][snmf2081]
[snmf2081]:https://cloud.githubusercontent.com/assets/12142475/20887077/6ce017a4-baf9-11e6-858f-d4918824192e.png




We observe that the minimum value is around K=5. We can also visualize a barplot of ancestry coeffcients as follows.

```
barplot(t(Q(obj.snmf, K=5)), col=1:5)
```

![alt_txt][barplot2041]
[barplot2041]:https://cloud.githubusercontent.com/assets/12142475/20887148/c74b3a7a-baf9-11e6-9cf1-709b181f71c7.png



###Run LFMM

on fgcz47 

```
scp * fgcz47:/srv/kenlab/alexjvr_p1795/SE.Analyses/LFMM.Dec2016/SE.2041.lfmm/

##R
obj.lfmm = lfmm("SE132.FINAL.lfmm", "SE132.env", K=5, rep=5, project="new")


export.lfmmProject(obj.lfmmProject)   ##this creates a .zip file in the current directory, which can easily be transferred to other folders. 

import.lfmmProject(obj.lfmmProject) ##to import the project into R from the new folder
```
move the project back onto the mac

/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/LFMM/SE132.FINAL

And calculate the corrected p-values of all the loci. 

#####Genomic inflation (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3137506/)

https://bioinformaticsngs.wordpress.com/2016/03/08/genomic-inflation-factor-calculation/

The genomic inflation factor λgc  is defined as the ratio of the median of the empirically observed distribution of the test statistic to the expected median, thus quantifying the extent of the bulk inflation and the excess false positive rate. In other words, λgc is defined as the median of the resulting chi-squared test statistics divided by the expected median of the chi-squared distribution. The median of a chi-squared distribution with one degree of freedom is 0.4549364 (qchisq(0.5,1) in R). A λgc value can be calculated from z-scores, chi-square statistics, or p-values, depending on the output you have from the association analysis.

Population structure, including population stratification and cryptic relatedness, can cause spurious associations in genome-wide association studies (GWAS). Usually, the scaled median or mean test statistic for association calculated from multiple single-nucleotide-polymorphisms across the genome is used to assess such effects, and ‘genomic control' can be applied subsequently to adjust test statistics at individual loci by a genomic inflation factor. Published GWAS have clearly shown that there are many loci underlying genetic variation for a wide range of complex diseases and traits, implying that a substantial proportion of the genome should show inflation of the test statistic. Here, we show by theory, simulation and analysis of data that in the absence of population structure and other technical artefacts, but in the presence of polygenic inheritance, substantial genomic inflation is expected. Its magnitude depends on sample size, heritability, linkage disequilibrium structure and the number of causal variants. Our predictions are consistent with empirical observations on height in independent samples of ∼4000 and ∼133 000 individuals

Normally we divide by the genomic inflation factor 0.456 to get lambda. Lambda should be close to 1, and the adj.p-value distribution should be flat. If this is not correct, then we can increase the genomic inflation factor to obtain a lambda closer to 1. (since the genomic inflation factor is known to be conservative). 
Here I have adjusted the values to 0.85, except for d3, which is adjusted by 0.45. 

```
zs.d1 <- z.scores(SE132.lfmm, K=5, d=1)
zs.d1.median =apply(zs.d1, MARGIN=1, median)


lambda=median(zs.d1.median^2)/0.4549364
lambda
	1] 1.928976
lambda=median(zs.d1.median^2)/0.85
lambda
	[1] 1.034839
adj.p.values.d1 =pchisq(zs.d1.median^2/0.85, df=1, lower=F)
q = 0.05
L = length(adj.p.values.d1)
w = which(sort(adj.p.values.d1) < q * (1:L) / L)
candidates.d1.k5 = order(adj.p.values.d1)[w]



zs.d2 <- z.scores(SE132.lfmm, K=5, d=2)
zs.d2.median =apply(zs.d2, MARGIN=1, median)
lambda=median(zs.d2.median^2)/0.4549364
lambda
	[1] 1.946945
lambda=median(zs.d2.median^2)/0.85
lambda
	[1] 1.044479
adj.p.values.d2 =pchisq(zs.d2.median^2/0.85, df=1, lower=F)

q = 0.05
L = length(adj.p.values.d2)
w = which(sort(adj.p.values.d2) < q * (1:L) / L)
candidates.d2.k5 = order(adj.p.values.d2)[w]




zs.d3 <- z.scores(SE132.lfmm, K=5, d=3)
zs.d3.median =apply(zs.d3, MARGIN=1, median)
lambda=median(zs.d3.median^2)/0.4549364
lambda
	[1] 1.034471
	
lambda=median(zs.d3.median^2)/0.85
lambda
	[1] 0.5549635
adj.p.values.d3 =pchisq(zs.d3.median^2/0.4549364, df=1, lower=F)
q = 0.05
L = length(adj.p.values.d3)
w = which(sort(adj.p.values.d3) < q * (1:L) / L)
candidates.d3.k5 = order(adj.p.values.d3)[w]




zs.d4 <- z.scores(SE132.lfmm, K=5, d=4)
zs.d4.median =apply(zs.d4, MARGIN=1, median)
lambda=median(zs.d4.median^2)/0.4549364
lambda
	[1] 1.492845
lambda=median(zs.d4.median^2)/0.85
lambda
	[1] 0.8008673
adj.p.values.d4 =pchisq(zs.d4.median^2/0.85, df=1, lower=F)

q = 0.05
L = length(adj.p.values.d4)
w = which(sort(adj.p.values.d4) < q * (1:L) / L)
candidates.d4.k5 = order(adj.p.values.d4)[w]



zs.d5 <- z.scores(SE132.lfmm, K=5, d=5)
zs.d5.median =apply(zs.d5, MARGIN=1, median)
lambda=median(zs.d5.median^2)/0.4549364
lambda
	[1] 2.888479
lambda=median(zs.d5.median^2)/0.85
lambda
	[1] 1.034839
adj.p.values.d5 =pchisq(zs.d5.median^2/0.85, df=1, lower=F)

q = 0.05
L = length(adj.p.values.d5)
w = which(sort(adj.p.values.d5) < q * (1:L) / L)
candidates.d5.k5 = order(adj.p.values.d5)[w]



par(mfrow=c(3,2))
hist(adj.p.values.d1)
hist(adj.p.values.d2)
hist(adj.p.values.d3)
hist(adj.p.values.d4)
hist(adj.p.values.d5)



```

![alt_txt][hist.adj.p.values]
[hist.adj.p.values]:https://cloud.githubusercontent.com/assets/12142475/22503337/906866fe-e871-11e6-9052-8d13ad5ccdc4.png


Note that I am not calculating the FDR and TP as is suggested in the LFMM tutorial. I get really strange results. This could be because the p-value distributions are too liberal, but according to everything I've read about the BH FDR adjustment, my FDR should be what I have specified. Here 5%. 

See slide 15 here: http://www.stat.cmu.edu/~genovese/talks/hannover1-04.pdf

I just realised that the FDR and TP calculations are only done on the simulated datasets - so to prove that the LFMM method is conservative enough. Thats why id doesnt work on my data - because its impossible to determine what the real FDR is. I just know that it should be below my threshold of 5%. 


Select all the candidates from the full list of SNPs
```
#read the SNP names into R

locus.names <- read.table("SE132.plink.map", header=F)
colnames(locus.names$V2) <- "SNP"
locus.names$ID <- seq.int(nrow(locus.names)) #add an index of the SNP numbers, since the LFMM output is a numbered list corresponding to the original genotype input order
candidates.d1.k5 <- as.character(candidates.d1.k5)  ##change the list of candidates from LFMM output to a list of characters
candidates.d1.k5.names <- locus.names[locus.names$ID %in% candidates.d1.k5,]  ##select from locus.names$ID the rows that match candidates vector

candidates.d1.k5.names <- paste("X", candidates.d1.k5.names$SNP, sep=".") #rename the SNPs so that they don't get renamed in excel


candidates.d2.k5 <- as.character(candidates.d2.k5)  ##change the list of candidates from LFMM output to a list of characters
candidates.d2.k5.names <- locus.names[locus.names$ID %in% candidates.d2.k5,]  ##select from locus.names$ID the rows that match candidates vector

candidates.d2.k5.names <- paste("X", candidates.d2.k5.names$SNP, sep=".") #rename the SNPs so that they don't get renamed in excel




candidates.d3.k5 <- as.character(candidates.d3.k5)  ##change the list of candidates from LFMM output to a list of characters
candidates.d3.k5.names <- locus.names[locus.names$ID %in% candidates.d3.k5,]  ##select from locus.names$ID the rows that match candidates vector

candidates.d3.k5.names <- paste("X", candidates.d3.k5.names$SNP, sep=".") #rename the SNPs so that they don't get renamed in excel



candidates.d4.k5 <- as.character(candidates.d4.k5)  ##change the list of candidates from LFMM output to a list of characters
candidates.d4.k5.names <- locus.names[locus.names$ID %in% candidates.d4.k5,]  ##select from locus.names$ID the rows that match candidates vector

candidates.d4.k5.names <- paste("X", candidates.d4.k5.names$SNP, sep=".") #rename the SNPs so that they don't get renamed in excel



candidates.d5.k5 <- as.character(candidates.d5.k5)  ##change the list of candidates from LFMM output to a list of characters
candidates.d5.k5.names <- locus.names[locus.names$ID %in% candidates.d5.k5,]  ##select from locus.names$ID the rows that match candidates vector

candidates.d5.k5.names <- paste("X", candidates.d5.k5.names$SNP, sep=".") #rename the SNPs so that they don't get renamed in excel

```

Draw a VennDiagram of all the overlapping loci associated with the different environmental variables (LFMM results only)
```
d12 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d2.k5.names))
d12
d13 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d3.k5.names))
d14 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d4.k5.names))
d15 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d5.k5.names))
d23 <- Reduce(intersect, list(candidates.d2.k5.names, candidates.d5.k3.names))
d23 <- Reduce(intersect, list(candidates.d2.k5.names, candidates.d3.k5.names))
d24 <- Reduce(intersect, list(candidates.d2.k5.names, candidates.d4.k5.names))
d25 <- Reduce(intersect, list(candidates.d2.k5.names, candidates.d5.k5.names))
d34 <- Reduce(intersect, list(candidates.d3.k5.names, candidates.d4.k5.names))
d35 <- Reduce(intersect, list(candidates.d3.k5.names, candidates.d5.k5.names))
d45 <- Reduce(intersect, list(candidates.d4.k5.names, candidates.d5.k5.names))
d123 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d2.k5.names,candidates.d3.k5.names))
d124 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d2.k5.names,candidates.d4.k5.names))
d125 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d2.k5.names,candidates.d5.k5.names))
d234 <- Reduce(intersect, list(candidates.d2.k5.names, candidates.d3.k5.names,candidates.d4.k5.names))
d134 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d3.k5.names,candidates.d4.k5.names))
d135 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d3.k5.names,candidates.d5.k5.names))
d145 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d4.k5.names,candidates.d5.k5.names))
d235 <- Reduce(intersect, list(candidates.d2.k5.names, candidates.d3.k5.names,candidates.d5.k5.names))
d245 <- Reduce(intersect, list(candidates.d2.k5.names, candidates.d4.k5.names,candidates.d5.k5.names))
d345 <- Reduce(intersect, list(candidates.d3.k5.names, candidates.d4.k5.names,candidates.d5.k5.names))

d2345 <- Reduce(intersect, list(candidates.d2.k5.names, candidates.d3.k5.names,
candidates.d4.k5.names, candidates.d5.k5.names))
d1245 <- Reduce(intersect, list(candidates.d2.k5.names, candidates.d1.k5.names,
candidates.d4.k5.names, candidates.d5.k5.names))
d1345 <- Reduce(intersect, list(candidates.d3.k5.names, candidates.d1.k5.names,
candidates.d4.k5.names, candidates.d5.k5.names))
d12345 <- Reduce(intersect, list(candidates.d1.k5.names, candidates.d2.k5.names, candidates.d3.k5.names, 
candidates.d4.k5.names, candidates.d5.k5.names))


draw.quintuple.venn(area1=200, area2=153, area3=198, area4=90, area5=393,
n12=29, n13=67, n14=28, n15=44, n23=26, n24=16, n25=91, n34=42, n35=56, n45=61,
n123=10, n124=8, n125=10, n134=19, n135=21, n145=17, n234=7, n235=10, n245=11, n345=29,
n1234=5, n1235=3, n1245=3, n1345=14, n2345=4, n12345=2, 
category=c("bio5", "bio15", "bio13", "bio18", "bio2"),
lty="blank", 
fill=c("yellow", "orange", "skyblue1", "skyblue3", "blue")
)
```

Prepare the data to incorporate with the Fst outlier graph
```
colnames(d1.names) <- "names"
colnames(d2.names) <- "names"
colnames(d3.names) <- "names"
colnames(d4.names) <- "names"
colnames(d5.names) <- "names"

d12345.names <- rbind(d1.names, d2.names, d3.names, d4.names, d5.names)  ##Join all data.frames by "name" column. This only works of colnames are the same (at least one column name)

d12345.names <- lapply(d12345.names, unique)  #select only the unique rows. 

d12345.names <- sub(":", ".", d12345.names$names) ##replace the ":" in the locus names so that they're in the same format as the Fst and RDA lists

write.table(d12345.names$d12345.names, "LFMM.alloutliers", col.names=F, row.names=F, quote=F)

##linux.
##copy the list over to /Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/SumStats

#Read into R

colnames(LFMM.outliers) <- ("loci")
LFMM.outliers <- as.character(LFMM.outliers$loci)
colnames(LFMM.outliers) <- ("loci")
LFMM.outliers <- as.character(LFMM.outliers$loci)

```


##BayEnv2

BayEnv2 should be one of the best Fst outlier analyses. I will attempt to run it on the SE data. 

Location: /Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/BayEnv2


Input files: 

https://bitbucket.org/tguenther/bayenv2_public/src/8e4039f64d61?at=default

1. Input file with subset of 500 SNPs for co-variance matrix (pop structure)

2. Input file with all SNPs for the association analysis

3. ENV input file with normalised environmental parameters. I will first use only temperature.

NB: population order in the input files should all be the same.

###1. Large input

Convert using pgdspider. Specify populations. Individual names can be obtained from the vcf file: 

```
bcftools queryl -l file.vcf

```

Add a second column specifying the number of populations. If there is a mistake here, BayEnv2 will fail with the error: segmentation fault 11. 


###2. 500SNPs

First convert to genepop using pgspider. Select the first 500SNPs in linux as follows: 
```
sed -n -e `2,501p` SE132.FINAL.2081.genepop.txt > 500loci.names
```

Then select these snps from the file.vcf, and convert to bayenv2 format using pgdspider. (remember to specify the same populations with the spid.popfile as with the full dataset). 

```
vcftools --vcf SE132.FINAL.vcf --snps 500loci.names --recode --recode-INFO-all --out SE132.500loci
```


###Step1: Estimate co-variance matrix. 

This runs fairly quickly (~2hours for 132 indivs, 500loci), so I ran it on the mac. 

The command needs to be run from the installation folder: /Users/alexjvr/Applications/bayenv2/compiled_on_a_mac

```
cp /Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/BayEnv2/SE132.500loci.bayenv.txt .

./bayenv2 -i SE132.500loci.bayenv.txt -p 15 -k 100000 r23468 > SE132.500.matrix.out

./bayenv2 -i SE132.500loci.bayenv.txt -p 15 -k 100000 r67328 > SE132.500.matrix2.out
```


Check that the covariance matrix are highly correlated within and between runs. 

I will copy 10 matrices randomly from each of the two runs, and visualise their correlation in R. As well as compare this with the pairwise Fst (this can be used to double check the order of the populations in the matrix). 

I made new files for a matrix every 10k steps from both runs. ie. 10 matrices per run. 

/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/BayEnv2/inputfiles/SE500.matrix

```
matrix1.10 <- read.table("matrix1.10")
matrix1.10 <- as.matrix(matrix1.10)

matrix2.10 <- read.table("matrix2.10")
matrix2.10 <- as.matrix(matrix2.10)

par(mfrow=c(1,2))
image(matrix1.10)
image(matrix2.10)


cor.test(matrix1.10, matrix2.10)

	Pearson's product-moment correlation

data:  matrix1.10 and matrix2.10
t = 132.21, df = 223, p-value < 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.9917881 0.9951397
sample estimates:
      cor 
0.9936817 

##plot
par(mfrow=c(1,2))
image(matrix1.10)
image(matrix2.10)
```

![alt_txt][SE500.matrix]
[SE500.matrix]:https://cloud.githubusercontent.com/assets/12142475/22548890/f9df9f08-e948-11e6-90be-0811346d5b27.png


And draw dist trees for the last 5 matrices of both runs to check that the same result was obtained. 
```
##calculate a distance
diss6 <- 1-cor(matrix1.6)
diss7 <- 1-cor(matrix1.7)
diss8 <- 1-cor(matrix1.8)
diss9 <- 1-cor(matrix1.9)
diss10 <- 1-cor(matrix1.10)
diss6.2 <- 1-cor(matrix2.6)
diss7.2 <- 1-cor(matrix2.7)
diss8.2 <- 1-cor(matrix2.8)
diss9.2 <- 1-cor(matrix2.9)
diss10.2 <- 1-cor(matrix2.10)

##change the df type
dist6 <- as.dist(diss6)
dist7 <- as.dist(diss7)
dist8 <- as.dist(diss8)
dist9 <- as.dist(diss9)
dist10 <- as.dist(diss10)
dist2.6 <- as.dist(diss2.6)
dist2.6 <- as.dist(diss6.2)
dist2.7 <- as.dist(diss7.2)
dist2.8 <- as.dist(diss8.2)
dist2.9 <- as.dist(diss9.2)
dist2.10 <- as.dist(diss10.2)


##plot and inspect
par(mfrow=c(2,5))

plot(hclust(dist6), main="matrix1.6", xlab="")
plot(hclust(dist7), main="matrix1.7", xlab="")
plot(hclust(dist8), main="matrix1.8", xlab="")
plot(hclust(dist9), main="matrix1.9", xlab="")
plot(hclust(dist10), main="matrix1.10", xlab="")
plot(hclust(dist2.6), main="matrix2.6", xlab="")
plot(hclust(dist2.7), main="matrix2.7", xlab="")
plot(hclust(dist2.8), main="matrix2.8", xlab="")
plot(hclust(dist2.9), main="matrix2.9", xlab="")
plot(hclust(dist2.10), main="matrix2.10", xlab="")

```

![alt_txt][tree.corrmat]
[tree.corrmat]:https://cloud.githubusercontent.com/assets/12142475/23128141/1235cdb4-f77e-11e6-9f68-67258b04564b.png



open the .csv. Copy and paste the environmental data to a new file on the server *.ENV Do this to ensure that all the spacing works. This was a problem before, so check on this!

###Run bayenv2

In the folder on the server:

1. bayenv2 executable (make sure this is the linux version

2. .ENV input

3. .MATRIX input

4. .bayenv.txt input file (converted with pgdspider).

First split the input file into individual loci

```
split -d -a 10 -l 2 SE132.FINAL.2081.bayenv.txt snp_batch
```


This creates files with allele counts per locus.

and loop bayenv through all input files:

```
for f in $(ls snp_batch*); do ./bayenv2 -i $f -e ENVIRONFILE.SE132.15pops.env -m SE132.MATRIX -k 100000 -p 15 -n 5 -r $RANDOM -t -c -X; done 
```

Ran 3 independent runs with different (Random) seeds. 

Download all the results to /Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/BayEnv2

###Compare convergence between runs. 

I need to calculate convergence of BF, p, and ranking 1000 loci between the 3 runs for each of the 5 environmental variables. (i.e. 45 correlations)

In R: 
```
BF.run1 <- read.table("BayEnv_Run1/bf_environ.ENVIRONFILE.SE132.15pops.env", header=F)
colnames(BF.run1) <- c("snp", "bio2.BF", "bio2.rho", "bio2.r", "bio5.BF", "bio5.rho", "bio5.p",
"bio13.BF", "bio13.rho", "bio13.r", "bio15.BF", "bio15.rho", "bio15.r", "bio18.BF", "bio18.rho",
"bio18.r"
)

BF.run2 <- read.table("BayEnv_Run2/bf_environ.ENVIRONFILE.SE132.15pops.env", header=F)
colnames(BF.run2) <- c("snp", "bio2.BF", "bio2.rho", "bio2.r", "bio5.BF", "bio5.rho", "bio5.p",
"bio13.BF", "bio13.rho", "bio13.r", "bio15.BF", "bio15.rho", "bio15.r", "bio18.BF", "bio18.rho",
"bio18.r"
)

BF.run3 <- read.table("BayEnv_Run3/bf_environ.ENVIRONFILE.SE132.15pops.env", header=F)
colnames(BF.run3) <- c("snp", "bio2.BF", "bio2.rho", "bio2.r", "bio5.BF", "bio5.rho", "bio5.p",
"bio13.BF", "bio13.rho", "bio13.r", "bio15.BF", "bio15.rho", "bio15.r", "bio18.BF", "bio18.rho",
"bio18.r"
)

```

Correlation between log BF of different runs: 
```
cor.test(log(BF.run1$bio2.BF), log(BF.run2$bio2.BF))
cor.test(log(BF.run1$bio2.BF), log(BF.run3$bio2.BF))
cor.test(log(BF.run2$bio2.BF), log(BF.run3$bio2.BF))

cor.test(log(BF.run1$bio5.BF), log(BF.run2$bio5.BF))
cor.test(log(BF.run1$bio5.BF), log(BF.run3$bio5.BF))
cor.test(log(BF.run2$bio5.BF), log(BF.run3$bio5.BF))

cor.test(log(BF.run1$bio13.BF), log(BF.run2$bio13.BF))
cor.test(log(BF.run1$bio13.BF), log(BF.run3$bio13.BF))
cor.test(log(BF.run2$bio13.BF), log(BF.run3$bio13.BF))

cor.test(log(BF.run1$bio15.BF), log(BF.run2$bio15.BF))
cor.test(log(BF.run1$bio15.BF), log(BF.run3$bio15.BF))
cor.test(log(BF.run2$bio15.BF), log(BF.run3$bio15.BF))

cor.test(log(BF.run1$bio18.BF), log(BF.run2$bio18.BF))
cor.test(log(BF.run1$bio18.BF), log(BF.run3$bio18.BF))
cor.test(log(BF.run2$bio18.BF), log(BF.run3$bio18.BF))
```

Correlation between empirical p (Spearmans rho) 
```
cor.test(BF.run1$bio2.rho, BF.run2$bio2.rho)
cor.test(BF.run1$bio2.rho, BF.run3$bio2.rho)
cor.test(BF.run2$bio2.rho, BF.run3$bio2.rho)

cor.test(BF.run1$bio5.rho, BF.run2$bio5.rho)
cor.test(BF.run1$bio5.rho, BF.run3$bio5.rho)
cor.test(BF.run2$bio5.rho, BF.run3$bio5.rho)

cor.test(BF.run2$bio13.rho, BF.run3$bio13.rho)
cor.test(BF.run1$bio13.rho, BF.run3$bio13.rho)
cor.test(BF.run1$bio13.rho, BF.run2$bio13.rho)

cor.test(BF.run1$bio15.rho, BF.run2$bio15.rho)
cor.test(BF.run1$bio15.rho, BF.run3$bio15.rho)
cor.test(BF.run2$bio15.rho, BF.run3$bio15.rho)

cor.test(BF.run1$bio18.rho, BF.run2$bio18.rho)
cor.test(BF.run1$bio18.rho, BF.run3$bio18.rho)
cor.test(BF.run2$bio18.rho, BF.run3$bio18.rho)
```

Compare ranking SNPs 5% at a time
```
bio2.run1.top100 <- BF.run1[order(-BF.run1$bio2.rho),]  ##descending order by rho
bio2.run2.top100 <- BF.run2[order(-BF.run2$bio2.rho),]  ##descending order by rho
bio2.run3.top100 <- BF.run3[order(-BF.run3$bio2.rho),]  ##descending order by rho


##overlap in top 5% of outliers loci for bio2
###############
bio2.run1.top100.set1 <- head(bio2.run1.top100, 100)
bio2.run2.top100.set1 <- head(bio2.run2.top100, 100)
bio2.run3.top100.set1 <- head(bio2.run3.top100, 100)
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run3.top100.set1$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run3.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.12
#83

bio2.13
#88

bio2.23
#83

bio2.123
#78

##overlap in 6-10% of outlier loci for bio2
###############
bio2.run1.top100.set2 <- bio2.run1.top100[101:200,]
bio2.run2.top100.set2 <- bio2.run2.top100[101:200,]
bio2.run3.top100.set2 <- bio2.run3.top100[101:200,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.123

##overlap in 11-15% of outlier loci for bio2
###############
bio2.run1.top100.set2 <- bio2.run1.top100[201:300,]
bio2.run2.top100.set2 <- bio2.run2.top100[201:300,]
bio2.run3.top100.set2 <- bio2.run3.top100[201:300,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.123



#######
##overlap in top 5% of outliers loci for bio5
###############

bio5.run1.top100 <- BF.run1[order(-BF.run1$bio5.rho),]  ##descending order by rho
bio5.run2.top100 <- BF.run2[order(-BF.run2$bio5.rho),]  ##descending order by rho
bio5.run3.top100 <- BF.run3[order(-BF.run3$bio5.rho),]  ##descending order by rho

bio2.run1.top100.set1 <- bio5.run1.top100[1:100,]
bio2.run2.top100.set1 <- bio5.run2.top100[1:100,]
bio2.run3.top100.set1 <- bio5.run3.top100[1:100,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run3.top100.set1$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run3.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.123


##overlap in 6-10% of outlier loci for bio5
###############
bio2.run1.top100.set2 <- bio5.run1.top100[101:200,]
bio2.run2.top100.set2 <- bio5.run2.top100[101:200,]
bio2.run3.top100.set2 <- bio5.run3.top100[101:200,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.123

##overlap in 11-15% of outlier loci for bio5
###############
bio2.run1.top100.set2 <- bio5.run1.top100[201:300,]
bio2.run2.top100.set2 <- bio5.run2.top100[201:300,]
bio2.run3.top100.set2 <- bio5.run3.top100[201:300,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.123


#######
##overlap in top 5% of outliers loci for bio13
###############

bio13.run1.top100 <- BF.run1[order(-BF.run1$bio13.rho),]  ##descending order by rho
bio13.run2.top100 <- BF.run2[order(-BF.run2$bio13.rho),]  ##descending order by rho
bio13.run3.top100 <- BF.run3[order(-BF.run3$bio13.rho),]  ##descending order by rho

bio2.run1.top100.set1 <- bio13.run1.top100[1:100,]
bio2.run2.top100.set1 <- bio13.run2.top100[1:100,]
bio2.run3.top100.set1 <- bio13.run3.top100[1:100,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run3.top100.set1$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run3.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.123


##overlap in 6-10% of outlier loci for bio13
###############
bio2.run1.top100.set2 <- bio13.run1.top100[101:200,]
bio2.run2.top100.set2 <- bio13.run2.top100[101:200,]
bio2.run3.top100.set2 <- bio13.run3.top100[101:200,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.123

##overlap in 11-15% of outlier loci for bio13
###############
bio2.run1.top100.set2 <- bio13.run1.top100[201:300,]
bio2.run2.top100.set2 <- bio13.run2.top100[201:300,]
bio2.run3.top100.set2 <- bio13.run3.top100[201:300,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.123


#########
##overlap in 5% of outlier loci for bio15
###############
bio15.run1.top100 <- BF.run1[order(-BF.run1$bio15.rho),]  ##descending order by rho
bio15.run2.top100 <- BF.run2[order(-BF.run2$bio15.rho),]  ##descending order by rho
bio15.run3.top100 <- BF.run3[order(-BF.run3$bio15.rho),]  ##descending order by rho

bio2.run1.top100.set1 <- bio15.run1.top100[1:100,]
bio2.run2.top100.set1 <- bio15.run2.top100[1:100,]
bio2.run3.top100.set1 <- bio15.run3.top100[1:100,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run3.top100.set1$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run3.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.123


##overlap in 6-10% of outlier loci for bio15
###############
bio2.run1.top100.set2 <- bio15.run1.top100[101:200,]
bio2.run2.top100.set2 <- bio15.run2.top100[101:200,]
bio2.run3.top100.set2 <- bio15.run3.top100[101:200,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.123

##overlap in 11-15% of outlier loci for bio15
###############
bio2.run1.top100.set2 <- bio15.run1.top100[201:300,]
bio2.run2.top100.set2 <- bio15.run2.top100[201:300,]
bio2.run3.top100.set2 <- bio15.run3.top100[201:300,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.123


##########
##overlap in 5% of outlier loci for bio18
###############

bio18.run1.top100 <- BF.run1[order(-BF.run1$bio18.rho),]  ##descending order by rho
bio18.run2.top100 <- BF.run2[order(-BF.run2$bio18.rho),]  ##descending order by rho
bio18.run3.top100 <- BF.run3[order(-BF.run3$bio18.rho),]  ##descending order by rho

bio2.run1.top100.set1 <- bio18.run1.top100[1:100,]
bio2.run2.top100.set1 <- bio18.run2.top100[1:100,]
bio2.run3.top100.set1 <- bio18.run3.top100[1:100,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run3.top100.set1$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set1$snp, bio2.run3.top100.set1$snp, bio2.run1.top100.set1$snp))
bio2.123


##overlap in 6-10% of outlier loci for bio18
###############
bio2.run1.top100.set2 <- bio18.run1.top100[101:200,]
bio2.run2.top100.set2 <- bio18.run2.top100[101:200,]
bio2.run3.top100.set2 <- bio18.run3.top100[101:200,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.123

##overlap in 11-15% of outlier loci for bio18
###############
bio2.run1.top100.set2 <- bio18.run1.top100[201:300,]
bio2.run2.top100.set2 <- bio18.run2.top100[201:300,]
bio2.run3.top100.set2 <- bio18.run3.top100[201:300,]
bio2.12 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.13 <- Reduce(intersect, list(bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.23 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp))
bio2.123 <- Reduce(intersect, list(bio2.run2.top100.set2$snp, bio2.run3.top100.set2$snp, bio2.run1.top100.set2$snp))
bio2.123
```


#####Run more BayEnv

According to Blair et al. 2014, there is a lot of inter-run variation in BayEnv2 results. They suggest that the best approach (if the covariance matrix converges) is to combine results from several different runs. 

Im following the methods in Božičević 2016 (Mol Ecol) to get the most robust results: 

"
To test the convergence of Bayenv2, we used several independent Markov chain Monte Carlo runs with a maximum chain length of 10 000 iterations. We observed convergence after about 5000 iterations (Fig. S1, Supporting information). However, these chains might converge to different solutions. To be most stringent, we used the median results from 10 independent runs (Blair et al. 2014). We then tested for correlations between each single SNP and six environmental variables: geographical latitude, height above mean sea level and four temperature measures (average daily minimum of the coldest and warmest month, and average daily minimum and maximum throughout the year) (Table 2). The results for environmental variables are given as Bayes factors (BFs). A higher BF gives higher support to the model where the environmental variable has a significant effect on allele frequency distribution over an alternative model with no effect (Coop et al. 2010). Similar as above, we finally report the median BF of ten independent runs of each SNP that has been described to improve the proportion of false positives (Blair et al. 2014; Lotterhos & Whitlock 2014). With BF values, we also used a resampling approach, analogously to the one applied on FST. We randomly sampled sets of SNPs of the same size from the genomic background and then assessed the null distribution for BF of CCRT-, RSS- and SR-associated SNPs.
"


Ive run 10 independend BayEnv2 association analyses. now Im combining all the output tables by the median of all the values. 

Read in the rest of the data.frames (BF.run1 - 3 read in before)
```
BF.run4 <- read.table("BayEnv_Run4/bf_environ.ENVIRONFILE.SE132.15pops.env", header=F)
colnames(BF.run4) <- c("snp", "bio2.BF", "bio2.rho", "bio2.r", "bio5.BF", "bio5.rho", "bio5.p",
"bio13.BF", "bio13.rho", "bio13.r", "bio15.BF", "bio15.rho", "bio15.r", "bio18.BF", "bio18.rho",
"bio18.r"
)

BF.run5 <- read.table("BayEnv_Run5/bf_environ.ENVIRONFILE.SE132.15pops.env", header=F)
colnames(BF.run5) <- c("snp", "bio2.BF", "bio2.rho", "bio2.r", "bio5.BF", "bio5.rho", "bio5.p",
"bio13.BF", "bio13.rho", "bio13.r", "bio15.BF", "bio15.rho", "bio15.r", "bio18.BF", "bio18.rho",
"bio18.r"
)

BF.run6 <- read.table("BayEnv_Run6/bf_environ.ENVIRONFILE.SE132.15pops.env", header=F)
colnames(BF.run6) <- c("snp", "bio2.BF", "bio2.rho", "bio2.r", "bio5.BF", "bio5.rho", "bio5.p",
"bio13.BF", "bio13.rho", "bio13.r", "bio15.BF", "bio15.rho", "bio15.r", "bio18.BF", "bio18.rho",
"bio18.r"
)

BF.run7 <- read.table("BayEnv_Run7/bf_environ.ENVIRONFILE.SE132.15pops.env", header=F)
colnames(BF.run7) <- c("snp", "bio2.BF", "bio2.rho", "bio2.r", "bio5.BF", "bio5.rho", "bio5.p",
"bio13.BF", "bio13.rho", "bio13.r", "bio15.BF", "bio15.rho", "bio15.r", "bio18.BF", "bio18.rho",
"bio18.r"
)

BF.run8 <- read.table("BayEnv_Run8/bf_environ.ENVIRONFILE.SE132.15pops.env", header=F)
colnames(BF.run8) <- c("snp", "bio2.BF", "bio2.rho", "bio2.r", "bio5.BF", "bio5.rho", "bio5.p",
"bio13.BF", "bio13.rho", "bio13.r", "bio15.BF", "bio15.rho", "bio15.r", "bio18.BF", "bio18.rho",
"bio18.r"
)

BF.run9 <- read.table("BayEnv_Run9/bf_environ.ENVIRONFILE.SE132.15pops.env", header=F)
colnames(BF.run9) <- c("snp", "bio2.BF", "bio2.rho", "bio2.r", "bio5.BF", "bio5.rho", "bio5.p",
"bio13.BF", "bio13.rho", "bio13.r", "bio15.BF", "bio15.rho", "bio15.r", "bio18.BF", "bio18.rho",
"bio18.r"
)

BF.run10 <- read.table("BayEnv_Run10/bf_environ.ENVIRONFILE.SE132.15pops.env", header=F)
colnames(BF.run10) <- c("snp", "bio2.BF", "bio2.rho", "bio2.r", "bio5.BF", "bio5.rho", "bio5.p",
"bio13.BF", "bio13.rho", "bio13.r", "bio15.BF", "bio15.rho", "bio15.r", "bio18.BF", "bio18.rho",
"bio18.r"
)

```

Calculate a final matrix as the median of all the matrices
```
BF.all <- rbindlist(list(BF.run1, BF.run2, BF.run3, BF.run4, BF.run5, BF.run6, BF.run7, BF.run8, 
BF.run9, BF.run10))[,lapply(.SD,median), list(snp)]
```

Identify the outliers associated with the different env variables. 
"
Strength of evidence for significant associations was based on the value of the log10 Bayes factor (log10BF), with the following log10BF cut-offs: 0.5–1= substantial evidence; 1–2 = strong evidence; >2 =decisive (Kass & Raftery 1995). The linear model underlying the Bayes factor might not be correct or outliers within our data might misguide the model (bayenv2.0 manual, https://bitbucket.org/tguenther/bayenv2_public/src). To deal with this, bayenv2 also calculates the nonparametric Spearman's rank correlation coefficient, ρ. SNPs with a log10BF >0.5 as well as an absolute value of ρ > 0.3 (where ρ ranges from −1 to 1) were therefore considered as robust candidates demonstrating signatures of selection.
"  Christmas et al. 2016

```
#calculate the log10 of BF for each environmental variable

BF.all$bio2.log10BF <- log10(BF.all$bio2.BF)
BF.all$bio5.log10BF <- log10(BF.all$bio5.BF)
BF.all$bio13.log10BF <- log10(BF.all$bio13.BF)
BF.all$bio15.log10BF <- log10(BF.all$bio15.BF)
BF.all$bio18.log10BF <- log10(BF.all$bio18.BF)

##Sort and plot

BF.all.sort <- BF.all[order(BF.all$bio2.log10BF),]
par(mfrow=c(3,2))
plot(BF.all.sort$bio2.log10BF, main="bio2: Mean diurnal range")
abline(h=0.5, col=4, lty=2)
abline(h=1.0, col=3, lty=2)
BF.all.sort <- BF.all[order(BF.all$bio5.log10BF),]
plot(BF.all.sort$bio5.log10BF, main="bio5: Max temp warmest mnth")
abline(h=0.5, col=4, lty=2)
abline(h=1.0, col=3, lty=2)
BF.all.sort <- BF.all[order(BF.all$bio5.log10BF),]
BF.all.sort <- BF.all[order(BF.all$bio13.log10BF),]
plot(BF.all.sort$bio13.log10BF, main="bio13: Precip wettest mnth")
abline(h=0.5, col=4, lty=2)
abline(h=1.0, col=3, lty=2)
```

![alt_txt][bayenv.fig1]
[bayenv.fig1]:https://cloud.githubusercontent.com/assets/12142475/22776460/05cd6810-eeb0-11e6-8678-954e44f5c8d6.png


```
##plot BF vs absolute p value

BF.all.sort <- BF.all[order(BF.all$bio2.log10BF),]
plot(BF.all.sort$bio2.log10BF~(abs(BF.all.sort$bio2.rho)), main="bio2: Mean diurnal range")
abline(h=0.5, col=4, lty=2)
abline(h=1.0, col=3, lty=2)
abline(v=0.3, col=3, lty=2)

BF.all.sort <- BF.all[order(BF.all$bio5.log10BF),]
plot(BF.all.sort$bio5.log10BF~(abs(BF.all.sort$bio5.rho)), main="bio5: Max temp warmest mnth")
abline(h=0.5, col=4, lty=2)
abline(h=1.0, col=3, lty=2)
abline(v=0.3, col=3, lty=2)

BF.all.sort <- BF.all[order(BF.all$bio13.log10BF),]
plot(BF.all.sort$bio13.log10BF~(abs(BF.all.sort$bio13.rho)), main="bio13: Precip wettest mnth")
abline(h=0.5, col=4, lty=2)
abline(h=1.0, col=3, lty=2)
abline(v=0.3, col=3, lty=2)

BF.all.sort <- BF.all[order(BF.all$bio15.log10BF),]
plot(BF.all.sort$bio15.log10BF~(abs(BF.all.sort$bio15.rho)), main="bio15: Precip Seasonality")
abline(h=0.5, col=4, lty=2)
abline(h=1.0, col=3, lty=2)
abline(v=0.3, col=3, lty=2)

BF.all.sort <- BF.all[order(BF.all$bio18.log10BF),]
plot(BF.all.sort$bio18.log10BF~(abs(BF.all.sort$bio18.rho)), main="bio18: Precip warmest Q")
abline(h=0.5, col=4, lty=2)
abline(h=1.0, col=3, lty=2)
abline(v=0.3, col=3, lty=2)

```

![alt_txt][bayenv.fig2]
[bayenv.fig2]:https://cloud.githubusercontent.com/assets/12142475/22776468/0d0a3f5e-eeb0-11e6-90bf-fa39dd46c522.png


Identify the candidates for each env variable
```
bio2.bayenv.candidates <- BF.all[which(BF.all$bio2.log10BF>0.5 & (abs(BF.all$bio2.rho))>0.3)]
bio5.bayenv.candidates <- BF.all[which(BF.all$bio5.log10BF>0.5 & (abs(BF.all$bio5.rho))>0.3)]
bio13.bayenv.candidates <- BF.all[which(BF.all$bio13.log10BF>0.5 & (abs(BF.all$bio13.rho))>0.3)]
bio15.bayenv.candidates <- BF.all[which(BF.all$bio15.log10BF>0.5 & (abs(BF.all$bio15.rho))>0.3)]
bio18.bayenv.candidates <- BF.all[which(BF.all$bio18.log10BF>0.5 & (abs(BF.all$bio18.rho))>0.3)]

```



Determine the SNP names of these loci
```
##linux
vcftools --vcf SE132.FINAL.vcf --plink --out SE132.FINAL.plink ##convert the vcf file to plink to get the locus names

#R

locus.names <- read.table("inputfiles/SE132.FINAL.plink.map", header=F) #import the locus names into R
locus.names$ID <- seq.int(nrow(locus.names)) ##index the locus.names file so that all the loci are numbered in order of appearance

BF.all$ID <- seq.int(nrow(BF.all))  ##do the same with the BF.all file. Make sure this is the original output from BayEnv, and not a sorted file. 

##Find all the candidate loci
bio2.bayenv.candidates <- BF.all[which(BF.all$bio2.log10BF>0.5 & (abs(BF.all$bio2.rho))>0.3)]
bio5.bayenv.candidates <- BF.all[which(BF.all$bio5.log10BF>0.5 & (abs(BF.all$bio5.rho))>0.3)]
bio13.bayenv.candidates <- BF.all[which(BF.all$bio13.log10BF>0.5 & (abs(BF.all$bio13.rho))>0.3)]
bio15.bayenv.candidates <- BF.all[which(BF.all$bio15.log10BF>0.5 & (abs(BF.all$bio15.rho))>0.3)]
bio18.bayenv.candidates <- BF.all[which(BF.all$bio18.log10BF>0.5 & (abs(BF.all$bio18.rho))>0.3)]

bio2.bayenv.candidates$ID <- as.character(bio2.bayenv.candidates$ID)
bio2.bayenv.candidates.names <- locus.names[locus.names$ID %in% bio2.bayenv.candidates$ID,]  #Find the actual locus names
colnames(bio2.bayenv.candidates.names) <- c("V1", "SNP", "V3", "V4", "ID")
bio2.bayenv.candidates.names <- paste("X", bio2.bayenv.candidates.names$SNP, sep=".")

bio5.bayenv.candidates$ID <- as.character(bio5.bayenv.candidates$ID)
bio5.bayenv.candidates.names <- locus.names[locus.names$ID %in% bio5.bayenv.candidates$ID,]  #Find the actual locus names
colnames(bio5.bayenv.candidates.names) <- c("V1", "SNP", "V3", "V4", "ID")
bio5.bayenv.candidates.names <- paste("X", bio5.bayenv.candidates.names$SNP, sep=".")

bio13.bayenv.candidates$ID <- as.character(bio13.bayenv.candidates$ID)
bio13.bayenv.candidates.names <- locus.names[locus.names$ID %in% bio13.bayenv.candidates$ID,]  #Find the actual locus names
colnames(bio13.bayenv.candidates.names) <- c("V1", "SNP", "V3", "V4", "ID")
bio13.bayenv.candidates.names <- paste("X", bio13.bayenv.candidates.names$SNP, sep=".")

bio15.bayenv.candidates$ID <- as.character(bio15.bayenv.candidates$ID)
bio15.bayenv.candidates.names <- locus.names[locus.names$ID %in% bio15.bayenv.candidates$ID,]  #Find the actual locus names
colnames(bio15.bayenv.candidates.names) <- c("V1", "SNP", "V3", "V4", "ID")
bio15.bayenv.candidates.names <- paste("X", bio15.bayenv.candidates.names$SNP, sep=".")

bio18.bayenv.candidates$ID <- as.character(bio18.bayenv.candidates$ID)
bio18.bayenv.candidates.names <- locus.names[locus.names$ID %in% bio18.bayenv.candidates$ID,]  #Find the actual locus names
colnames(bio18.bayenv.candidates.names) <- c("V1", "SNP", "V3", "V4", "ID")
bio18.bayenv.candidates.names <- paste("X", bio18.bayenv.candidates.names$SNP, sep=".")

```

Determine the overlap between loci associated with bioxx for BayEnv using a VennDiagram: 

```
library(VennDiagram)

d1 <- length(bio2.bayenv.candidates.names)
d2 <- length(bio5.bayenv.candidates.names)
d3 <- length(bio13.bayenv.candidates.names)
d4 <- length(bio15.bayenv.candidates.names)
d5 <- length(bio18.bayenv.candidates.names)


d12 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio5.bayenv.candidates.names)))
d13 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio13.bayenv.candidates.names)))
d14 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio15.bayenv.candidates.names)))
d15 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio18.bayenv.candidates.names)))
d23 <- length(Reduce(intersect, list(bio5.bayenv.candidates.names, bio13.bayenv.candidates.names)))
d24 <- length(Reduce(intersect, list(bio5.bayenv.candidates.names, bio15.bayenv.candidates.names)))
d25 <- length(Reduce(intersect, list(bio5.bayenv.candidates.names, bio18.bayenv.candidates.names)))
d34 <- length(Reduce(intersect, list(bio13.bayenv.candidates.names, bio15.bayenv.candidates.names)))
d35 <- length(Reduce(intersect, list(bio13.bayenv.candidates.names, bio18.bayenv.candidates.names)))
d45 <- length(Reduce(intersect, list(bio15.bayenv.candidates.names, bio18.bayenv.candidates.names)))

d123 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio5.bayenv.candidates.names,bio13.bayenv.candidates.names)))
d124 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio5.bayenv.candidates.names,bio15.bayenv.candidates.names)))
d125 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio5.bayenv.candidates.names,bio18.bayenv.candidates.names)))
d234 <- length(Reduce(intersect, list(bio5.bayenv.candidates.names, bio13.bayenv.candidates.names,bio15.bayenv.candidates.names)))
d134 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio13.bayenv.candidates.names,bio15.bayenv.candidates.names)))
d135 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio13.bayenv.candidates.names,bio18.bayenv.candidates.names)))
d145 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio15.bayenv.candidates.names,bio18.bayenv.candidates.names)))
d235 <- length(Reduce(intersect, list(bio5.bayenv.candidates.names, bio13.bayenv.candidates.names,bio18.bayenv.candidates.names)))
d245 <- length(Reduce(intersect, list(bio5.bayenv.candidates.names, bio15.bayenv.candidates.names,bio18.bayenv.candidates.names)))
d345 <- length(Reduce(intersect, list(bio13.bayenv.candidates.names, bio15.bayenv.candidates.names,bio18.bayenv.candidates.names)))


d1234 <- length(Reduce(intersect, list(bio5.bayenv.candidates.names, bio13.bayenv.candidates.names,
bio15.bayenv.candidates.names, bio2.bayenv.candidates.names)))
d1235 <- length(Reduce(intersect, list(bio5.bayenv.candidates.names, bio13.bayenv.candidates.names,
bio18.bayenv.candidates.names, bio2.bayenv.candidates.names)))

d2345 <- length(Reduce(intersect, list(bio5.bayenv.candidates.names, bio13.bayenv.candidates.names,
bio15.bayenv.candidates.names, bio18.bayenv.candidates.names)))
d1245 <- length(Reduce(intersect, list(bio5.bayenv.candidates.names, bio2.bayenv.candidates.names,
bio15.bayenv.candidates.names, bio18.bayenv.candidates.names)))
d1345 <- length(Reduce(intersect, list(bio13.bayenv.candidates.names, bio2.bayenv.candidates.names,
bio15.bayenv.candidates.names, bio18.bayenv.candidates.names)))
d12345 <- length(Reduce(intersect, list(bio2.bayenv.candidates.names, bio5.bayenv.candidates.names, bio13.bayenv.candidates.names, 
bio15.bayenv.candidates.names, bio18.bayenv.candidates.names)))


draw.quintuple.venn(area1=d1, area2=d2, area3=d3, area4=d4, area5=d5,
n12=d12, n13=d13, n14=d14, n15=d15, n23=d23, n24=d24, n25=d25, n34=d34, n35=d35, n45=d45,
n123=d123, n124=d124, n125=d125, n134=d134, n135=d135, n145=d145, n234=d234, n235=d235, n245=d245, n345=d345,
n1234=d1234, n1235=d1235, n1245=d1245, n1345=d1345, n2345=d2345, n12345=d12345, 
category=c("bio5", "bio15", "bio13", "bio18", "bio2"),
lty="blank", 
fill=c("yellow", "orange", "skyblue1", "skyblue3", "blue")
)

```


Prepare the data for overlap with the Fst chart


```
d1.names <- as.data.frame(bio2.bayenv.candidates.names)
colnames(d1.names) <- "names"

d2.names <- as.data.frame(bio5.bayenv.candidates.names)
colnames(d2.names) <- "names"

d3.names <- as.data.frame(bio13.bayenv.candidates.names)
colnames(d3.names) <- "names"

d4.names <- as.data.frame(bio15.bayenv.candidates.names)
colnames(d4.names) <- "names"

d5.names <- as.data.frame(bio18.bayenv.candidates.names)
colnames(d5.names) <- "names"


d12345.names <- rbind(d1.names, d2.names, d3.names, d4.names, d5.names)  ##Join all data.frames by "name" column. This only works of colnames are the same (at least one column name)

d12345.names <- lapply(d12345.names, unique)  #select only the unique rows. 

d12345.names <- sub(":", ".", d12345.names$names) ##replace the ":" in the locus names so that they're in the same format as the Fst and RDA lists

d12345.names <- as.data.frame(d12345.names)
write.table(d12345.names$d12345.names, "BayEnv.alloutliers", col.names=F, row.names=F, quote=F)

##linux.
##copy the list over to /Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/SumStats

#Read into R
BayEnv.outliers <- read.table("BayEnv.alloutliers")
colnames(BayEnv.outliers) <- ("loci")
BayEnv.outliers <- as.character(BayEnv.outliers$loci)

```


Find the top 100 ranking XtX loci (~Fst corrected for population structure) and compare these to the Fst distribution. 
```
XtX.run1 <- read.table("BayEnv_Run1/XtX_out.ENVIRONFILE.SE132.15pops.env", header=F)
XtX.run2 <- read.table("BayEnv_Run2/XtX_out.ENVIRONFILE.SE132.15pops.env", header=F)
XtX.run3 <- read.table("BayEnv_Run3/XtX_out.ENVIRONFILE.SE132.15pops.env", header=F)
XtX.run4 <- read.table("BayEnv_Run4/XtX_out.ENVIRONFILE.SE132.15pops.env", header=F)
XtX.run5 <- read.table("BayEnv_Run5/XtX_out.ENVIRONFILE.SE132.15pops.env", header=F)
XtX.run6 <- read.table("BayEnv_Run6/XtX_out.ENVIRONFILE.SE132.15pops.env", header=F)
XtX.run7 <- read.table("BayEnv_Run7/XtX_out.ENVIRONFILE.SE132.15pops.env", header=F)
XtX.run8 <- read.table("BayEnv_Run8/XtX_out.ENVIRONFILE.SE132.15pops.env", header=F)
XtX.run9 <- read.table("BayEnv_Run9/XtX_out.ENVIRONFILE.SE132.15pops.env", header=F)
XtX.run10 <- read.table("BayEnv_Run10/XtX_out.ENVIRONFILE.SE132.15pops.env", header=F)

XtX.all <- rbindlist(list(XtX.run1, XtX.run2, XtX.run3, XtX.run4, XtX.run5, XtX.run6, XtX.run7, XtX.run8, 
XtX.run9, XtX.run10))[,lapply(.SD,median), list(V1)]   ##combine by SNP and calculate the median XtX value

##index the loci 
XtX.all$ID <- seq.int(nrow(XtX.all))   ##index the loci in the original output
XtX.all.sort <- XtX.all[order(-XtX.all$XtX),]  ##order to get the top 100 loci. The minus means its descending order
XtX.top100 <- XtX.all.sort[1:100,]  #select the first 100 loci
XtX.top100.names <- locus.names[locus.names$ID %in% XtX.top100$ID,]  ##get their names
summary(XtX.top100.names)
colnames(XtX.top100.names) <- c("V1", "SNP", "V3", "V4", "ID")   #rename the columns to match the code
XtX.top100.names <- paste("X", XtX.top100.names$SNP, sep=".")  #rename loci
XtX.top100.names <- as.data.frame(XtX.top100.names)
XtX.top100.names
colnames(XtX.top100.names) <- "names"
XtX.top100.names <- sub(":", ".", XtX.top100.names$names)  ##rename to match Fst table
XtX.top100.names <- as.data.frame(XtX.top100.names)
write.table(XtX.top100.names$XtX.top100.names, "XtX.100outliers", col.names=F, row.names=F, quote=F)  ##write the table


##linux.
##copy the list over to /Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/SumStats

#Read into R
XtX.outliers <- read.table("XtX.100outliers")
colnames(XtX.outliers) <- ("loci")

```








##Overlap between Fst chart and outliers

plot the histogram of Fst for the outliers identified with 1)RDA, and 2) LFMM, on the full Fst plot to look determine whether the same loci are outliers. 

Run all outlier analyses (RDA & LFMM) with 2 datasets: 1) full dataset, 2) reduced dataset including only loci variable in >5 populations. 

```
##Get the overall per locus Fst using R
#the genind object would've been read in earlier for the sum stats. If not, convert .ped to .str format and import into R
library(adegenet)
library(hierfstat)

#SE132.genind <- read.structure("SE132all.str")

hier.SE132 <- genind2hierfstat(SE132.genind)
stats.SE132 <- basic.stats(hier.SE132)
stats.SE132.perlocus <- stats.SE132$perloc

##Linux
nano Fstoutliers.names  ##paste the locus names of all the outliers identified by different methods

##R

#Change the locus names if necessary: 
rownames(stats.SE132.perlocus) <- sub("X", "X.", rownames(stats.SE132.perlocus))
fstoutliers.all <- read.table("Fstoutlier.names", header=T)
names(fstoutliers.all)

RDA1.list <- as.character(fstoutliers.all$RDA1)
RDA2.list <- as.character(fstoutliers.all$RDA2)

RDA1.fst <- stats.SE132.perlocus[RDA1.list,]
RDA2.fst <- stats.SE132.perlocus[RDA2.list,]

RDA2.fst <- RDA2.fst$Fst
RDA1.fst <- RDA1.fst$Fst

LFMM.outliers <- as.character(LFMM.outliers$loci)
LFMM.fst <- stats.SE132.perlocus[LFMM.outliers,]
LFMM.fst <- LFMM.fst$Fst

BayEnv.outliers <- as.character(BayEnv.outliers$loci)
BayEnv.fst <- stats.SE132.perlocus[BayEnv.outliers, ]
BayEnv.fst <- BayEnv.fst$Fst

XtX.outliers <- as.character(XtX.outliers$loci)
XtX.fst <- stats.SE132.perlocus[XtX.outliers, ]
XtX.fst <- XtX.fst$Fst


fstall.hist <- hist(stats.SE132.perlocus$Fst, breaks=115)
RDA1.hist <- hist(RDA1.fst, breaks=58)
RDA2.hist <- hist(RDA2.fst, breaks=58)
XtX.hist <- hist(XtX.fst, breaks=58)
LFMM.hist <- hist(LFMM.fst, breaks=115)
BayEnv.hist <- hist(BayEnv.fst, breaks=58) ##Im not sure why I needed to change the breaks here. 
plot(fstall.hist, xlim=c(-0.15,1.0))
plot(RDA1.hist, col=rgb(0,0,1,1/4), xlim=c(-0.15,1.0), add=T)
plot(RDA2.hist, col=rgb(1,0,0,1/4), xlim=c(-0.15,1.0), add=T)
plot(XtX.hist, col=rgb(1,1,0,1/4), xlim=c(-0.15, 1.0), add=T)
plot(LFMM.hist, col=rgb(1,1,0,1/4), xlim=c(-0.15, 1.0), add=T)
plot(BayEnv.hist, col=rgb(1,1,1,1/4), xlim=c(-0.15, 1.0), add=T)

```

![alt_txt][Fst.outliers]
[Fst.outliers]:https://cloud.githubusercontent.com/assets/12142475/20971473/d786eb8e-bc91-11e6-897e-40c15fade58b.png



##IBD vs IBE using EEMS

Looking for deviations from IBE across the landscape

https://github.com/dipetkov/eems/blob/master/Documentation/EEMS-doc.pdf

3 input files needed

pairwise genetic distance

polygon of coordinates

coords file.

1. pairwise genetic distance as calculated with bed2diffs

/Users/alexjvr/2016RADAnalysis/Bombina/BV234/Analyses_20161128/EEMS

```
/Users/alexjvr/Applications/eems-master/bed2diffs/src-wout-openmp/bed2diffs_v1 --bfile SE132.FINAL.plink

```

2. Polygon of coordinates. (anticlockwise. First and last coordinates should be the same

Can be obtained here:

http://www.birdtheme.org/useful/v3tool.html


3. Coords file

Coordinates for each individual. One line per sample. Two columns.

Same order as in the plink file.


Run the simulations from the EEMS directory: /Users/alexjvr/Applications/eems-master/runeems_snps/src

First, create 3 parameter files  (see BV.params-chain1.ini)

Then run all three chains
```
./runeems_snps --params BV.params-chain1.ini --seed 123

```

Launch R in directory where results are deposited
```
install.packages("rEEMSplots", repos=NULL, type="source")
library(rEEMSplots)

eems.results.to.plot = paste(path.package("rEEMSplots"), "/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/EEMS/SE.EEMS.plots", sep="")

name.figures.to.save="SE132.rEEMSplots"
eems.plots(mcmcpath="./SE132.chain1", plotpath=paste(name.figures.to.save, sep=""), longlat=T)

```

##RandomForest Model

Pool all of the outliers and map them using the RandomForest model. 




#Repeat everything with Dataset2

Create a second dataset which includes only loci that are variable in >5 populations. 

1. Filter the dataset

2. calculate per locus Fst and plot

3. RDA with Dataset2

4. LFMM with Dataset2

5. Fst plot with outliers overlaid

6. Compare Dataset1 & Dataset2 outliers
 


```
##R
setwd("/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/rawdataFilter/SE132.FINAL/SE132.FINAL.subset.data")

frq.var.files <- list.files(pattern="frq$")
frq.var.files
frq.var.files.list <- lapply(frq.var.files, read.table, header=T)
names(frq.var.files.list) <- frq.var.files
names(frq.var.files.list)

attach(frq.var.files.list)
frq.var.frequency.keep <- do.call(rbind, lapply(names(frq.var.files.list), get))
summary(frq.var.frequency.keep)
frq.var.frequency.keep <- data.frame(table(frq.var.frequency$SNP))
hist(frq.var.frequency.keep$Freq, xlab="Nr pops", ylab="Frequency of loci across 15 pops")


frq.var.files.list <- lapply(frq.var.files.list, subset, MAF>0.01)  ##keep only the variable loci
summary(frq.var.files.list)
attach(frq.var.files.list)
frq.var.frequency <- do.call(rbind, lapply(names(frq.var.files.list), get))
summary(frq.var.files.list)
frq.var.frequency.keep <- data.frame(table(frq.var.frequency$SNP))
summary(frq.var.frequency.keep)

frq.var.frequency.keep <- subset(frq.var.frequency.keep, Freq>2)
hist(frq.var.frequency.keep$Freq, xlab="Nr pops", ylab="Frequency of loci", main="Frequency of loci variable in >2 pops")


write.table(frq.var.frequency.keep, "loci.var2pops.names", quote=F, row.names=F)
```

2064 loci 

![alt_txt][SE132.FINAL.variable.loci]
[SE132.FINAL.variable.loci]:https://cloud.githubusercontent.com/assets/12142475/20984944/9923e35c-bcc2-11e6-9461-4724704bbc4f.png

1707 loci variable in >5 populations

![alt_txt][SE132.FINAL.variable.loci]
[SE132.FINAL.variable.loci]:https://cloud.githubusercontent.com/assets/12142475/21183148/d6dd49f6-c206-11e6-8e56-beab79cab89f.png


subset the dataset
```
vcftools --vcf SE132.FINAL.vcf --snps SE132.FINAL.subset.data/loci.var2pops.names --recode --recode-INFO-all --out SE132.2064.FINAL

vcftools --vcf SE132.2064.FINAL.recode.vcf --plink --out SE132.2064.plink
plink --file SE132.2064.plink --recode --recodeA --out SE132.2064.plink
```

Sumstats Dataset2

```
##linux
mkdir Dataset2/SumSats
cp rawdataFilter/SE132.FINAL/SE132.2064* Dataset2/SumSats/

##convert to structure using pgdspider

##R

library(adegenet)
library(hierfstat)

SE.data2 <- read.structure("SE132.2064.str")

 How many genotypes are there? 132

 How many markers are there? 2064

data2.names <- read.table("SE.data2.names", header=T)
SE.data2.genind <- SE.data2

data2.pop <- as.factor(data2.names$pop.name)
SE.data2.genind@pop <- data2.pop
SE.data2.genind@pop

hier.SEdata2 <- genind2hierfstat(SE.data2.genind)
stats.hier.SEdata2 <- basic.stats(hier.SEdata2)
stats.hier.SEdata2.perloc <- stats.hier.SEdata2$perloc

summary(stats.hier.SEdata2.perloc$Fst)
hist(stats.hier.SEdata2.perloc$Fst, xlim=c(-0.1, 1.0), breaks=110)
```

Dataset2 Fst distribution 

![alt_txt][Data2.Fst]
[Data2.Fst]:https://cloud.githubusercontent.com/assets/12142475/21184009/d136b240-c20a-11e6-9a1d-6b3b0796c6df.png


###RDA: Dataset2

Directory: /Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/Dataset2/RDA.SE.data2



Input files:

MAF of all loci

Geographic coordinates

Climate variables

1. BIO5: Mat temp warmest month

2. BIO15: Precipitation seasonality

3. BIO13: Precipitation Wettest month

4. BIO18: Precipitation warmest quarter (correlated with growth season)

5. BIO2: Mean diurnal range

2064 loci


```
###1. MAF

#Calculate MAF for the full dataset within region using PLINK

/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/Dataset2/RDA.SE.data2/

###Use the *nosex file to create a file for subsetting the data within pop

plink --file SumSats/SE132.2064.plink --freq --within RDA.SE.data2/SE132.pop.cluster --out RDA.SE.data2/SE132.data2.pop


###R
######Reformat PLINK output
###MAF for each locus -> melt and reformat rows as pops, and columns as loci.

SE.MAF <- read.table("SE132.data2.pop.frq.strat", header=T)
SE.MAF2 <- SE.MAF[,c(3,2,6)]
summary(SE.MAF2)
        CLST               SNP             MAF        
 X1.Sk.Ho  : 1707   100865:23 :   15   Min.   :0.0000  
 X1.Sk.SF  : 1707   101108:100:   15   1st Qu.:0.0000  
 X1.Sk.SL  : 1707   101142:72 :   15   Median :0.1667  
 X2.Upp.Gra: 1707   101367:84 :   15   Mean   :0.2320  
 X2.Upp.K  : 1707   101609:30 :   15   3rd Qu.:0.3889  
 X2.Upp.O  : 1707   101834:91 :   15   Max.   :1.0000  
 (Other)   :15363   (Other)   :25515    

library("ggplot2")
library("reshape2")

SE.MAF3 <- melt(SE.MAF2, id.vars = c("CLST", "SNP"), variable_name = c("MAF"))
str(SE.MAF3)
head(SE.MAF3)


SE132.MAF4 <- dcast(SE.MAF3, formula= CLST ~ SNP)
summary(SE132.MAF4)



##Add X infront of all locusnames. 
colnames(SE132.MAF4) <- paste("X", colnames(SE132.MAF4), sep=".")
write.csv(SE132.MAF4, file="SE.132.MAF.csv")
```

Run RDA

See this tutorial for the interpretation: REDUNDANCY ANALYSIS TUTORIAL: Landscape Genetics Paul Gugger redundancy-analysis-for-landscape-genetics.pdf on mac

```
library(vegan)

GenData <- read.csv("SE.132.Data2.MAF.csv", header=T)
GenData <- GenData[,15:2078]

Climate.Data <- read.csv("SE.132.Data2.MAF.csv", header=T)
names(Climate.Data)
Climate.variables <- Climate.Data[,1:14]
names(Climate.variables)
Climate.Data <- Climate.variables[,10:14]
Climate.Data$Lat <- Climate.variables$Lat
Climate.Data$Long <- Climate.variables$Long
```



```
##1. Run Full RDA model to determine how much of the variation is explainable by the expanatory variables we have
##H0: climate data does not affect genotype

RDA.SEfull <- rda(GenData ~ Lat + Long +bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled, Climate.Data) 
RDA.SEfull
Call: rda(formula = GenData ~ Lat + Long + bio5.scaled + bio15.scaled +
bio13.scaled + bio18.scaled + bio2.scaled, data = Climate.Data)

              Inertia Proportion Rank
Total         90.8292     1.0000     
Constrained   69.5333     0.7655    7
Unconstrained 21.2960     0.2345    7
Inertia is variance 

Eigenvalues for constrained axes:
  RDA1   RDA2   RDA3   RDA4   RDA5   RDA6   RDA7 
29.976 15.402  9.512  5.653  4.538  2.493  1.960 

Eigenvalues for unconstrained axes:
  PC1   PC2   PC3   PC4   PC5   PC6   PC7 
6.322 3.914 3.534 2.325 2.229 1.693 1.277 

anova(RDA.SEfull)

Permutation test for rda under reduced model
Permutation: free
Number of permutations: 999

Model: rda(formula = GenData ~ Lat + Long + bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled, data = Climate.Data)
         Df Variance      F Pr(>F)    
Model     7   69.533 3.2651  0.001 ***
Residual  7   21.296                  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

##to see which variables are most important, we can plot the results in a biplot

plot(RDA.SEfull)
```

![alt_txt][RDA.SEfull]
[RDA.SEfull]:https://cloud.githubusercontent.com/assets/12142475/21185926/e251e150-c212-11e6-962d-5e2bf0100053.png





```
##Partial out geog
H0: Climate does not explain genetic data

pRDA.geog <- rda(GenData~bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled+ Condition(Lat + Long), Climate.Data)
head(summary(pRDA.geog))

anova(pRDA.geog)

Permutation test for rda under reduced model
Permutation: free
Number of permutations: 999

Model: rda(formula = GenData ~ bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled + Condition(Lat + Long), data = Climate.Data)
         Df Variance      F Pr(>F)   
Model     5   34.278 2.2534  0.002 **
Residual  7   21.296                 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1


H0 rejected: Climate does explain GeneticData

pRDA.geog
Call: rda(formula = GenData ~ bio5.scaled + bio15.scaled + bio13.scaled
+ bio18.scaled + bio2.scaled + Condition(Lat + Long), data =
Climate.Data)

              Inertia Proportion Rank
Total         90.8292     1.0000     
Conditional   35.2554     0.3882    2
Constrained   34.2779     0.3774    5
Unconstrained 21.2960     0.2345    7
Inertia is variance 

Eigenvalues for constrained axes:
  RDA1   RDA2   RDA3   RDA4   RDA5 
15.782  9.211  4.563  2.755  1.967 

Eigenvalues for unconstrained axes:
  PC1   PC2   PC3   PC4   PC5   PC6   PC7 
6.322 3.914 3.534 2.325 2.229 1.693 1.277 


plot(pRDA.geog, main="pRDA (geog partialled out)")
```


![alt_txt][Data2.pRDA.geog]
[Data2.pRDA.geog]:https://cloud.githubusercontent.com/assets/12142475/21186294/4fa6ec36-c214-11e6-8ef1-c8dabaa013ca.png


```
##Partial out climate

H0: Geog alone does not explain Genetic data

pRDA.climate <- rda(GenData~Lat+Long + Condition(bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled), Climate.Data)
pRDA.climate
Call: rda(formula = GenData ~ Lat + Long + Condition(bio5.scaled +
bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled), data =
Climate.Data)

               Inertia Proportion Rank
Total         90.82925    1.00000     
Conditional   61.62676    0.67849    5
Constrained    7.90653    0.08705    2
Unconstrained 21.29596    0.23446    7
Inertia is variance 

Eigenvalues for constrained axes:
 RDA1  RDA2 
5.457 2.449 

Eigenvalues for unconstrained axes:
  PC1   PC2   PC3   PC4   PC5   PC6   PC7 
6.322 3.914 3.534 2.325 2.229 1.693 1.277 

head(summary(pRDA.climate))

anova(pRDA.climate)

Permutation test for rda under reduced model
Permutation: free
Number of permutations: 999

Model: rda(formula = GenData ~ Lat + Long + Condition(bio5.scaled + bio15.scaled + bio13.scaled + bio18.scaled + bio2.scaled), data = Climate.Data)
         Df Variance      F Pr(>F)
Model     2   7.1307 1.2476  0.234
Residual  7  20.0038              

H0: not rejected -> Geography alone does not explain GenData

plot(pRDA.climate, main="pRDA (climate partialled out)")
```

![alt_txt][Data2.pRDA.climate]
[Data2.pRDA.climate]:https://cloud.githubusercontent.com/assets/12142475/20987019/49875e6a-bccb-11e6-99e0-c5e44face732.png


Find the most important loci associated with Climate
```
 summary(pRDA.geog)

Accumulated constrained eigenvalues
Importance of components:
                         RDA1   RDA2   RDA3    RDA4    RDA5
Eigenvalue            15.7816 9.2109 4.5633 2.75477 1.96727
Proportion Explained   0.4604 0.2687 0.1331 0.08037 0.05739
Cumulative Proportion  0.4604 0.7291 0.8622 0.94261 1.00000

                 RDA1     RDA2    RDA3     RDA4     RDA5 PC1
bio5.scaled   0.10986 -0.16433  0.2364 -0.06605  0.11174   0
bio15.scaled  0.50934  0.23680  0.3228 -0.04644 -0.05738   0
bio13.scaled  0.47493 -0.23529 -0.3000 -0.18937 -0.52835   0
bio18.scaled  0.47455 -0.06589  0.2094  0.01916 -0.26167   0
bio2.scaled  -0.04982 -0.13866  0.2566 -0.06456 -0.02174   0
```

RDA1: BIO13, BIO15, BIO18

RDA2: BIO13, BIO15


```
##Select the top 5% loci 
summary(pRDA.geog)

#copy and paste the Species scores into a nano folder. Add header for first column. 
##linux

nano pRDA.geog.scores

##R
RDA.loadings <- read.table("pRDA.geog.scores", header=T)

##look at the distribution of loadings on the first four axes
par(mfrow=c(2,2))
hist(RDA.loadings$RDA1)
hist(RDA.loadings$RDA2)
hist(RDA.loadings$RDA3)
hist(RDA.loadings$RDA4)


##Find the top 5% of the absolute values
RDA1.sorted <- RDA.loadings[order(abs(RDA.loadings$RDA1)),]
head(RDA1.sorted)
#           Locus       RDA1      RDA2      RDA3      RDA4       RDA5       PC1
#1721 X.401747.62  6.079e-05  0.002489 -0.029150  0.007205  0.0030610  0.007136
#1797  X.50856.38 -1.899e-04 -0.014530 -0.001315  0.001222 -0.0213400  0.013350
#289  X.152146.31 -2.630e-04  0.025360 -0.011290  0.028060 -0.0112000  0.015910
#1581 X.374529.46 -2.900e-04  0.007951  0.011490 -0.039880 -0.0071740 -0.040760
#1231 X.316499.20  3.235e-04  0.002353 -0.001125  0.003082  0.0168400  0.009722
#702  X.222608.93 -3.308e-04  0.023790 -0.031170  0.009570  0.0003368 -0.001316

RDA1.outliers <- RDA1.sorted[1:102,1:2]
head(RDA1.outliers)
#           Locus       RDA1
#1721 X.401747.62  6.079e-05
#1797  X.50856.38 -1.899e-04
#289  X.152146.31 -2.630e-04
#1581 X.374529.46 -2.900e-04
#1231 X.316499.20  3.235e-04
#702  X.222608.93 -3.308e-04

write.table(RDA1.outliers, "RDA1.outliers", sep=" ", col.names=T, row.names=F, quote=F)

RDA2.sorted <- RDA.loadings[order(abs(RDA.loadings$RDA2)),]
RDA2.outliers <- RDA2.sorted[1:102,c(1,3)]
write.table(RDA2.outliers, "RDA2.outliers", sep=" ", col.names=T, row.names=F, quote=F)
head(RDA2.outliers)
#           Locus       RDA2
#1078 X.287793.76 -5.272e-05
#623  X.209141.31  5.993e-05
#1049  X.284272.5  1.334e-04
#1803  X.51680.55  1.460e-04
#484  X.185106.34 -1.539e-04
#1195  X.308824.7  1.859e-04
```


![alt_txt][RDA1234]
[RDA1234]:https://cloud.githubusercontent.com/assets/12142475/21190068/ed5eca82-c220-11e6-8ff1-6c9c9e182833.png






##LFMM: Dataset 2

wd: /Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/LFMM/SE132.2064.Data2

First calculate the most likely number of latent factors (K) using snmf. 

snmf finds the most likely number of clusters in the population based on 

on fgcz47 

```
##R
SE132.2064 <- vcf2lfmm("SE132.2064.FINAL.recode.vcf")


scp * fgcz47:/srv/kenlab/alexjvr_p1795/SE.Analyses/LFMM.Dec2016/LFMM.Dataset2/

##R
obj.lfmm = lfmm("SE132.1707.FINAL.lfmm", "SE132.env", K=1:5, rep=5, project="new")


export.lfmmProject(obj.lfmmProject)   ##this creates a .zip file in the current directory, which can easily be transferred to other folders. 

import.lfmmProject(obj.lfmmProject) ##to import the project into R from the new folder
```

move the project back onto the mac

/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/LFMM/SE132.FINAL

And calculate the corrected p-values of all the loci. 


```
####Now that we’ve chosen K, we need to manually adjust lambda
L=length(adj.p.values.d1)
q=0.1


zs.d1 <- z.scores(SE132.Data2.lfmm, K=5, d=1)
zs.d1.median =apply(zs.d1, MARGIN=1, median)
lambda=median(zs.d1.median^2)/0.4549364
lambda

	[1] 1.947514

par(mfrow=c(2,3))
adj.p.values.d1 =pchisq(zs.d1.median^2/0.4549364, df=1, lower=F)
hist(adj.p.values.d1, main="K5,d1, 0.455")
summary(adj.p.values.d1)
w=which(sort(adj.p.values.d1)<q*(1:L)/L)
candidates.d1 = order(adj.p.values.d1)[w]
FDR.d1 =  mean(candidates.d1 < ((1-q)*L))
TP.d1 = sum(candidates.d1>((1-q)*L))/(q*L)
FDR.d1
TP.d1


adj.p.values.d1 =pchisq(zs.d1.median^2/0.8, df=1, lower=F)
hist(adj.p.values.d1, main="K5,d1, 0.8")
summary(adj.p.values.d1)
w=which(sort(adj.p.values.d1)<q*(1:L)/L)
candidates.d1 = order(adj.p.values.d1)[w]
FDR.d1 =  mean(candidates.d1 < ((1-q)*L))
TP.d1 = sum(candidates.d1>((1-q)*L))/(q*L)
FDR.d1
TP.d1

adj.p.values.d1 =pchisq(zs.d1.median^2/1.2, df=1, lower=F)
hist(adj.p.values.d1, main="K5,d1, 1.2")
summary(adj.p.values.d1)
w=which(sort(adj.p.values.d1)<q*(1:L)/L)
candidates.d1 = order(adj.p.values.d1)[w]
FDR.d1 =  mean(candidates.d1 < ((1-q)*L))
TP.d1 = sum(candidates.d1>((1-q)*L))/(q*L)
FDR.d1
TP.d1

adj.p.values.d1 =pchisq(zs.d1.median^2/1.6, df=1, lower=F)
hist(adj.p.values.d1, main="K5,d1, 1.6")
summary(adj.p.values.d1)
w=which(sort(adj.p.values.d1)<q*(1:L)/L)
candidates.d1 = order(adj.p.values.d1)[w]
FDR.d1 =  mean(candidates.d1 < ((1-q)*L))
TP.d1 = sum(candidates.d1>((1-q)*L))/(q*L)
FDR.d1
TP.d1

adj.p.values.d1 =pchisq(zs.d1.median^2/2.0, df=1, lower=F)
hist(adj.p.values.d1, main="K5,d1, 2.0")
summary(adj.p.values.d1)
w=which(sort(adj.p.values.d1)<q*(1:L)/L)
candidates.d1 = order(adj.p.values.d1)[w]
FDR.d1 =  mean(candidates.d1 < ((1-q)*L))
TP.d1 = sum(candidates.d1>((1-q)*L))/(q*L)
FDR.d1
TP.d1





zs.d2 <- z.scores(SE132.Data2.lfmm, K=5, d=2)
zs.d2.median =apply(zs.d2, MARGIN=1, median)
lambda=median(zs.d2.median^2)/0.4549364
lambda



###d3: BIO13 - precipitation of wettest month

zs.d3 <- z.scores(SE132.Data2.lfmm, K=5, d=3)
zs.d3.median =apply(zs.d3, MARGIN=1, median)
lambda=median(zs.d3.median^2)/0.4549364
lambda

	[1] 1.116385

par(mfrow=c(2,3))
adj.p.values.d3 =pchisq(zs.d3.median^2/0.4549364, df=1, lower=F)
hist(adj.p.values.d3, main="K5,d3, 0.455")
summary(adj.p.values.d3)
w=which(sort(adj.p.values.d3)<q*(1:L)/L)
candidates.d3 = order(adj.p.values.d3)[w]
FDR.d3 =  mean(candidates.d3 < ((1-q)*L))
TP.d3 = sum(candidates.d3>((1-q)*L))/(q*L)
FDR.d3
TP.d3


adj.p.values.d3 =pchisq(zs.d3.median^2/0.8, df=1, lower=F)
hist(adj.p.values.d3, main="K5,d3, 0.8")
summary(adj.p.values.d3)
w=which(sort(adj.p.values.d3)<q*(1:L)/L)
candidates.d3 = order(adj.p.values.d3)[w]
FDR.d3 =  mean(candidates.d3 < ((1-q)*L))
TP.d3 = sum(candidates.d3>((1-q)*L))/(q*L)
FDR.d3
TP.d3

adj.p.values.d3 =pchisq(zs.d3.median^2/1.2, df=1, lower=F)
hist(adj.p.values.d3, main="K5,d3, 1.2")
summary(adj.p.values.d3)
w=which(sort(adj.p.values.d3)<q*(1:L)/L)
candidates.d3 = order(adj.p.values.d3)[w]
FDR.d3 =  mean(candidates.d3 < ((1-q)*L))
TP.d3 = sum(candidates.d3>((1-q)*L))/(q*L)
FDR.d3
TP.d3

adj.p.values.d3 =pchisq(zs.d3.median^2/1.6, df=1, lower=F)
hist(adj.p.values.d3, main="K5,d3, 1.6")
summary(adj.p.values.d3)
w=which(sort(adj.p.values.d3)<q*(1:L)/L)
candidates.d3 = order(adj.p.values.d3)[w]
FDR.d3 =  mean(candidates.d3 < ((1-q)*L))
TP.d3 = sum(candidates.d3>((1-q)*L))/(q*L)
FDR.d3
TP.d3

adj.p.values.d3 =pchisq(zs.d3.median^2/2.0, df=1, lower=F)
hist(adj.p.values.d3, main="K5,d3, 2.0")
summary(adj.p.values.d3)
w=which(sort(adj.p.values.d3)<q*(1:L)/L)
candidates.d3 = order(adj.p.values.d3)[w]
FDR.d3 =  mean(candidates.d3 < ((1-q)*L))
TP.d3 = sum(candidates.d3>((1-q)*L))/(q*L)
FDR.d3
TP.d3






zs.d4 <- z.scores(SE132.Data2.lfmm, K=5, d=4)
zs.d4.median =apply(zs.d4, MARGIN=1, median)
lambda=median(zs.d4.median^2)/0.4549364
lambda





zs.d5 <- z.scores(SE132.Data2.lfmm, K=5, d=5)
zs.d5.median =apply(zs.d5, MARGIN=1, median)
lambda=median(zs.d5.median^2)/0.4549364
lambda
```



#Gradient Forest Analysis

Aim: map the genomic turnover of the outlier loci across the landscape

I'm using scripts from Karina & Victoria, so they are slightly different to before.  

For Gradient Forest we need

1. BIOclim variables (which have been obtained before)

2. Moran's Eigenvector Variables (accounting for unsampled climatic/geographic variation)

3. Allele frequency data


```
#Calculating MEM variables
#install.packages("tripack")
#install.packages("spacemakeR", repos="http://R-Forge.R-project.org")
library(spacemakeR)


climatic=read.table("climatic_all.txt", header=T) #lat, long, environmental variables for 17 populations
head(climatic)
#extract x and y
xy <- climatic[, c("Long","Lat")]

#install.packages("geosphere")
library(geosphere) #calculate a matrix of geographic distances
dxy <- distm(xy)
dxy <- as.dist(dxy)

#Function that returns the maximum distance of the minimum spanning tree based on a distance matrix.
th <- give.thresh(dxy)
#Function to compute neighborhood based on the minimum spanning tree. Returns an object of the class nb (see spdep package).
nb1 <- mst.nb(dxy)
wh1 <- which(as.matrix(dxy)==th,arr.ind=TRUE)
plot(nb1,xy,pch=20,cex=2,lty=3)
lines(xy[wh1[1,],1],xy[wh1[1,],2],lwd=2)
title(main="Maximum distance of the minimum spanning tree in bold")
#thershold distance
th 
#[1] 493707.1
nb1
#Neighbour list object:
#Number of regions: 15 
#Number of nonzero links: 28 
#Percentage nonzero weights: 12.44444 
#Average number of links: 1.866667 

#install.packages("spdep")
library(spdep)
#transform nb to listw (spdep package)
listw=nb2listw(nb1, glist=NULL, style="W", zero.policy=NULL)
#The can.be.simmed helper function checks whether a spatial weights object is similar to
#symmetric and can be so transformed to yield real eigenvalues or for Cholesky decomposition.
can.be.simmed(listw)
#[1] TRUE
 
#Function to compute Moran's eigenvectors of a listw object
#This functions compute eigenvector's of a doubly centered spatial weighting matrix. 
#Corresponding eigenvalues are linearly related to Moran's index of spatial autocorrelation.
#scores=scores.listw(listw, echo = FALSE, MEM.autocor = c("all","positive", "negative"))
#MEM.autocor: A string indicating if all MEMs must be returned or only those corresponding to positive or negative autocorrelation.
#Only positive correlations:
scores=scores.listw(listw, echo = FALSE, MEM.autocor = "positive")
	#listw not symmetric, (w+t(w)) used in the place of w 

#Function to compute and test Moran's I for eigenvectors of spatial weighting matrices. 
#This function tests Moran's I for each eigenvector of a spatial weighting matrix
test.scores(scores,listw,nsim=999)


#         stat  pval
#1 0.9934033 0.001
#2 0.9164982 0.001
#3 0.7945233 0.001
#4 0.6336452 0.003
#5 0.4408883 0.026
#6 0.2268045 0.125


#Five significant MEM eigenfunctions with positive correlations. OBS. use first half (3 in our case)

write.table (scores$vectors[,1], "scores_MEM1.txt") 
write.table (scores$vectors[,2], "scores_MEM2.txt") 
write.table (scores$vectors[,3], "scores_MEM3.txt") 


####I need to ask Karina if she got different results from the large grid. Perhaps it's better to use this??
#calculatin MEM variables from grid data(10,000 random points from Antonio)
grid=read.table("qrug_XY.txt", header=T) #lat, long for 10,000 point locations
XY<-grid[, c("lon", "lat")] 
plot(XY)
DXY <- dist(XY) # it only works in sorklab2, data set is too big
#Function that returns the maximum distance of the minimum spanning tree based on a distance matrix.
th <- give.thresh(DXY) 
#Function to compute neighborhood based on the minimum spanning tree. Returns an object of the class nb (see spdep package).
nb1 <- mst.nb(DXY)
wh1 <- which(as.matrix(DXY)==th,arr.ind=TRUE)
plot(nb1,XY,pch=20,cex=2,lty=3)
lines(XY[wh1[1,],1],XY[wh1[1,],2],lwd=2)
title(main="Maximum distance of the minimum spanning tree in bold")
#thershold distance
th 
#[1] 1.361023
nb1
#Neighbour list object:
#Number of regions: 10000
#Number of nonzero links: 19998
#Percentage nonzero weights: 0.019998
#Average number of links: 1.9998


#transform nb to listw (spdep package)
listw=nb2listw(nb1, glist=NULL, style="W", zero.policy=NULL)
#The can.be.simmed helper function checks whether a spatial weights object is similar to
#symmetric and can be so transformed to yield real eigenvalues or for Cholesky decomposition.
can.be.simmed(listw)
#[1] TRUE
 
#Function to compute Moran's eigenvectors of a listw object
#This functions compute eigenvector's of a doubly centered spatial weighting matrix. 
#Corresponding eigenvalues are linearly related to Moran's index of spatial autocorrelation.
#scores=scores.listw(listw, echo = FALSE, MEM.autocor = c("all","positive", "negative"))
#MEM.autocor: A string indicating if all MEMs must be returned or only those corresponding to positive or negative autocorrelation.
#Only positive correlations:
scores=scores.listw(listw, echo = FALSE, MEM.autocor = "positive")

write.table (scores$vectors[,1], "scores_biggrid1.txt") 
write.table (scores$vectors[,2], "scores_biggrid2.txt") 
write.table (scores$vectors[,3], "scores_biggrid3.txt") 
#Function to compute and test Moran's I for eigenvectors of spatial weighting matrices. 
#This function tests Moran's I for each eigenvector of a spatial weighting matrix
test.scores(scores,listw,nsim=500)
```


#Great circle distance between samples

```
##Geographic distance between all SE populations
##using package Rdist

library(fields)
#Import .csv with coordinates

setwd("/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/GradientForest")

SE.dist <- read.table("climatic_all.txt", header=T)
head(SE.dist)



#rdist.earth (in fields package) wants only long & lat
SE_lon.lat <- cbind(SE.dist$Long, SE.dist$Lat)
SE_lon.lat

#calculate great circle distances
distance.matrix.SERAD <- rdist.earth(SE_lon.lat)
summary(distance.matrix.SERAD)
dim(distance.matrix.SERAD)

#and use only the lower half of the matrix
upper.tri(distance.matrix.SERAD)
distance.matrix.SERAD[lower.tri(distance.matrix.SERAD)]<-NA
distance.matrix.SERAD

#change from matrix to dataframe
bli <- as.data.frame(distance.matrix.SERAD)
head(bli)
colnames(bli) <- SE.dist$Pop
rownames(bli) <- SE.dist$Pop

bli[lower.tri(bli,diag=TRUE)]=NA  #Prepare to drop duplicates and meaningless information
bli=as.data.frame(as.table(as.matrix(bli)))  #Turn into a 3-column table
bli
bli=na.omit(bli)  #Get rid of the junk we flagged above
bli
colnames(bli)<-c("site1", "site2", "dist(km)")
head(bli)

bli2 <- bli[sort(bli$site2),]

head(bli2)


##write to csv
write.csv(bli, file="distance.SE.csv",row.names=F)
```

Use the first population (Sk.Ho) as distance 0. So all distances are measured from here. 


Input file: 

Create 3 SNP input files

1. All Fst outliers (259 loci)

2. All env associated loci (LFMM & BayEnv2)

3. All other (neutral) loci 


Create a .csv including all the environmental variables, MEMs, and candidate loci variable in more than 5 populations. 

copy and paste all the outliers locus names into a table. The names need to be corrected, and then the SNPs extracted from the .vcf file which contains only the loci variable in >5 populations

/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/GradientForest
```
##Dataset1: Fstoutliers (RDA + XtX)
##These seem to overlap a lot. I end up with 259 loci in total. 


#read in RDA and XtX outlier names

outliers.names <- read.table("outliers.vcffriendlynames", header=F)
outliers.names$outliers1 <- outliers.names$V1
head(outliers.names)
outliers.names$outliers1 <- gsub("X.", "", outliers.names$outliers1)
head(outliers.names)
outliers.names$outliers1 <- gsub("\\.", ":", outliers.names$outliers1)
outliers.names$outliers1
##write.table(outliers.names$outliers1, "outliernames.vcffriendly", col.names=F, quote=F, row.names=F)

XtX.outliers <- read.table("XtX.100outliers", header=F)
summary(XtX.outliers)
XtX.outliers$outliers1 <- XtX.outliers$V1  #rename column
XtX.outliers$outliers1 <- gsub("X.", "", XtX.outliers$outliers1)  ##rename loci
XtX.outliers$outliers1 <- gsub("\\.", ":", XtX.outliers$outliers1)  ##rename loci
head(XtX.outliers)


Fst.outliers.all <- rbind(XtX.outliers, outliers.names)  #join the names together in one column
summary(Fst.outliers.all)
Fst.outliers.all <- subset(Fst.outliers.all,!duplicated(Fst.outliers.all$V1))  #remove all duplicates
summary(Fst.outliers.all)
write.table(Fst.outliers.all$outliers1, "Fst.outliers.vcffriendly", col.names=F, row.names=F, quote=F)


##linux
vcftools --vcf SE132.FINAL.vcf --snps Fst.outliers.vcffriendly --recode --recode-INFO-all --out SE132.Fstoutliers
After filtering, kept 259 out of a possible 2081 Sites
```

Calculate MAF for all loci in Plink:

```
vcftools --vcf SE132.Fstoutliers.recode.vcf --plink --out SE132.Fstoutliers.plink

plink --file SE132.Fstoutliers.plink --recode --recodeA --out SE132.Fstoutliers.plink
```

Find the sample names in the *nosex file, and add pop names (i.e. 3 columns) to create a file for specifying clusters > SE132.235.PlinkCluster

And calculate MAF with Plink

```
plink --file SE132.Fstoutliers.plink --within SE132.235.PlinkCluster --freq --out SE132.Fst.258.plink
```


Import into R to reformat the output - by population and loci as columns

```
######Reformat PLINK output
###For Gradient Forest
###MAF for each locus -> melt and reformat rows as pops, and columns as loci. 


setwd("/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/GradientForest")

SE.MAF <- read.table("SE132.Fst.258.plink", header = T)
head(SE.MAF)

SE.MAF <- SE.MAF[,c(3,2,6)]

library("ggplot2")
library("reshape2")

SE.MAF2 <- melt(SE.MAF, id.vars = c("CLST", "SNP"), variable_name = c("MAF"))
str(SE.MAF2)
head(SE.MAF2)


SE.MAF3 <- dcast(SE.MAF2, formula= CLST ~ SNP)
head(SE.MAF3)
colnames(SE.MAF3) <- paste("X", colnames(SE.MAF3), sep=".")  ##Change colnames, so that excel doesn't change the SNP names
write.csv(SE.MAF3, file="SE132.259.MAF.csv")
```

##Greengrid

A grid of the background environmental data

Random sampling points can be generated here: http://www.geomidpoint.com/random/
and exported as decimal degrees. 

1. extract bioclim data for these points

2. Create MEM variables for these points

```
##extract env data
##Tutorial: https://ecologicaconciencia.wordpress.com/2013/11/29/obtaining-macroclimate-data-with-r-to-model-species-distributions/

#install.packages("rgdal") ##had to install gdal first: brew install gdal on comp command line
library(rgdal)
#install.packages("raster")
library(raster)

#climate <- getData('worldclim', var='bio', res=2.5) ##extracts 19 BioClim variables from worldclim at 2.5' resolution. 

#######
##linux: cp -r /Users/alexjvr/2016RADAnalysis/5_SE.MS1/Analyses_old/GradForest/wc2-5 .
#######
#climate <- getData('worldclim', var='bio', res=2.5) ##extracts 19 BioClim variables from worldclim at 2.5' resolution. (downloaded
climate  ##make sure it's a RasterStack
names(climate)  ##lists bio1-19
plot(climate$bio1) ##This should be of the whole world

climate2 <- crop(climate, extent(9,23,52,70)) ##crop to map extent
climate2    
#spplot(climate2, main="BioClim Variables", xlim=c(9,23) , ylim=c(52,70)) ##check that the extent is correct

###plot sample localities on one of these maps: 

SE2000random.coords <- read.table(file = "random.2000SE.coords", header=T)
head(SEpop.coords)


xy <- SE2000random.coords[,c(2,1)]
spdf <- SpatialPointsDataFrame(coords = xy, data = SE2000random.coords,
                               proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))
spdf  ##convert dataframe to spatial points

spplot(climate2$bio2, main="BIO2:Mean Diurnal Range", 
       sp.layout = c("sp.points", spdf, col="black", pch=16))
       
##Extract point location information:

presvals <- extract(climate2, xy)   ##extract bioclim data for point locations
head(presvals)


setwd("/Users/alexjvr/2016RADAnalysis/5_SE.MS1/DEC2016_SEonly/GradientForest")
write.csv(presvals, file = "SE2000random.BIOclim.csv")  ###write to .csv


```

###Create MEM for these data points
```
#calculation MEM variables from grid data(2,000 random points)
grid <- read.table("random.2000SE.coords", header=T) #lat, long for 2,000 point locations
XY<-grid[, c("Long", "Lat")] 
plot(XY)
DXY <- dist(XY) 
#Function that returns the maximum distance of the minimum spanning tree based on a distance matrix.
th <- give.thresh(DXY) 
#Function to compute neighborhood based on the minimum spanning tree. Returns an object of the class nb (see spdep package).
nb1 <- mst.nb(DXY)
wh1 <- which(as.matrix(DXY)==th,arr.ind=TRUE)
plot(nb1,XY,pch=20,cex=2,lty=3)
lines(XY[wh1[1,],1],XY[wh1[1,],2],lwd=2)
title(main="Maximum distance of the minimum spanning tree in bold")
#thershold distance
th 
#[1] 0.6976318
nb1
#Neighbour list object:
#Number of regions: 2000 
#Number of nonzero links: 3998 
#Percentage nonzero weights: 0.09995 
#Average number of links: 1.999


#transform nb to listw (spdep package)
listw=nb2listw(nb1, glist=NULL, style="W", zero.policy=NULL)
#The can.be.simmed helper function checks whether a spatial weights object is similar to
#symmetric and can be so transformed to yield real eigenvalues or for Cholesky decomposition.
can.be.simmed(listw)
#[1] TRUE
 
#Function to compute Moran's eigenvectors of a listw object
#This functions compute eigenvector's of a doubly centered spatial weighting matrix. 
#Corresponding eigenvalues are linearly related to Moran's index of spatial autocorrelation.
#scores=scores.listw(listw, echo = FALSE, MEM.autocor = c("all","positive", "negative"))
#MEM.autocor: A string indicating if all MEMs must be returned or only those corresponding to positive or negative autocorrelation.
#Only positive correlations:
scores=scores.listw(listw, echo = FALSE, MEM.autocor = "positive")
test.scores(scores,listw,nsim=500)

write.table (scores$vectors[,1], "scores_biggrid1.txt") 
write.table (scores$vectors[,2], "scores_biggrid2.txt") 
write.table (scores$vectors[,3], "scores_biggrid3.txt") 
#Function to compute and test Moran's I for eigenvectors of spatial weighting matrices. 
#This function tests Moran's I for each eigenvector of a spatial weighting matrix

```

paste coordinates, MEM and bioclim variables all into 1 .csv. 
 Open in R to remove all NAs and scale the variables
 
```
SE2000.dat <- read.csv("greengrid.SE2000pts.csv", header=T)

 ##remove all NA (as they are in the sea
presvals.2000 <- SE2000.dat[complete.cases(SE2000.dat),]
summary(presvals.2000)


##Scale these variables
library(corrplot)
library(caret)
datMy <- presvals.2000[,3:7]
datMy
summary(datMy)
datMy.scale <- scale(datMy)
summary(datMy.scale)
datMy.scale <- as.data.frame(datMy.scale)
presvals.2000$bio5.scaled <- datMy.scale$bio5
presvals.2000$bio15.scaled <- datMy.scale$bio15
presvals.2000$bio13.scaled <- datMy.scale$bio13
presvals.2000$bio18.scaled <- datMy.scale$bio18
presvals.2000$bio2.scaled <- datMy.scale$bio2
summary(presvals.2000)
write.csv(presvals.2000, "SE2000.GF.data.csv", quote=F, row.names=F)
```


Run Gradient Forest
```
library(gradientForest)
library(raster)
gf.SEtemp <- read.csv("SE132.235.SNP.env.data5GF_20161209.csv", header=T)
envGF <- gf.SEtemp[,9:17]
#envGF <- gf.SEtemp[,4:12]
#envGF$MEM1 <- as.numeric(envGF$MEM1)
#envGF$MEM2 <- as.numeric(envGF$MEM2)
#envGF$MEM3 <- as.numeric(envGF$MEM3)

SNP_Fstcandidate.SE132 <- SE.MAF3.Fst[,grep("X.", colnames(SE.MAF3.Fst))]
#SNP_candidate.SE132 <- gf.SEtemp[,grep("X.", colnames(gf.SEtemp))]

maxLevel <- log2(0.368*nrow(envGF)/2)
#[1] 1.464668

gf.Fst.model.SEtemp <- gradientForest(cbind(envGF, SNP_Fstcandidate.SE132), predictor.vars=colnames(envGF), response.vars=colnames(SNP_Fstcandidate.SE132), ntree=2000, nbin =1001,maxLevel=maxLevel, trace=T, corr.threshold=0.5)

gf.ENVcandidates.model.SEtemp <- gradientForest(cbind(envGF, SNP_ENVcandidate.SE132), predictor.vars=colnames(envGF), response.vars=colnames(SNP_ENVcandidate.SE132), ntree=2000, nbin =1001,maxLevel=maxLevel, trace=T, corr.threshold=0.5)

gf.NEUTRAL.model.SEtemp <- gradientForest(cbind(envGF, SNP_NEUTRAL.SE132), predictor.vars=colnames(envGF), response.vars=colnames(SNP_NEUTRAL.SE132), ntree=2000, nbin =1001,maxLevel=maxLevel, trace=T, corr.threshold=0.5)


#gf.Ref.model.SEtemp <- gradientForest(cbind(envGF, SNP_candidate.SE132), predictor.vars=colnames(envGF), #response.vars=colnames(SNP_candidate.SE132), ntree=2000, nbin =1001,maxLevel=maxLevel, trace=T, corr.threshold=0.5)
#Calculating forests for 235 species
#.....................
#There were 11 warnings (use warnings() to see them)
#> warnings()
#Warning messages:
#1: In randomForest.default(m, y, ...) :
#  The response has five or fewer unique values.  Are you sure you want to do regression?

gf.Ref.model.SEtemp
#A forest of 2000 regression trees for each of 219 species
#Call:
#gradientForest(data = cbind(envGF, SNP_candidate.SE132), predictor.vars = colnames(envGF), 
#    response.vars = colnames(SNP_candidate.SE132), ntree = 2000, 
#    maxLevel = maxLevel, corr.threshold = 0.5, nbin = 1001, trace = T)
#Important variables:
#[1] bio5.scaled  dist.km      MEM1         bio2.scaled  bio18.scaled

summary(gf.Ref.model.SEtemp)

all_gfmod <- gf.Ref.model.SEtemp

write.table(all_gfmod$Y, file="all_gf_Y.txt")
write.table(all_gfmod$X, file="all_gf_X.txt")
write.table(all_gfmod$imp.rsq, file="all_gf_impRsq.txt")
write.table(all_gfmod$result, file="all_gf_result.txt")
write.table(all_gfmod$res.u, file="all_gf_res_u.txt")
write.table(all_gfmod$res, file="all_gf_res.txt")
all_gfmod$overall.imp
all_gfmod$overall.imp2
#bio13.scaled bio15.scaled bio18.scaled  bio2.scaled  bio5.scaled      dist.km 
#0.0007625546 0.0058518950 0.0065758759 0.0105153494 0.0155421756 0.0141958620 
#        MEM1         MEM2         MEM3 
#0.0122804311 0.0053440700 0.0036496265 
#> all_gfmod$overall.imp2
#bio13.scaled bio15.scaled bio18.scaled  bio2.scaled  bio5.scaled      dist.km 
#  0.05442106   0.09751299   0.12220551   0.16025542   0.19124232   0.18028763 
#        MEM1         MEM2         MEM3 
#  0.17807204   0.09456112   0.10960729 
  
  
#plot output, see ?plot.gradientForest
#predictoroverallimportance
pdf(file="~/all_predictoroverallimportance.pdf")
plot.gradientForest(all_gfmod,plot.type="O")
dev.off()

all_predictors=names(importance(all_gfmod))
all_predictors

#[1] "bio5.scaled"  "dist.km"      "MEM1"         "bio2.scaled"  "bio18.scaled"
#[6] "bio15.scaled" "MEM2"         "MEM3"         "bio13.scaled"

#splitsdensityplots
pdf(file="all_splitsdensityplots.pdf")
plot(all_gfmod, plot.type="S", imp.vars=all_predictors, leg.posn="topright", cex.legend=0.4, cex.axis=0.6, cex.lab=0.7, line.ylab=0.9, par.args=list(mgp=c(1.5, 0.5, 0), mar=c(3.1,1.5,0.1,1)))
dev.off()

#speciescumulativeplot #the legend identifies the top 5 most responsive SNPs for each predictor
pdf(file="all_speciescumulativeplot.pdf")
plot.gradientForest(all_gfmod, plot.type="Cumulative.Importance", imp.vars=all_predictors, show.overall=T, legend=T,common.scale=T,leg.posn="topleft", leg.nspecies=5, cex.lab=0.7, cex.legend=0.4, cex.axis=0.6, line.ylab=0.9, par.args=list(mgp=c(1.5, 0.5, 0), mar=c(3.1,1.5,0.1,1),omi=c(0,0.3,0,0)))
dev.off()

#predictorcumulative #show cumulative change in overall composition of the community, where changes occur on the gradient
pdf(file="all_predictorcumulative.pdf")
plot(all_gfmod, plot.type="C", imp.vars=all_predictors, show.species=F, common.scale=T, cex.axis=0.6, cex.lab=0.7, line.ylab=0.9, par.args=list(mgp=c(1.5, 0.5, 0), mar=c(2.5,1.0,0.1,0.5), omi=c(0,0.3,0,0)))
dev.off()

#R2
pdf(file="all_R2.pdf")
plot(all_gfmod, plot.type="P", show.names=F, horizontal=F, cex.axis=1, cex.labels=0.7, line=2.5)
dev.off()

#transform grid and environmental predictors
all_predictors <- names(importance(all_gfmod)[1:4])
all_predictors
#[1] "bio5.scaled" "dist.km"     "MEM1"        "bio2.scaled"

###New grid data (2000 random points)
Grid=read.csv("SE2000.GF.data.csv",header=T)  #environental data (MEM, lat, long and climatic for a grid plus sampling points)
grid=Grid[, c("Long", "Lat")] #only lon-lat of grid
greengrid=Grid[,c("Long", "Lat","MEM1","MEM2","MEM3","bio5.scaled", "bio15.scaled", "bio13.scaled", "bio18.scaled", "bio2.scaled")]


all_Trns_site <- predict(all_gfmod)
all_tgrid=cbind(greengrid[,c("Long","Lat")], predict.gradientForest(all_gfmod,greengrid[,all_predictors])
##I had to use test_predictors here, since I haven't calculated distance (km) between randomly sampled sites)
write.table(all_tgrid, file="all_trans_predictors.txt")
#pcs
all_PCs=prcomp(all_tgrid[,3:10], center=TRUE, scale.=FALSE)
all_PCs
#Standard deviations:
#Standard deviations:
#[1] 0.0395686813 0.0277794278 0.0154269240 0.0126690543 0.0013105520
#[6] 0.0006121946 0.0003723189 0.0001814975
#Rotation:
#                       PC1          PC2           PC3          PC4          PC5
#MEM1         -0.0004467114  0.002375862 -0.0033333655 -0.001770914  0.030098308
#MEM2          0.0003448774 -0.000258878  0.0001245132 -0.000103350 -0.004855483
#MEM3         -0.0006386975  0.001864999 -0.0029636501 -0.001850711  0.038052378
#bio5.scaled  -0.9272990329  0.317183640  0.1906186182 -0.056108280  0.005082218
#bio15.scaled -0.0323592264 -0.308722690  0.0780514784 -0.947159676 -0.020966296
#bio13.scaled  0.0374623339  0.010322220  0.1338255928 -0.015500838  0.988970233
#bio18.scaled  0.2049092595  0.032111856  0.9663097548  0.065199609 -0.137294986
#bio2.scaled  -0.3093176790 -0.896068039  0.0767383051  0.308613953  0.015655143
#                       PC6           PC7           PC8
#MEM1          9.780904e-01 -0.1766747399 -1.058264e-01
#MEM2         -7.891191e-02  0.1523984643 -9.851517e-01
#MEM3          1.889767e-01  0.9719015013  1.350230e-01
#bio5.scaled  -9.742897e-04 -0.0006931120 -4.322236e-04
#bio15.scaled -7.581128e-05 -0.0001947075  2.583153e-04
#bio13.scaled -3.621798e-02 -0.0303722116 -6.642696e-03
#bio18.scaled  8.971874e-03  0.0066136276  1.159708e-03
#bio2.scaled   2.595012e-03  0.0012192402  8.095144e-06



summary(all_PCs)
#Importance of components:
#                           PC1     PC2     PC3     PC4      PC5       PC6
#Standard deviation     0.03957 0.02778 0.01543 0.01267 0.001311 0.0006122
#Proportion of Variance 0.57181 0.28183 0.08692 0.05862 0.000630 0.0001400
#Cumulative Proportion  0.57181 0.85364 0.94055 0.99917 0.999800 0.9999400
#                             PC7       PC8
#Standard deviation     0.0003723 0.0001815
#Proportion of Variance 0.0000500 0.0000100
#Cumulative Proportion  0.9999900 1.0000000

# set up a colour palette for the mapping
a1 <- all_PCs$x[,1]
a2 <- all_PCs$x[,2]
a3 <- all_PCs$x[,3]
r <- a1+a2
g <- -a2
b <- a3+a2-a1
r <- (r-min(r)) / (max(r)-min(r)) * 255
g <- (g-min(g)) / (max(g)-min(g)) * 255
b <- (b-min(b)) / (max(b)-min(b)) * 255

nvs <- dim(all_PCs$rotation)[1] # number of variables
vec <- c("bio15.scaled", "bio13.scaled", "bio18.scaled")
lv <- length(vec)
vind <- rownames(all_PCs$rotation) %in% vec
# choose a scaling factor to plot the vectors over the grid
scal <- 60
xrng <- range(all_PCs$x[,1], all_PCs$rotation[,1]/scal)*1.1
yrng <- range(all_PCs$x[,2], all_PCs$rotation[,2]/scal)*1.1
pdf(file="all_PCplot.pdf")
plot((all_PCs$x[,1:2]), xlim=xrng, ylim=yrng, pch=".", cex=7, col=rgb(r,g,b, max = 255), asp=1)
# plot the other predictors with "+"
points(all_PCs$rotation[! vind,1:2]/scal, pch="+")  
# plot the chosen predictors as arrows
arrows(rep(0,lv), rep(0,lv), all_PCs$rotation[,1]/scal, all_PCs$rotation[,2]/scal, length = 0.1)
jit <- 0.0015
#text(cand_PCs$rotation[vec,1]/scal+jit*sign(cand_PCs$rotation[vec,1]), cand_PCs$rotation[vec,2]/scal+jit*sign(cand_PCs$rotation[vec,2]), labels = vec)
text(all_PCs$rotation[,1]/scal+jit*sign(all_PCs$rotation[,1]), all_PCs$rotation[,2]/scal+jit*sign(all_PCs$rotation[,2]), labels = vec)

# first predict the PCs for the transformed site data
all_PCsites <- predict(all_PCs,all_Trns_site[,test_predictors])
# plot all the sites as points on the biplot
points(all_PCsites[,1:2])
# calc & plot the weighted mean locations of SNPs (from gf$Y)
#SpsWtd <- sweep(cand_gfmod$Y,2,apply(cand_gfmod$Y,2,min),"-")
#SpsWtdPCs <- (t(SpsWtd) %*% (cand_PCsites[,1:2]))/colSums(SpsWtd)
#points(SpsWtdPCs, col="red", pch="+")
## interactively label some of the species, if desired
#identify(SpsWtdPCs, labels = as.character(rownames(SpsWtdPCs)), col="blue")
dev.off()

#plot these in geographic space

pdf(file="all_map.pdf")
green.pred <- predict(all_gfmod, greengrid[,test_predictors])
plot(all_tgrid[,c("Long","Lat")],pch=15,cex=1.0,asp=1,col=rgb(r,g,b, max=255),main="SNP turnover R.temporaria")
points(sites)
dev.off()
#clustered version
require(cluster)
ncl <- 8 
clPCs <- clara(all_PCs$x,ncl,sampsize=1000) 
# get the medoid colour palette
medcolR <- r[clPCs$i.med]
medcolG <- g[clPCs$i.med]
medcolB <- b[clPCs$i.med]

# re-plot the biplot -- coloured by cluster medoids
pdf(file="all_map_cluster.pdf")
plot((all_PCs$x[,1:2]), xlim=xrng, ylim=yrng,pch=".", cex=4, col=rgb(medcolR[clPCs$clustering], medcolG[clPCs$clustering], medcolB[clPCs$clustering],  max = 255), asp=1)
points(all_PCs$rotation[! vind,1:2]/scal, pch="+") 
#arrows(rep(0,lv), rep(0,lv), cand_PCs$rotation[vec,1]/scal, cand_PCs$rotation[vec,2]/scal, length = 0.0625) 
#text(cand_PCs$rotation[vec,1]/scal+jit*sign(cand_PCs$rotation[vec,1]), cand_PCs$rotation[vec,2]/scal+jit*sign(cand_PCs$rotation[vec,2]), labels = vec)
arrows(rep(0,lv), rep(0,lv), all_PCs$rotation[,1]/scal, all_PCs$rotation[,2]/scal, length = 0.0625) 
text(all_PCs$rotation[,1]/scal+jit*sign(all_PCs$rotation[,1]), all_PCs$rotation[,2]/scal+jit*sign(all_PCs$rotation[,2]), labels = vec)

# plot the cluster medoids with ID number
text(clPCs$medoids[,1:2], labels = seq(1,ncl)) 
legend("bottomleft",as.character(seq(1,ncl)), pch=15, cex=1,col=rgb(medcolR,medcolG,medcolB, max = 255))
dev.off()

#export map for use in ArcGIS
greencols=rgb(r,g,b,max=255)
greencols2=col2rgb(greencols)
greencols3=t(greencols2)
gradients=cbind(all_tgrid[c("lon","lat")],greencols3)
write.csv(gradients,file="all_gradients4arcgis.csv")
```




##Bedassle

To quantify the effect of IBD vs IBE across latitude, I'm using Bedassle. This promises to give a ratio of IBD/IBE, ie. difference of x in temp = xx m geographic distance. I can potentially use this to compare latitude and elevation later on. 




