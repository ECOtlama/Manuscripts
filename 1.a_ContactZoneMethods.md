#Methods for Contact zone MS

NBNB 

Popgen code: 

https://popgencode.wordpress.com

https://github.com/rystanley/genepopedit




Questions:

1. Are Alps barrier to dispersal?

2. Contact zone - How genomically distinct?

3. Is differentiation structured or bias in introgression?

4. For biased loci, any association with temperature/elevation?








##Sampling

To determine the genomic cline across the contact zone, populations were sampled from pure northern mitochondrial haplotypes across the contact zone including pure southern mitochonridal haplotypes. 

Reduced representation libraries were produced using the double digest Restriction Associated DNA (ddRAD) sequencing protocol (Peterson et al. 2012) with some modifications. Specifically, EcoRI and MSeI were chosen as restriction enzymes. 

Single end libraries were sequenced for 150bp on an Illumina xx. 

##Datasets

###1. SNP calling with pyRAD

###2. Generate datasets
    
    a. subset: 2Mil reads per sample (downsampled .trim reads and rerun pyRAD)
    
    b. Full dataset. 
    
    
a. subset: 

This was run on gdcsrv. Located at alexjvr@gdcsrv1.ethz.ch:/gdc_home4/alexjvr/CH.Phylogenomics/outfiles_subset.CH.Phylo

Filtering: 

1. Keep only the EAST individuals

2. MAC

3. Missingness

4. Convert to Plink and filter for Het >0.7


####1. Keep all the EAST individuals

Got the list of names from /Users/alexjvr/2016RADAnalysis/1_Phylo/GenomicClines/EAST.names

copied it to the server and renamed to the full names (including cat & .fq.trim)

```
vcftools --vcf subset.CH.Phylo.vcf --keep EAST.names --recode --recode-INFO-all --out subset.EAST.s1
```

Kept 118 out of 230 individuals 


####2. Filters

Filter for missingness of 0.5 and MAC 3 (3/(118*2) = 1.2% MAF)

50% genotyping rate. And MAC of 3. (across 230 indivs = 3/460 = 0.65%) - I should probably increase this! Rather use a MAF of 1%
```
vcftools --vcf subset.EAST.s1.recode.vcf --max-missing 0.5 --mac 3 --recode --recode-INFO-all --out subset.EAST.s2
```

Output: 
```
Parameters as interpreted:
	--vcf subset.EAST.s1.recode.vcf
	--recode-INFO-all
	--mac 3
	--max-missing 0.5
	--out subset.EAST.s2
	--recode

After filtering, kept 118 out of 118 Individuals
Outputting VCF file...
After filtering, kept 30981 out of a possible 1063455 Sites
Run Time = 48.00 seconds
```


And check the missingness for the individuals: 
```
vcftools --vcf subset.EAST.s2.recode.vcf --missing-indv

mawk '!/IN/' out.imiss | cut -f5 > totalmissing

gnuplot << \EOF 
set terminal dumb size 120, 30
set autoscale 
unset label
set title "Histogram of % missing data per individual"
set ylabel "Number of Occurrences"
set xlabel "% of missing data"
#set yr [0:100000]
binwidth=0.01
bin(x,width)=width*floor(x/width) + binwidth/2.0
plot 'totalmissing' using (bin( $1,binwidth)):(1.0) smooth freq with boxes
pause -1
EOF
```

Output: 

![alt_txt][Fig1]
[Fig1]:https://cloud.githubusercontent.com/assets/12142475/17170713/47a3577e-53e5-11e6-9c2f-9c422b357f6b.png


All of the samples have <50% missing data. 


And if I try with --max-missing 0.8 followed by the rest of the filtering 

```
vcftools --vcf CH.Phylo.vcf.recode.vcf --max-missing 0.8 --mac 3 --recode --recode-INFO-all --out s1.Phylo.RAD.vcf

Parameters as interpreted:
	--vcf subset.EAST.s1.recode.vcf
	--recode-INFO-all
	--mac 3
	--max-missing 0.8
	--out subset.EAST.s2
	--recode

After filtering, kept 118 out of 118 Individuals
Outputting VCF file...
After filtering, kept 5802 out of a possible 1063455 Sites
Run Time = 43.00 seconds
```

![alt_txt][Fig2]
[Fig2]:https://cloud.githubusercontent.com/assets/12142475/17170898/1e328a4e-53e6-11e6-9865-140dbbd990c9.png

So <0.3 missing data. 


Rename the samples in vcf file: 

First get a list of all the samples: 
```
bcftools query -l subset.imiss80.recode.vcf

```

copy and paste this to excel. And rename accordingly (I remove the "cat" and ".fq.trim"). Nano and paste into a new file. 

Paste back: 
```
bcftools reheader subset.EAST.s2.recode.vcf -s EAST.names -o subset.EAST.s3.vcf
```


###3. Thin to 1 SNP per locus: 

```
vcftools --vcf subset.EAST.s3.vcf --thin 500 --recode --recode-INFO-all --out subset.EAST.thinned.vcf

Parameters as interpreted:
	--vcf subset.EAST.s3.vcf
	--recode-INFO-all
	--thin 500
	--out subset.EAST.thinned.vcf
	--recode

After filtering, kept 118 out of 118 Individuals
Outputting VCF file...
After filtering, kept 2399 out of a possible 5802 Sites
Run Time = 0.00 seconds
```

####4.Filter for >0.7 obs Het

Based on my recent checks on the pyRAD data, I should also filter all SNPs with >0.7 observed Heterozygosity. 

I will do this in R using the PLINK file.

Convert to plink
```
vcftools --vcf subset.EAST.s4THIN.recode.vcf --plink --out subset.EAST.s4
```

Calculate HWE for all loci in PLINK
```
plink --file subset.EAST.s4 --hardy
```

PLINK output plink.hwe has a very strange format - multiple spaces between columns - so I couldn't figure out how to cut a specific column using linux

I sorted everything in excel.

There are only 24 SNPs with O.Het >0.6 (i.e. 0.19%) 

62398:4
78226:93
133358:7
267920:70
39465:59
126930:112
19060:14
286067:81
178295:84
173522:78
79355:21
140229:67
277990:41
161923:106
133065:51
191402:71
3053:75
61372:10
157987:33
19818:115
229342:14
758:72
17377:3
247581:17

Remove with plink 
```
nano SNPstoexclude.txt

plink --file subset.EAST.s4 --exclude SNPtoexclude.txt --recodeA --recode --out subsetEAST.Final
```

Final dataset: 

118 individuals

2358 SNPs 

0.885 Genotyping rate

Use pgdspider to convert .ped PLINK file to vcf. And keep plink and vcf files on mac: 

/Users/alexjvr/2016RADAnalysis/1.2_Phylo/input.files/

And replace the headers
    
##Population Structure

Dataset 1. 

###1. Geographic prior

   #### a. TESS3
 
 TESS3 uses a new method to infer ancestry: Geographically constrained least-squares estimation of ancestry coefficients.  
http://onlinelibrary.wiley.com/doi/10.1111/1755-0998.12471/epdf

K is chosen by evaluating the cross-entropy criterion for each K. This method finds the minimum number of "bits" or samples from a normal probability distribution (p) that can predict a non-normal probability distribution (q). So the smaller this number is, the better the K. 


R package has been released in devtools: 

https://github.com/cayek/TESS3/blob/master/README.md

I need to use LEA to convert my data into TESS3 format: 

For this I had to upgrade R. The following link shows how to set up R-studio to use different versions of R: 

https://support.rstudio.com/hc/en-us/articles/200486138-Using-Different-Versions-of-R

In command line: 

```
export RSTUDIO_WHICH_R=/usr/local/bin/R

open -a rstudio
```

Make sure that R3.2.5 is launched. 

LEA is a bioconductor package. 

http://www.bioconductor.org/packages/release/bioc/html/LEA.html


In R: 

```
source("http://bioconductor.org/biocLite.R")
biocLite("LEA")

library(LEA)

setwd(/Users/alexjvr/2016RADAnalysis/1_Phylo/TESS)
output = vcf2geno("CH.230.Phylo.FINAL.vcf")
```

```
	- number of detected individuals:	118
	- number of detected loci:		2358

For SNP info, please check ./subsetEAST.Final.vcfsnp.

0 line(s) were removed because these are not SNPs.
Please, check ./subsetEAST.Final.removed file, for more informations.
```

Now I need the .coords file, which is a file with a lat & long column for each individual (no individual names). 

list all the samples in the vcf file

```
bcftools query -l subsetEAST.Final.vcf
```

```
nano subsetEAST.Final.coords  ##paste all the coords into this file
```

To run TESS3: 

The executable needs to be copied to the current directory
```
cp ~/Applications/TESS3-master/build/TESS3 .

./TESS3 -x subsetEAST.Final.geno -r subsetEAST.coords -K 2 -q K2.1.Q -g K2.1.G -f K2.1.Fst -y K2.1.sum -c 0.05
```

Run this for K 1-5 x 10 iterations. 

-I can be used to select a random subset of samples. But this full dataset ran in ~10sec, so probably not necessary. 

-y = least-squares criterion

-c = percentage of the masked genotypes. (0.05 by default). If this is set, the cross-entropy criterion is calculated. 

-i = max nr of iterations. (default = 200)

Paste all of the entropy scores into a csv file. And plot in R: 

```
####################################
######Graph of cross-entropy scores

setwd("/Users/alexjvr/2016RADAnalysis/1_Phylo/TESS")
library(ggplot2)

CHS.entropy <- read.csv("Cross-entropy.scores.CHS.Brown.csv")
CHS.entropy <- as.data.frame(CHS.entropy)
CHS.entropy

ggplot(CHS.entropy, aes(x=CHS.entropy$K, y=CHS.entropy$Cross.entropy)) + geom_point(shape=1) + ggtitle("Cross-entropy for CHS subset") + ylab("Cross-entropy") + xlab("K")
```



From the Min-Entropy graph, K = 2

I interpret this as the biggest change in cross-entropy scores, as they do in this tutorial: http://membres-timc.imag.fr/Olivier.Francois/tutoRstructure.pdf

![alt_txt][Fig3]
[Fig3]:https://cloud.githubusercontent.com/assets/12142475/17185505/4631bba8-5429-11e6-9c3b-300fb94c6f9e.png


```
###Graphic display of TESS output
#########
setwd("/Users/alexjvr/2016RADAnalysis/1_Phylo/TESS")

install.packages("fields")
install.packages("RColorBrewer")
source("MapDisplay/POPSutilities.R")

Qmatrix <- read.table("CH.230.Phylo.FINAL.3.Q")
coords <- read.table("CH.230.new.coords")
plot(coords, pch = 19, xlab = "Longitude", ylab= "Latitude")
#?map
map(add = T, boundary = T, interior = T, col = "grey80")

asc.raster=("srtm_38_02.asc")
asc.raster
grid=createGridFromAsciiRaster(asc.raster)
constraints=getConstraintsFromAsciiRaster(asc.raster,cell_value_min=0)   ##constrains the map to the raster file size
maps(matrix = Qmatrix, coords, grid, method = "max", main = "Ancestry coefficients", xlab = "Longitude", ylab = "Latitude")


```
Remember to swap long & lat from the order in my own data. And move MapDisplay and ascii files to the working directory. 


Tess3 graph for K=2 and K=3

![alt_txt][EAST.K2]
[EAST.K2]:https://cloud.githubusercontent.com/assets/12142475/17185599/9b31e2c2-5429-11e6-9278-9a99ace217d9.png


![alt_txt][EAST.K3]
[EAST.K3]:https://cloud.githubusercontent.com/assets/12142475/17185606/a03470e6-5429-11e6-9c50-58ad4caeb3d4.png

    
   ### b. sPCA

###2. Non-geographic prior

   ### a. fastStructure
    
Information here:

https://rajanil.github.io/fastStructure/

http://phylobotanist.blogspot.co.uk/2014/08/trying-to-use-faststructure.html

####Run fastStructure

Convert input to Structure format using pgdSpider. Choose the specific fastStructure format. And change marker type to SNPs. Everything else should be left as is. All the columns are in the PLINK files.

fastStructure can be run from the Applications folder, or specify the path in bash: 

```
python /Users/alexjvr/Applications/fastStructure/fastStructure/structure.py -K 1 --format=str --input=subsetEAST.Final --output=subsetEAST_K1.1
```

I haven't figured out how to write a script to loop through fastStructure, but change the output file for each run. I.e. I have to manually run K 1-10 x 10 runs. According to the google group, this is how it has to be done. The runs take just a few seconds each.


####Assessing K:

```    
python /Users/alexjvr/Applications/fastStructure/fastStructure/chooseK.py --input=subsetEAST_*


Model complexity that maximizes marginal likelihood = 2
Model components used to explain structure in data = 3
```

chooseK.py gives a range of best K, rather than an optimal K. So this is not very useful for me!!

https://groups.google.com/forum/#!topic/structure-software/s_rc_ueq6CU

Google groups suggests comparing plots for all optimal K, here K2-3. The way I've chosen the populations, I'm trying to differentiate between two clades with secondary contact. I'm quite confident that K=2, based on my sampling scheme. So I will compare both K, but wil work with K=2. 

In stead of choosing the most likely K as before in Structure, use CLUMPP to average over all the iterations for the most likely K.

####CLUMPP

http://web.stanford.edu/group/rosenberglab/papers/clumppNote.pdf

    
   ### b. DAPC
    


##Hybrid Index calculation

Dataset 2. 

1. Identify "pure" individuals

2. Simulate hybrids

3. Calculate hybrid index of all individuals. 


##Genomic Cline Analysis

I'm using the subset data (2358 loci, 118 individuals) to test the Genomic Cline Analysis in Introgress. 

Manual: 

https://cran.r-project.org/web/packages/introgress/introgress.pdf


1. Download introgress package in R

Input files needed: 

1. admix.dat = transposed .str file of genotypes of all admixed individuals (or non-parentals)

2. P1 & P2 = same as admix.dat, but for each of the parents. 

	For inputs 1 & 2, first split the vcf file into the three datasets. Then convert to .str using pgdspider. Lastly, transpose in R.  
	
I need the following files for this: 

1. admix.gen = transposed .str of all admixed individuals

2. loci.dat = info on all the markers (column 1 = marker name, column 2 = Dominance (c), column 3 (opt) = linkage grp

3. Parental 1 & 2 = same format as the admix.gen file.  

	
Rename the sample names in subsetEAST.Final.vcf  --out subsetEAST.Final.2.vcf


```
#list of names in vcf file

bcftools query -l subsetEAST.Final.2.vcf

nano CHS.P2.names

nano CHN.P2.names

nano EAST.admix.names

###Split dataset in bash

```

**I'm having trouble splitting my .vcf file. 

Error: ID required in FORMAT field description: T,1,String,"Genotype"

In one of the working .vcf files, this is how FORMAT is coded: 

##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">

i.s.o

##FORMAT=GT,1,String,"Genotype"

Once I corrected this with nano, the file worked. 

```
##Split the dataset with bash: 

vcftools --vcf subsetEAST.Final.2.vcf --keep CHN.P1.names --recode --recode-INFO-all --out CHN.P1

Keeping individuals in 'keep' list
After filtering, kept 15 out of 118 Individuals
Outputting VCF file...
After filtering, kept 2358 out of a possible 2358 Sites

vcftools --vcf subsetEAST.Final.2.vcf --keep CHS.P2.names --recode --recode-INFO-all --out CHS.P2

Keeping individuals in 'keep' list
After filtering, kept 11 out of 118 Individuals
Outputting VCF file...
After filtering, kept 2358 out of a possible 2358 Sites

vcftools --vcf subsetEAST.Final.2.vcf --keep EAST.admix.names --recode --recode-INFO-all --out EAST.admix

Keeping individuals in 'keep' list
After filtering, kept 92 out of 118 Individuals
Outputting VCF file...
After filtering, kept 2358 out of a possible 2358 Sites


```

Convert to .str using pgdspider

In nano add "name" tab "pop" to the header otherwise it won't read into R. 


###2. loci.dat

loci.dat = column 1: locus names, column 2 (type): c (for co-dominant markers)
	
	Generate this by copying all the locus names from the structure file and pasting c in the second column.  


```
head -1 CHN.P1.str > loci.names
```

nano & delete "name" & "pop"

Transpose with excel


Transpose

Tried some code (see below), but in the end transposed in excel and added a column with c (dominance). 

**** Couldn't get this to work!!

n is nr of columns to be printed (should match dashes in "paste")

-F is the separator 

```
rowcount=2358
for (( i=1; i<=rowcount; i++ )); do
    awk -v i="$i" -F : '{printf("%s\t ", $i)}' loci.names 
    echo | awk '{print}' >qq.tmp
done
```


###3. P1 & P2

Correct P1 & P2 structure files by removing individual and pop info: 

```
cut -f 3- CHS.P2.str > CHS.P2.str.nonames
```


##R

```
###INTROGRESS for genomic cline analysis

setwd("/Users/alexjvr/2016RADAnalysis/1.2_Phylo/Introgress/")
library(introgress)

EAST <- read.table("EAST.admix.str")
EAST.1 <- as.matrix(EAST)
rawdata <- t(EAST.1)

P1 <- read.table("CHN.P1.str.nonames")
P1.1 <- as.matrix(P1)
P1.2 <- t(P1.1)

P2 <- read.table("CHS.P2.str.nonames")
P2.1 <- as.matrix(P2)
P2.2 <- t(P2.1)

loci.names <- read.table("loci.names2")
loci.names.2 <- as.matrix(loci.names)
loci.names.2

EASTnew <- prepare.data(admix.gen = rawdata, loci.data = loci.names.2, parental1 = P1.2, parental2 = P2.2, pop.id=T, ind.id=T, fixed=F, sep.rows=F)

```


##UPDATE

Introgress on their website has a disclaimer: Please note that while introgress does a fine job at summarizing and detecting variation in introgression among loci, it does not necessarily lead to strong inferences about the causes of the variation (e.g., detection of selected loci). For example, variation can arise due to genetic drift and does so commonly under certain reasonable conditions. For detecting expectionally differentiated loci, with less sensititivity to genetic drift, we suggest using bgc. Please see Gompert and Buerkle 2012 for a comparison of bgc and introgress. In both cases please be aware of the mutliple evolutionary processes that can lead to expectional differentiation.

They suggest using bgc in stead. 



#BGC 

Bayesian Genomic Cline Analysis

https://sites.google.com/site/bgcsoftware/

I just found a conversion package that's under development. I'll see if this works: 

https://github.com/rystanley/genepopedit

##Install Genepopedit

This doesn't work on older versions of R, so I'm running R from the command line: 

```
install.packages("devtools") # to install

devtools::install_github("rystanley/genepopedit")

library(genepopedit) # load the library
```

There was a problem with package stringr ->

I had to remove a folder and then reinstall this package separately. 

http://stackoverflow.com/questions/31866299/cannot-install-ggplot2-error-in-library-dynamlib-package-package-lib-sha

Then install genepopedit as above. 


Genepopedit takes genepop format. But it also needs the indiv names to be in a specific format: 

pop_indivname

So rename all the individuals with the following populations: P1, P2, Admix (you can use the normal pop names if the pops perfectly correspond with P1, P2 and Admix. But since I have multiple individuals per pop falling into the different catagories, this won't work)

```
bcftools reheader subsetEAST.Final.2.vcf -s genepopedit.names -o subsetEAST.newnames.vcf
```


Convert the correct vcf into genepop format using pgdspider

```
subsetEAST.newnames.vcf  ##renamed individuals

##make pop file, but use P1, P2, and admix as the populations

P1_indname	P1
P2_indname 	P2
...

BGC.newnames ##pop file for the .spid file
```

R

```
setwd("/Users/alexjvr/2016RADAnalysis/1.2_Phylo/BGC")

output_dir <- "/Users/alexjvr/2016RADAnalysis/1.2_Phylo/BGC/"

#specify which populations are going to be included in the analysis and to which class they belong. Note Pops identified in P1 and-or P2 can also be specified as "Admixed" to test BGC output. 
  BGC_groups = data.frame(pops = c("Admix","P1","P2"),groups = c("Admixed","P1","P2"))

#convert Genepop to BGC input files (3). Note in this case the variable path is a path to the directory where the input files will be saved.
  genepop_bgc(genepop="subsetEAST.genepop.txt",popdef = BGC_groups, fname="BGC_EAST",path = output_dir)
```



Input files: 

1. Parental files

list of loci with total allele counts in the population

locus 1
count count
locus 2
count count

2. Admix file

list of each locus, each pop, and allele count for each individual at each locus. 

locus 1
pop0
1 1
2 0 
...
locus 2



How to do it manually: 

```

###1 Parental file

Get the allele counts using vcftools

```
vcftools --vcf subsetEAST.Final.2.vcf --counts --recode-INFO-all --out subsetEAST.Final.2.allele.counts
```

Cut and paste the allele counts in excel. Text to column and then nano into a new file

```
nano P1.allelecounts
```

Use the textfile from before with the locus names in and create the parental input files. Do this for both parental files: 

```
paste -d '\n' locusnames P1.allelecounts > P1.BGCinput
```

###2 Admix file

Change all the allele counts to 012 coding: this counts the number of non-reference alleles at that locus in a particular individual. 

```
vcftools --vcf input.files/EAST.admix.recode.vcf --012 --out BGC/EAST.admix.count
```

open this in excel. Add in column headers (locus names), and pop (i.e. first two rows). Paste back with nano. 

Replace all missing data (-1) with -9. 

```
sed -i -e 's/-1/-9/g' EAST.admix.count.012_2
```
```

###BGCanalysis

Now we have the input files from genepopedit: 

Gompert et al. 2012 (Mol Ecol Resources)

and BGC R manual

####Install BGC

```
h5c++ -Wall -O2 -o bgc bgc_main.C bgc_func_readdata.C bgc_func_initialize.C bgc_func_mcmc.C bgc_func_write.C bgc_func_linkage.C bgc_func_ngs.C bgc_func_hdf5.C mvrandist.c -lgsl -lm
```

I kept getting an error: Fatal error, missing omp.h. 

I thought this might be a problem with gcc/c++ on the mac. I've reinstalled gcc with the latest version and exported to $PATH, but still get this error. 

Christine said she deleted "omp.h" from all the .C files in the BGC folder (for me this was bgc_main.C and bgc_mcmc.C). 

This took care of the error, but now bgc doesn't run, and I can't autocomplete any of the files in the folder, even though I can see them. I can't understand what the problem is. 

Had BGC installed on the GDC server: /usr/local/bgc-1.03/bgc

Copied my input files onto the server and ran bgc: 

```
/gdc_home4/alexjvr/CH.ContactZone/bgc

/usr/local/bgc-1.03/bgc -a BGC_EAST_Parental1_BGC.txt -b BGC_EAST_Parental2_BGC.txt -h BGC_EAST_Admixed_BGC.txt -x 125000 -n 25000 -p 1 -q 1 -N 1 -t 10 -E 0.0001

Reading input files
Number of loci: 2358
Number of admixed populations: 1
Number of individuals: 92
Allowing for uncertainty in allele counts
Allocating memory
Initializing MCMC chain
Initialization complete
mcmc iteration: 0 ........ LnL: -235733
mcmc iteration: 1000 ........ LnL: -221988
mcmc iteration: 2000 ........ LnL: -223612
mcmc iteration: 3000 ........ LnL: -222707
mcmc iteration: 4000 ........ LnL: -220551
mcmc iteration: 5000 ........ LnL: -224167
mcmc iteration: 6000 ........ LnL: -220827
mcmc iteration: 7000 ........ LnL: -222873
mcmc iteration: 8000 ........ LnL: -229443
mcmc iteration: 9000 ........ LnL: -225135
mcmc iteration: 10000 ........ LnL: -227014
mcmc iteration: 11000 ........ LnL: -239949
...
mcmc iteration: 124000 ........ LnL: -231983
Finished mcmc
Runtime: 38 hr 19 min 26 sec
```

Time taken: ~39 hours


For next gen data, 2 independent runs need to be assessed. 

Tuning parameters may need adjustment to achieve sufficient mixing of the mcmc chains. 

Evaluate mixing and convergence by 

1. convert the output from hdf5 to ascii using estpost

2. plotting in R

```
/usr/local/bgc-1.03/estpost -i mcmcout.hdf5 -p alpha -o a.out -s 0 -c 0.95 -w 0
parameter dimensions for alpha: loci = 2358, samples = 10000, pouplations = 1

estpost -i mcmcout.hdf5 -p beta -o b.out -s 0 -c 0.95 -w 0
parameter dimensions for beta: loci = 2358, samples = 10000, pouplations = 1

estpost -i mcmcout.hdf5 -p LnL -o LnL.out -s 2 -c 0.95 -w 0
parameter dimensions for LnL: samples = 10000
```

copy over to mac and plot in R

```
scp alexjvr@gdcsrv1.ethz.ch:/gdc_home4/alexjvr/CH.ContactZone/bgc/*out .
```


***I'm doing this in R now, but this is good to know: 

add column of row numbers as the x variable  
```
awk '$1=(FNR FS $1)' a.out > a.out2
```

R code: Plot alpha, beta, and LnL to check for convergence. If necessary, adjust burn-in and run parameters (see bgc manual). And run a second independent bgc analysis (i.e. 2 total). 
```
####Plots of BGC
##test for convergence

setwd("/Users/alexjvr/2016RADAnalysis/1.2_Phylo/BGC")

##alpha1
a1 <- read.csv("a.out", header = F)

colnames(a1) <- c("y1", "y2", "y3", "y4")  ##add column names
head(a1)

a1$x <- 1:nrow(a1) ##add column x
head(a1)


library("ggplot2")
library("reshape")

a1.melt <- melt(a1, id.vars=5)
head(a1.melt)

ggplot(a1.melt, aes(x, value, colour=variable)) +
  geom_point() +
  scale_color_manual(values = c("black", "black", "black", "black"))+
  xlab("mcmc") +
  ylab("alpha1") +
  ggtitle("EASTsubset alpha BGC run1") +
  theme(legend.position="none")

##beta1
b1 <- read.csv("b.out", header = F)

colnames(b1) <- c("y1", "y2", "y3", "y4")  ##add column names
head(b1)

b1$x <- 1:nrow(b1) ##add column x
head(b1)


library("ggplot2")
library("reshape")

b1.melt <- melt(b1, id.vars=5)
head(b1.melt)

ggplot(b1.melt, aes(x, value, colour=variable)) +
  geom_point() +
  scale_color_manual(values = c("black", "black", "black", "black"))+
  xlab("mcmc") +
  ylab("beta1") +
  ggtitle("EASTsubset beta BGC run1") +
  theme(legend.position="none")
  
##LnL1  
##This uses a function tcsv that transforms the data and reads in the header. I.e. can keep the numeric format, while specifying a text header
###Add "LnL," to the start of the file using nano in bash

read.tcsv = function(file, header=TRUE, sep=",", ...) {
  
  n = max(count.fields(file, sep=sep), na.rm=TRUE)
  x = readLines(file)
  
  .splitvar = function(x, sep, n) {
    var = unlist(strsplit(x, split=sep))
    length(var) = n
    return(var)
  }
  
  x = do.call(cbind, lapply(x, .splitvar, sep=sep, n=n))
  x = apply(x, 1, paste, collapse=sep) 
  out = read.csv(text=x, sep=sep, header=header, ...)
  return(out)
  
}

LnL1 <- read.tcsv("LnL.out.s2")
head(LnL1)

LnL1$x <- 1:nrow(LnL1) ##add column x
head(LnL1)

ggplot(LnL1, aes(x, LnL1$LnL, colour="black")) +
  geom_point() +
  scale_color_manual(values = c("black"))+
  xlab("mcmc") +
  ylab("LnL1") +
  ggtitle("EASTsubset Log likelihood BGC run1") +
  theme(legend.position="none")

```

Assessing BGC TestRun 1

![alt_txt][BGC_1]
[BGC_1]:https://cloud.githubusercontent.com/assets/12142475/17731320/b03352e6-6464-11e6-8c21-d69e8bfb43ed.png

![alt_txt][BGC_2]
[BGC_2]:https://cloud.githubusercontent.com/assets/12142475/17731321/b0337d3e-6464-11e6-98aa-3bd18ceb186e.png

![alt_txt][BGC_3]
[BGC_3]:https://cloud.githubusercontent.com/assets/12142475/17731322/b036e08c-6464-11e6-892d-1d5795280380.png






#####Run 2: 

From the plots, it looks like I need a longer burn-in. I will increase the run to 100k burn-in + 100k mcmc

```
/usr/local/bgc-1.03/bgc -a BGC_EAST_Parental1_BGC.txt -b BGC_EAST_Parental2_BGC.txt -h BGC_EAST_Admixed_BGC.txt -x 200000 -n 100000 -p 1 -q 1 -N 1 -t 10 -E 0.0001
```

This should take 76hrs to run. I've started the second run as well. 

![alt_txt][Run2]
[Run2]:https://cloud.githubusercontent.com/assets/12142475/17969822/ce57b006-6acb-11e6-9676-fe02a66f1c74.png


LnL still not stabilised. I could use a larger burn-in or start changing the tuning parameters. 


#####Run 3: 

I've changed the tuning params as Christine did in her paper, but using shorter run again: 

```
/usr/local/bgc-1.03/bgc -a /gdc_home4/alexjvr/CH.ContactZone/bgc/BGC_EAST_Parental1_BGC.txt -b /gdc_home4/alexjvr/CH.ContactZone/bgc/BGC_EAST_Parental2_BGC.txt -h /gdc_home4/alexjvr/CH.ContactZone/bgc/BGC_EAST_Admixed_BGC.txt -x 150000 -n 125000 -p 1 -q 1 -N 1 -t 10 -E 0.0001 -u 0.05 -g 0.1, -z 0.025
```

28 hrs runtime. 

So this plots only the last 25k runs (burn-in 125k)

![alt_txt][Run3]
[Run3]:https://cloud.githubusercontent.com/assets/12142475/17969826/d494a366-6acb-11e6-8e9c-f7b2e4ba9a91.png



Asked Christine's opinion: 

```
25 Aug 2016

Hi Alex,

I did spend quite some time tuning the parameters for bgc and was still not completely happy with the outcome at the end…

I mostly varied u, g and z and then (as they are related) I also plotted hi, alpha and beta over iterations.
to plot tau can be helpful as well.
Thats about what Zach suggested me in an email (which I searched for without success :( )
He also said many people run too view iterations and suggested about the numbers I used in the paper.

I think your likelihoods don’t look too bad. If your runs don’t take ages, I would consider running them a bit longer and just using the last 25000 or so.
And have a quick look at hi, alpha and beta over the iterations as well.

Hope that helps!

See you soon! :)

Christine
```


Now running the longer run with the new tuning params and longer burnin: Run4.1 and 4.2
```
/usr/local/bgc-1.03/bgc -a /gdc_home4/alexjvr/CH.ContactZone/bgc/BGC_EAST_Parental1_BGC.txt -b /gdc_home4/alexjvr/CH.ContactZone/bgc/BGC_EAST_Parental2_BGC.txt -h /gdc_home4/alexjvr/CH.ContactZone/bgc/BGC_EAST_Admixed_BGC.txt -x 200000 -n 150000 -p 1 -q 1 -N 1 -t 10 -E 0.0001 -u 0.05 -g 0.1, -z 0.025
```


##Geographic Cline

Geographic clines 
