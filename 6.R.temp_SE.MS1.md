##Population genomics of Rana temporaria across the latitudinal environmental gradient in Scandinavia. 

Adaptation across elevation in Andrew's toad. 

http://onlinelibrary.wiley.com/doi/10.1111/mec.13722/abstract?campaign=woletoc

LHT variation in Rana temporaria across latitude. 

Very little known about their population structure. 

Aim: 

Detect loci associated with environment across latitude

1. Characterise population structure

2. Genome scan for ecologically relevant loci related to the latitudinal gradient. 

3. Spatial interpolation of genomic turnover/ temporal interpolation of genomic turnover (GF + GDM)










***
Compare to Johannsen et al. 2006

http://onlinelibrary.wiley.com/doi/10.1111/j.1365-294X.2006.02866.x/full

 (i) Are there clear geographic patterns of within population genetic variability and degree of among population genetic variability in this species? 
 
 (ii) Are there clear geographic patterns of occurrence and population sizes in this species? 
 
 (iii) Is there evidence that the current patterns of genetic variability and differentiation in this species are better explained by their current demography, rather than their colonization history?

1. What is the fine-scale connectivity within each sampling area?

2. IBD vs IBE?

3. Signatures of selection? 

I will run analyses based on Laurent et al. 2015 (white sand lizards) http://onlinelibrary.wiley.com/doi/10.1111/mec.13385/epdf
****



1. Population Structure 
   
  - PCA
  - TESS3
  - fastStructure
 
  - IBD
  
  - Fst
 
2.1 Outlier detection

2.2 Environmental Association Analysis

3.1 Random Forest

3.2 GDM


##Data

1. All data

2. Subset of the data adjusted for the number of reads (rerun pyRAD)

###Dataset1



###Dataset2 

copy all the .edit files from SEFinalSamples.edits > SEFinalSamples/subset5Mil.SE/edits

count the reads for each sample

```
grep -c ^'>' *edit -> SE.readcount
```

Downsample to 1Mil reads per sample. (Mostly because Finland samples have such few reads, and this population would drop out if I increased the minimum number of reads). 

Use a perl script (random_sequence_sample.pl). Which I've uploaded on gdc (and the mac). 


```
for i in edits/*.fq.trim.edit; do perl random_sequence_sample.pl -i $i -o subset/$i.subset.edit -n 1000000; done
```


###Filter Dataset

####Dataset 2

/gdc_home4/alexjvr/SEFinalSamples/subset1Mil.SE/outfiles/subset.Filter/SEsubset.vcf


Filter for missingness of 0.5 and MAC 3 (3/(118*2) = 1.2% MAF)

50% genotyping rate. And MAC of 3. (across 230 indivs = 3/460 = 0.65%) - I should probably increase this! Rather use a MAF of 1%

```
vcftools --vcf SEsubset.vcf --max-missing 0.5 --mac 3 --recode --recode-INFO-all --out SEsubset.vcf.s2

Output:

Parameters as interpreted:
	--vcf SEsubset.vcf
	--recode-INFO-all
	--mac 3
	--max-missing 0.5
	--out SEsubset.vcf.s2
	--recode

Eighth Header entry should be INFO: INFO    
After filtering, kept 177 out of 177 Individuals
Outputting VCF file...
After filtering, kept 19866 out of a possible 410602 Sites
Run Time = 28.00 seconds
```

And check the missingness for the individuals:

```
vcftools --vcf SEsubset.vcf.s2.recode.vcf --missing-indv

mawk '!/IN/' out.imiss | cut -f5 > totalmissing

gnuplot << \EOF 
set terminal dumb size 120, 30
set autoscale 
unset label
set title "Histogram of % missing data per individual"
set ylabel "Number of Occurrences"
set xlabel "% of missing data"
#set yr [0:100000]
binwidth=0.01
bin(x,width)=width*floor(x/width) + binwidth/2.0
plot 'totalmissing' using (bin( $1,binwidth)):(1.0) smooth freq with boxes
pause -1
EOF
```

Output:

![alt_txt][Fig.1]
[Fig.1]:https://cloud.githubusercontent.com/assets/12142475/17670563/8015ec8c-630a-11e6-8822-646a217ea4eb.png

Most of the samples have <50% missing data.

And if I try with --max-missing 0.8 followed by the rest of the filtering

```
vcftools --vcf SEsubset.vcf --max-missing 0.8 --mac 3 --recode --recode-INFO-all --out SEsubset.s2_0.8

Parameters as interpreted:
	--vcf SEsubset.vcf
	--recode-INFO-all
	--mac 3
	--max-missing 0.8
	--out SEsubset.s2_0.8
	--recode

Eighth Header entry should be INFO: INFO    
After filtering, kept 177 out of 177 Individuals
Outputting VCF file...
After filtering, kept 4929 out of a possible 410602 Sites
Run Time = 24.00 seconds
```

![alt_txt][Fig.2]
[Fig.2]:https://cloud.githubusercontent.com/assets/12142475/17670824/aab2b5fa-630b-11e6-8d23-cdb6de5be98a.png

This is significantly less loci, and not much better i.t.o. missingness. So I will use the first filter, and remove the 7 individuals with >0.5 missingness

```
--vcf SEsubset.vcf.s2.recode.vcf --remove remove.names --recode --recode-INFO-all --out SEsubset.s3

VCFtools - v0.1.12b
(C) Adam Auton and Anthony Marcketta 2009

Parameters as interpreted:
	--vcf SEsubset.vcf.s2.recode.vcf
	--exclude remove.names
	--recode-INFO-all
	--out SEsubset.s3
	--recode

Excluding individuals in 'exclude' list
After filtering, kept 167 out of 177 Individuals
Outputting VCF file...
After filtering, kept 19866 out of a possible 19866 Sites
Run Time = 3.00 seconds

```


Rename the samples in vcf file:

First get a list of all the samples:

```
bcftools query -l SEsubset.s3.recode.vcf
```

copy and paste this to excel. And rename accordingly (I remove the "cat" and ".fq.trim"). Nano and paste into a new file.

Paste back:

```
bcftools reheader SEsubset.s3.recode.vcf -s new.names -o SEsubset.s3.vcf
```

3. Thin to 1 SNP per locus:

```
vcftools --vcf SEsubset.s3.vcf --thin 500 --recode --recode-INFO-all --out SEsubset.s3.thinned.vcf

Parameters as interpreted:
	--vcf SEsubset.s3.vcf
	--recode-INFO-all
	--thin 500
	--out SEsubset.s3.thinned.vcf
	--recode

After filtering, kept 167 out of 167 Individuals
Outputting VCF file...
After filtering, kept 5290 out of a possible 19866 Sites
Run Time = 1.00 seconds
```

4.Filter for >0.6 obs Het

Based on my recent checks on the pyRAD data, I should also filter all SNPs with >0.7 observed Heterozygosity.

I will do this in R using the PLINK file.

Convert to plink

```
vcftools --vcf SEsubset.s3.thinned.vcf.recode.vcf --plink --out SEsubset.s4
```

Calculate HWE for all loci in PLINK

```
plink --file SEsubset.s4 --hardy
```

PLINK output plink.hwe has a very strange format - multiple spaces between columns - so I couldn't figure out how to cut a specific column using linux

I sorted everything in excel.

There are only 30 SNPs with O.Het >0.6 (i.e. 0.56%)

```
923:86
45299:91
131938:21
31269:23
15018:23
107525:44
80914:106
109021:9
53856:9
31387:44
88409:3
116529:47
17502:52
3086:06:00
33252:37
62935:14
121937:13
85675:18
132142:1
45819:46
75796:21
58278:85
130290:76
71033:77
95774:102
121298:22
67455:2
100258:5
113742:72
26:17:00
```

Remove with plink

```
nano SNPstoexclude.txt

plink --file SEsubset.s4 --exclude SNPtoexclude.txt --recodeA --recode --out SEsubset.Final
```

Final dataset:

167 individuals

5217 SNPs

0.706 Genotyping rate

Use pgdspider to convert .ped PLINK file to vcf. And keep plink and vcf files on mac:

/Users/alexjvr/2016RADAnalysis/SE.MS1/input.files/

And replace the headers


###Summary Statistics




###1. Population Structure

####1. Geographic prior

   ##### a. TESS3
 
 TESS3 uses a new method to infer ancestry: Geographically constrained least-squares estimation of ancestry coefficients.  
http://onlinelibrary.wiley.com/doi/10.1111/1755-0998.12471/epdf

K is chosen by evaluating the cross-entropy criterion for each K. This method finds the minimum number of "bits" or samples from a normal probability distribution (p) that can predict a non-normal probability distribution (q). So the smaller this number is, the better the K. 


R package has been released in devtools: 

https://github.com/cayek/TESS3/blob/master/README.md

I need to use LEA to convert my data into TESS3 format: 

For this I had to upgrade R. The following link shows how to set up R-studio to use different versions of R: 

https://support.rstudio.com/hc/en-us/articles/200486138-Using-Different-Versions-of-R

In command line: 

```
export RSTUDIO_WHICH_R=/usr/local/bin/R

open -a rstudio
```

Make sure that R3.2.5 is launched. 

LEA is a bioconductor package. 

http://www.bioconductor.org/packages/release/bioc/html/LEA.html


In R: 

```
source("http://bioconductor.org/biocLite.R")
biocLite("LEA")

library(LEA)

setwd(/Users/alexjvr/2016RADAnalysis/1_Phylo/TESS)
output = vcf2geno("CH.230.Phylo.FINAL.vcf")
```

```
	- number of detected individuals:	118
	- number of detected loci:		2358

For SNP info, please check ./subsetEAST.Final.vcfsnp.

0 line(s) were removed because these are not SNPs.
Please, check ./subsetEAST.Final.removed file, for more informations.
```

Now I need the .coords file, which is a file with a lat & long column for each individual (no individual names). 

list all the samples in the vcf file

```
bcftools query -l subsetEAST.Final.vcf
```

```
nano subsetEAST.Final.coords  ##paste all the coords into this file
```

To run TESS3: 

The executable needs to be copied to the current directory
```
cp ~/Applications/TESS3-master/build/TESS3 .

./TESS3 -x subsetEAST.Final.geno -r subsetEAST.coords -K 2 -q K2.1.Q -g K2.1.G -f K2.1.Fst -y K2.1.sum -c 0.05
```

Run this for K 1-5 x 10 iterations. 

-I can be used to select a random subset of samples. But this full dataset ran in ~10sec, so probably not necessary. 

-y = least-squares criterion

-c = percentage of the masked genotypes. (0.05 by default). If this is set, the cross-entropy criterion is calculated. 

-i = max nr of iterations. (default = 200)

Paste all of the entropy scores into a csv file. And plot in R: 

```
####################################
######Graph of cross-entropy scores

setwd("/Users/alexjvr/2016RADAnalysis/1_Phylo/TESS")
library(ggplot2)

CHS.entropy <- read.csv("Cross-entropy.scores.CHS.Brown.csv")
CHS.entropy <- as.data.frame(CHS.entropy)
CHS.entropy

ggplot(CHS.entropy, aes(x=CHS.entropy$K, y=CHS.entropy$Cross.entropy)) + geom_point(shape=1) + ggtitle("Cross-entropy for CHS subset") + ylab("Cross-entropy") + xlab("K")
```



From the Min-Entropy graph, K = 2

I interpret this as the biggest change in cross-entropy scores, as they do in this tutorial: http://membres-timc.imag.fr/Olivier.Francois/tutoRstructure.pdf

![alt_txt][Fig3]
[Fig3]:https://cloud.githubusercontent.com/assets/12142475/17185505/4631bba8-5429-11e6-9c3b-300fb94c6f9e.png


```
###Graphic display of TESS output
#########
setwd("/Users/alexjvr/2016RADAnalysis/1_Phylo/TESS")

install.packages("fields")
install.packages("RColorBrewer")
source("MapDisplay/POPSutilities.R")

Qmatrix <- read.table("CH.230.Phylo.FINAL.3.Q")
coords <- read.table("CH.230.new.coords")
plot(coords, pch = 19, xlab = "Longitude", ylab= "Latitude")
#?map
map(add = T, boundary = T, interior = T, col = "grey80")

asc.raster=("srtm_38_02.asc")
asc.raster
grid=createGridFromAsciiRaster(asc.raster)
constraints=getConstraintsFromAsciiRaster(asc.raster,cell_value_min=0)   ##constrains the map to the raster file size
maps(matrix = Qmatrix, coords, grid, method = "max", main = "Ancestry coefficients", xlab = "Longitude", ylab = "Latitude")


```
Remember to swap long & lat from the order in my own data. And move MapDisplay and ascii files to the working directory. 


Tess3 graph for K=2 and K=3

![alt_txt][EAST.K2]
[EAST.K2]:https://cloud.githubusercontent.com/assets/12142475/17185599/9b31e2c2-5429-11e6-9278-9a99ace217d9.png


![alt_txt][EAST.K3]
[EAST.K3]:https://cloud.githubusercontent.com/assets/12142475/17185606/a03470e6-5429-11e6-9c50-58ad4caeb3d4.png

    
   ### b. sPCA

###2. Non-geographic prior

   ### a. fastStructure
    
Information here:

https://rajanil.github.io/fastStructure/

http://phylobotanist.blogspot.co.uk/2014/08/trying-to-use-faststructure.html

####Run fastStructure

Convert input to Structure format using pgdSpider. Choose the specific fastStructure format. And change marker type to SNPs. Everything else should be left as is. All the columns are in the PLINK files.

fastStructure can be run from the Applications folder, or specify the path in bash: 

```
python /Users/alexjvr/Applications/fastStructure/fastStructure/structure.py -K 1 --format=str --input=subsetEAST.Final --output=subsetEAST_K1.1
```

I haven't figured out how to write a script to loop through fastStructure, but change the output file for each run. I.e. I have to manually run K 1-10 x 10 runs. According to the google group, this is how it has to be done. The runs take just a few seconds each.


####Assessing K:

```    
python /Users/alexjvr/Applications/fastStructure/fastStructure/chooseK.py --input=subsetEAST_*


Model complexity that maximizes marginal likelihood = 2
Model components used to explain structure in data = 3
```

chooseK.py gives a range of best K, rather than an optimal K. So this is not very useful for me!!

https://groups.google.com/forum/#!topic/structure-software/s_rc_ueq6CU

Google groups suggests comparing plots for all optimal K, here K2-3. The way I've chosen the populations, I'm trying to differentiate between two clades with secondary contact. I'm quite confident that K=2, based on my sampling scheme. So I will compare both K, but wil work with K=2. 

In stead of choosing the most likely K as before in Structure, use CLUMPP to average over all the iterations for the most likely K.

####CLUMPP

http://web.stanford.edu/group/rosenberglab/papers/clumppNote.pdf



###2.1 Fst outlier analysis


####PCAdapt


####BayPass???


###2.2 EAA

####LFMM


###3.1 Random Forest


###3.2 Generalised Dissimilarity Model (GDM)





